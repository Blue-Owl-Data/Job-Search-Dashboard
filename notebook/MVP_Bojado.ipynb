{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Plotly/Dash\n",
    "import plotly.express as px  # (version 4.7.0)\n",
    "import plotly.graph_objects as go\n",
    "import dash  # (version 1.12.0) pip install dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "url = first_page_url_indeed('data scientist', 'al')\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code of the request:  200\n",
      "Document type:  <!DOCTYPE html>\n",
      "Title of the response:  Data Scientist Jobs, Employment in Alabama | Indeed.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "type(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type:  <class 'bs4.element.Tag'>\n",
      "Name of the Tag:  div\n",
      "Attributes of the Tag:  {'id': 'searchCountPages'}\n",
      "Text within the Tag: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n                    Page 1 of 49 jobs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "print(\"Data Type: \", type(num_jobs))\n",
    "print(\"Name of the Tag: \", num_jobs.name)\n",
    "print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "print(\"Text within the Tag: \")\n",
    "num_jobs.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of the jobs in the text\n",
    "match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function num_jobs_indeed\n",
    "num_jobs_indeed(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function job_cards_indeed\n",
    "job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# Print the data type of job_cards\n",
    "type(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Solution Architect - Data & Analytics\\nnew',\n",
       " 'Video Processing Engineer (Machine Learning)\\nnew',\n",
       " 'Data Management Solution Architect\\nnew',\n",
       " 'STATISTICIAN II\\nnew',\n",
       " 'MANAGER, ACCOUNT DEVELOPMENT/DATA SCIENCE',\n",
       " 'Data Scientist',\n",
       " 'Data Analyst - Microsoft Stack (mid-senior)',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist Intern',\n",
       " 'Data Scientist\\nnew',\n",
       " 'Software Engineer/Data Scientist',\n",
       " 'Statistical Analyst',\n",
       " 'Lead Financial Analyst - Artificial Intelligence Strategic G...',\n",
       " 'Artificial Intelligence (AI) and Machine Learning (ML) Engin...\\nnew',\n",
       " 'Principal AI Engineer: DevSecOps\\nnew']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = job_titles_indeed(job_cards)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deloitte',\n",
       " 'Yardstick Assessment Strategies Inc.',\n",
       " 'Deloitte',\n",
       " 'University of Alabama at Birmingham',\n",
       " 'B.A.S.S., LLC',\n",
       " 'Quiq Inc',\n",
       " 'Vaco',\n",
       " 'Vision',\n",
       " 'LOCKHEED MARTIN CORPORATION',\n",
       " 'MTA Inc',\n",
       " 'Torch Technologies, Inc.',\n",
       " 'The Personnel Board of Jefferson County',\n",
       " 'Deloitte',\n",
       " 'COLSA',\n",
       " 'Pearson']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: comany_names_indeed\n",
    "\n",
    "company_names = company_names_indeed(job_cards)\n",
    "company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " '1 day ago',\n",
       " '4 days ago',\n",
       " '5 days ago',\n",
       " '8 days ago',\n",
       " '8 days ago',\n",
       " '8 days ago',\n",
       " '8 days ago',\n",
       " '12 days ago',\n",
       " '7 days ago',\n",
       " '13 days ago',\n",
       " '10 days ago',\n",
       " '13 days ago',\n",
       " '7 days ago',\n",
       " '7 days ago']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: post_ages_indeed\n",
    "ages = post_ages_indeed(job_cards)\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Huntsville, AL 35806',\n",
       " 'Birmingham, AL 35216',\n",
       " 'Birmingham, AL 35203 (Central City area)',\n",
       " 'Birmingham, AL 35294',\n",
       " 'Birmingham, AL 35243',\n",
       " 'United States',\n",
       " 'Hartselle, AL',\n",
       " 'Huntsville, AL',\n",
       " 'Huntsville, AL 35806',\n",
       " 'Huntsville, AL 35806',\n",
       " 'Huntsville, AL 35802',\n",
       " 'Jefferson County, AL',\n",
       " 'Birmingham, AL 35203 (Central City area)',\n",
       " 'Huntsville, AL',\n",
       " 'Montgomery, AL']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = job_locations_indeed(job_cards)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.0',\n",
       " 'missing',\n",
       " '4.0',\n",
       " '4.1',\n",
       " 'missing',\n",
       " 'missing',\n",
       " '3.7',\n",
       " 'missing',\n",
       " '4.0',\n",
       " '5.0',\n",
       " 'missing',\n",
       " 'missing',\n",
       " '4.0',\n",
       " '3.9',\n",
       " '3.8']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = company_rating_indeed(job_cards)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n",
      "Solution Architect - Data & Analytics - Huntsville, AL 35806 - Indeed.com\n",
      "Status Code:  200\n",
      "Video Processing Engineer (Machine Learning) - Birmingham, AL 35216 - Indeed.com\n",
      "Status Code:  200\n",
      "Data Management Solution Architect - Birmingham, AL 35203 - Indeed.com\n",
      "Status Code:  200\n",
      "STATISTICIAN II - Birmingham, AL 35294 - Indeed.com\n",
      "Status Code:  200\n",
      "MANAGER, ACCOUNT DEVELOPMENT/DATA SCIENCE - Birmingham, AL 35243 - Indeed.com\n",
      "Status Code:  200\n",
      "Data Scientist - United States - Indeed.com\n",
      "Status Code:  200\n",
      "Vaco Careers and Employment | Indeed.com\n",
      "Status Code:  200\n",
      "Data Scientist - Huntsville, AL - Indeed.com\n",
      "Status Code:  200\n",
      "Data Scientist Intern - Huntsville, AL 35806 - Indeed.com\n",
      "Status Code:  200\n",
      "Data Scientist - Huntsville, AL 35806 - Indeed.com\n",
      "Status Code:  200\n",
      "Software Engineer/Data Scientist - Huntsville, AL 35802 - Indeed.com\n",
      "Status Code:  200\n",
      "Statistical Analyst - Jefferson County, AL - Indeed.com\n",
      "Status Code:  200\n",
      "Lead Financial Analyst - Artificial Intelligence Strategic Growth Offering (AI SGO) Finance & Investment - Birmingham, AL 35203 - Indeed.com\n",
      "Status Code:  200\n",
      "Artificial Intelligence (AI) and Machine Learning (ML) Engineer - Huntsville, AL - Indeed.com\n",
      "Status Code:  200\n",
      "Principal AI Engineer: DevSecOps - Montgomery, AL - Indeed.com\n"
     ]
    }
   ],
   "source": [
    "# Test the function: job_links_and_contents_indeed\n",
    "links, descriptions = job_links_and_contents_indeed(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.indeed.com/rc/clk?jk=aa8db77103adac4c&fccid=9e215d88a6b33622&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=e62b97e0c7349e63&fccid=0099988d7a062fd2&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=711842cbd46ad126&fccid=9e215d88a6b33622&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=1016366ef3995d6f&fccid=61b32dc36af3abe6&vjs=3',\n",
       " 'https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DVr7NBsl4cHSE4aBurKzaRDI_6xQvAm7MDK25NP7GKwrPFBHjnIhnnMkAKtOTcAVcwvdJYQ1iX_OUlRkCcJ9lYHn6yw2Xoq1V-R4Wdf5ZtG2pRL3zKYjPaUIJ7FfKiCmO7nZu9zD3akcI0SzfFz7oRw23qh6o47s6hI0zsqL0wfonad9sPhl6t3UDFBwfjeQ8UsdkN1uSmy8FKX2FNZWe-fJW53S8ie_vstN1Ap00o4paMKCeXknkYgIrtJV8rHsi1LgbawmWtUu-OCJMr6-a3MVwpsIXK8Rhu4G2qk0wUYJgawwP0qOykN7COW640VsM-PysyrSSWhE7s4rN3nzmDI8Qk4964IteZ2AqIgcaSnqJxLL8X_iZ2EXQ-wU-94yMhtumDQHeVfZxE-uKpqUbF8uxg1hhVyfJb20-MJgI1qTUbjvT4xv1dynb_8B6oRRV77dRRIy2LzBnQCmzzTeYEY7Y3RpKmqfY=&p=4&fvj=1&vjs=3',\n",
       " 'https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0B9JzWmK0GDxRzYzuZf9xSyXN8pQP8ihv6GH-rkAji3LyaR-hLXYB_NfHRnutLWTuqKBdImdeUix5pYB3Uv4mWnHLZlzd5py00o1lMFznkqSTkX2n8aY8mwnVhJALIJXce_3bj6Yi5-UDH-VhWVn3o6zeb2Pm8VmBf7XYjX1VtegVcIdN3lcJzHzr7sT1skBgIFX4qfsi0HLJFp91lfg2m-aDbhCz6h1JMx-zo-LXTM1z7o9zMTInEProeB02w98wPeVdie8zPfyff9ihrwihzCHnwJcKb-cHAX0tbx-RyulDxey8DUy17Y_wXsoFJutbfT9Cpw_1IE0pYtsp2GX0_MfFEfcEdAe-xnrInJC_qSgndXUJNV4VaOicOWY3fxiyOnVhJH8i8V49-iEOjNNLV5lE_FwwjvLLfDPH8gqc90FvxO0CE1Ow87&p=5&fvj=1&vjs=3',\n",
       " 'https://www.indeed.com/cmp/Vaco',\n",
       " 'https://www.indeed.com/company/Vision/jobs/Data-Scientist-fba1d08fac204b1e?fccid=c803b4b45f85b24e&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=72f0e3a5889212e2&fccid=aeb15e43a6800b9d&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=8aaec604e12b2946&fccid=32fb43e03100a136&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=940f9552f9a578bd&fccid=7c60e205aab51bb4&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=8683b7b606b65f53&fccid=c2261154d654a05c&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=990a5622086ad064&fccid=9e215d88a6b33622&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=093a4819c1095988&fccid=7d1d39ef87954789&vjs=3',\n",
       " 'https://www.indeed.com/rc/clk?jk=476ec8523faf5108&fccid=915b1c0ee87e5e8a&vjs=3']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Company:\\nHeadquartered in Birmingham, AL, ProctorU a growing software business on a mission to uphold the integrity of academic and professional testing. We use technology - not to \"catch cheaters\" - but to build trust in the expansion of access to education. Our team is responsible for building and supporting digital solutions trusted by over 1,000 institutions in 129 countries who administer over 2,000,000 exams per year and we plan to continue growing!\\n\\n\\nThe Team:\\n\\nOur Engineering team gets to innovate and experiment daily with some of the latest technologies in our industry for a product that is paving the way in our space. We are always looking for opportunities to learn, grow, and have fun with each other.\\n\\nThe Product:\\n\\nLive+ uses a multi-layered approach to proctoring. Live proctors are augmented by AI that helps flag suspicious activity, and the whole process is overseen by interventionists who routinely audit live sessions and are specifically trained to deal with potential breaches of integrity. Read more here: https://www.proctoru.com/services/live-online-proctoring\\n\\n\\nWe are currently looking for an experienced, passionate, and creative software craftsperson with an interest in innovation to move ProctorU’s Machine Learning initiative forward as a Video Processing Engineer. This role will work with the various engineering teams to improve the operations of the leader in Online Proctoring. Specifically, you will watch videos that have been recorded as part of proctored testing sessions and build solutions to optimize and improve the AI infrastructure used to monitor thousands of test takers simultaneously on our global platform..\\n\\n\\nThe things you’ll do:\\n\\nAnalyze requirements and collaborate with Product Managers, Scrum team members, Architects, and QA Engineers\\nAssist in the creation of prototypes and architectural specifications for video processing pipeline and infrastructure (facial recognition, etc)\\nWork with UI/UX and Product Management to design technical solutions to problems faced by the business and our clients\\nBuild, develop and deploy high-performing and scalable image processing and video services\\nBuild, develop and deploy computer vision and machine learning services\\nBuild systems on top of state-of-the-art CV algorithms across ProctorU’s platforms, as well as third-party services\\nSuccessfully communicate conceptual ideas and design rationale to engineering team and stakeholders\\n\\n\\nThe skills and experiences you’ll need:\\n\\nStrong interest in Video processing and/or Machine Learning\\nExcellent analytical and problem-solving skills\\nSelf-motivated learner who thrives in a fast-paced environment with aggressive timelines\\nTechnical experience with most, if not all, of the following:\\nVideo Streaming/Recording\\nWebRTC\\nGstreamer\\nKurento Media Server\\nC/C++/Rust\\nFFMpeg\\nOpenVidu\\nAmazon Web Services Infrastructure\\nRekognition\\nKinesis Video Streams\\n\\n\\n\\nThe values that are important to us:\\n\\nIntegrity. Honest, open and ethical practices\\nService. Remarkable service and personal leadership\\nInnovation. Anticipate, challenge, and create\\nExcellence. Quality through relentless improvement\\n\\nThe benefits you’ll get:\\n\\nBCBS Health, Dental, & Vision Insurance with generous employer funding for employees and dependents\\nFlexible Spending Accounts\\nHealth Savings Account with generous employer funding\\n401(k) plan with company matching\\nCompany-paid short-term and long-term disability plans\\nCompany- paid life insurance\\nCompany-subsidized pet insurance\\nReimbursements for professional development, student loan debt, child care costs, gym memberships, home gym equipment, and massage therapy\\nDog-friendly office\\nCasual working environment\\n\\nThe news:\\n\\nProctorU Receives Second Patent\\nProctorU partners with Google\\nCIO of the Year\\nProctorU Intros AI-Based Online Proctoring\\nOur Reaction to the Varsity Blues Scandal\\n\\n\\nLearn more at www.proctoru.com!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code of the request:  200\n",
      "Document type:  <!DOCTYPE html>\n",
      "Title of the response:  Data Scientist Jobs, Employment in Alabama | Indeed.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: page_soup_indeed\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "soup = page_soup_indeed(url)\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the job cards from the soup\n",
    "type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'locations': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location):\n",
    "    '''\n",
    "    This function accepts the job title and location and return \n",
    "    the job information pull from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'locations', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <=40:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Job Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data scientist job posts in TX\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the data scientist jobs in TX\n",
    "df_ds_tx1 = pd.read_csv(f\"{database}data_scientist_tx_indeed_012121.csv\", index_col=0)\n",
    "df_ds_tx2 = pd.read_csv(f\"{database}data_scientist_tx_indeed_012221.csv\", index_col=0)\n",
    "df_ds_tx3 = pd.read_csv(f\"{database}data_scientist_tx_indeed_012421.csv\", index_col=0)\n",
    "df_ds_tx4 = pd.read_csv(f\"{database}data_scientist_tx_indeed_012521.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the information\n",
    "df_ds_tx1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the information\n",
    "df_ds_tx2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the information\n",
    "df_ds_tx3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the information\n",
    "df_ds_tx4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results\n",
    "\n",
    "df_ds_tx = pd.concat([df_ds_tx1, df_ds_tx2, df_ds_tx3, df_ds_tx4], ignore_index=True)\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of duplicate rows \n",
    "# using columns: title, locations, company, job links and job description\n",
    "\n",
    "df_ds_tx.duplicated(subset=['title', 'locations', 'company', 'job_link', 'job_description']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of new jobs posted\n",
    "\n",
    "num_duplicates = df_ds_tx.duplicated(subset=['title', 'locations', 'company', \n",
    "                                              'job_link', 'job_description'], keep=False).sum()\n",
    "num_new_jobs = df_ds_tx.shape[0] - num_duplicates\n",
    "num_new_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of unique jobs\n",
    "\n",
    "num_jobs =  df_ds_tx.shape[0] - 855\n",
    "num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.job_link[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ds_tx.job_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'locations', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, keep='last', inplace=True, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ds_tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-156b47c20223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_ds_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ds_tx' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "df_ds_tx = remove_duplicates(df_ds_tx)\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ds_tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4fd0580ed330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rank the companies based on the number of posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ds_tx' is not defined"
     ]
    }
   ],
   "source": [
    "# Rank the companies based on the number of posts\n",
    "df_ds_tx.company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MVP_Bojado' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-31c8c6c88f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Clean the text in the `job_description` column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_ds_tx_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMVP_Bojado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_job_description_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'job_description'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MVP_Bojado' is not defined"
     ]
    }
   ],
   "source": [
    "# Clean the text in the `job_description` column\n",
    "\n",
    "df_ds_tx_clean = MVP_Bojado.prep_job_description_data(df_ds_tx, 'job_description')\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ds_tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f4e5b1b44422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ds_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ds_tx' is not defined"
     ]
    }
   ],
   "source": [
    "df_ds_tx.clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the cleaned dataframe in the database\n",
    "# df_ds_tx.to_csv(f\"{database}df_tx_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the helper function: words_variables\n",
    "\n",
    "companies = ['Apple', 'Deloitte', 'USAA']\n",
    "dic = words_variables(df_ds_tx, companies)\n",
    "\n",
    "dic['all'][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['Apple'][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the keys in the dictionary\n",
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function word_frequency\n",
    "\n",
    "df_word_frequency = word_frequency(dic)\n",
    "df_word_frequency.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_word_frequency.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 'Bigram' column to dataframe\n",
    "df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute bigrams_frequency\n",
    "\n",
    "bigrams = bigrams_frequency(dic)\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trigrams_frequency\n",
    "\n",
    "trigrams = trigrams_frequency(dic)\n",
    "trigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library for all skills\n",
    "\n",
    "library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', \n",
    "           'aws', 'hadoop', 'hive', 'impala', 'matlab', 'model', 'algorithm', \n",
    "           'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "           'sharepoint', 'dashboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "# big data\n",
    "# software engineering\n",
    "# model\n",
    "# models\n",
    "# algorithms\n",
    "# storytelling\n",
    "# statistic\n",
    "# statistical\n",
    "# machine learning\n",
    "# deep learning\n",
    "# etl\n",
    "# extraction\n",
    "# crud\n",
    "# exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_skills_ds(company, k):\n",
    "    '''\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_csv(f\"{database}df_tx_ds.csv\", index_col=0)\n",
    "    # Create the words that appear in the job description\n",
    "    dic = words_variables(df, company)\n",
    "    df_word_frequency = word_frequency(dic)\n",
    "    # Define a library that has a complete sillset for data scientist\n",
    "    library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', 'aws', 'hadoop', 'hive', \n",
    "        'impala', 'matlab', 'model', 'algorithm', 'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "        'sharepoint', 'dashboard']\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (df_word_frequency.index == skill)\n",
    "        df = df_word_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.sort_values(by=company, ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function top_skills_ds\n",
    "\n",
    "top_skills = top_skills_ds(['Apple'],10)\n",
    "top_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'python')\n",
    "df_word_frequency[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'r')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'aws')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'sql')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds\n",
    "python_cloud = WordCloud(background_color='white',\n",
    "                      height=800, width=800).generate(python_words)\n",
    "\n",
    "c_plus_plus_cloud = WordCloud(background_color='white', \n",
    "                      height=800, width=800).generate(c_plus_plus_words)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "axs = [plt.axes([.25, 1, .5, .5]), plt.axes([.8, 1, .5, .5])]\n",
    "\n",
    "# imshow => display data as an image\n",
    "axs[0].imshow(python_cloud)\n",
    "axs[1].imshow(c_plus_plus_cloud)\n",
    "\n",
    "axs[0].set_title('Python')\n",
    "axs[1].set_title('C++')\n",
    "\n",
    "for ax in axs: ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px  # (version 4.7.0)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import dash  # (version 1.12.0) pip install dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# ---------- Import and clean data (importing csv into pandas)\n",
    "# df = pd.read_csv(\"intro_bees.csv\")\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Coding-with-Adam/Dash-by-Plotly/master/Other/Dash_Introduction/intro_bees.csv\")\n",
    "\n",
    "df = df.groupby(['State', 'ANSI', 'Affected by', 'Year', 'state_code'])[['Pct of Colonies Impacted']].mean()\n",
    "df.reset_index(inplace=True)\n",
    "print(df[:5])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# App layout\n",
    "app.layout = html.Div([\n",
    "\n",
    "    html.H1(\"Web Application Dashboards with Dash\", style={'text-align': 'center'}),\n",
    "\n",
    "    dcc.Dropdown(id=\"slct_year\",\n",
    "                 options=[\n",
    "                     {\"label\": \"2015\", \"value\": 2015},\n",
    "                     {\"label\": \"2016\", \"value\": 2016},\n",
    "                     {\"label\": \"2017\", \"value\": 2017},\n",
    "                     {\"label\": \"2018\", \"value\": 2018}],\n",
    "                 multi=False,\n",
    "                 value=2015,\n",
    "                 style={'width': \"40%\"}\n",
    "                 ),\n",
    "\n",
    "    html.Div(id='output_container', children=[]),\n",
    "    html.Br(),\n",
    "\n",
    "    dcc.Graph(id='my_bee_map', figure={})\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Connect the Plotly graphs with Dash Components\n",
    "@app.callback(\n",
    "    [Output(component_id='output_container', component_property='children'),\n",
    "     Output(component_id='my_bee_map', component_property='figure')],\n",
    "    [Input(component_id='slct_year', component_property='value')]\n",
    ")\n",
    "def update_graph(option_slctd):\n",
    "    print(option_slctd)\n",
    "    print(type(option_slctd))\n",
    "\n",
    "    container = \"The year chosen by user was: {}\".format(option_slctd)\n",
    "\n",
    "    dff = df.copy()\n",
    "    dff = dff[dff[\"Year\"] == option_slctd]\n",
    "    dff = dff[dff[\"Affected by\"] == \"Varroa_mites\"]\n",
    "\n",
    "    # Plotly Express\n",
    "    fig = px.choropleth(\n",
    "        data_frame=dff,\n",
    "        locationmode='USA-states',\n",
    "        locations='state_code',\n",
    "        scope=\"usa\",\n",
    "        color='Pct of Colonies Impacted',\n",
    "        hover_data=['State', 'Pct of Colonies Impacted'],\n",
    "        color_continuous_scale=px.colors.sequential.YlOrRd,\n",
    "        labels={'Pct of Colonies Impacted': '% of Bee Colonies'},\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "\n",
    "    # Plotly Graph Objects (GO)\n",
    "    # fig = go.Figure(\n",
    "    #     data=[go.Choropleth(\n",
    "    #         locationmode='USA-states',\n",
    "    #         locations=dff['state_code'],\n",
    "    #         z=dff[\"Pct of Colonies Impacted\"].astype(float),\n",
    "    #         colorscale='Reds',\n",
    "    #     )]\n",
    "    # )\n",
    "    #\n",
    "    # fig.update_layout(\n",
    "    #     title_text=\"Bees Affected by Mites in the USA\",\n",
    "    #     title_xanchor=\"center\",\n",
    "    #     title_font=dict(size=24),\n",
    "    #     title_x=0.5,\n",
    "    #     geo=dict(scope='usa'),\n",
    "    # )\n",
    "\n",
    "    return container, fig\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
