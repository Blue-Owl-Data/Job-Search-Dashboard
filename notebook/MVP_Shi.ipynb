{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Geospatial Libraries\n",
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import folium\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi, MVP_Ortiz\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up universal variables\n",
    "\n",
    "database = env_Shi.database\n",
    "library_ds_tech = MVP_Shi.library_ds_tech\n",
    "library_ds_soft = MVP_Shi.library_ds_soft\n",
    "library_ds_general = MVP_Shi.library_ds_general\n",
    "library_wd_tech = MVP_Shi.library_wd_tech\n",
    "library_wd_soft = MVP_Shi.library_wd_soft\n",
    "library_wd_general = MVP_Shi.library_wd_general\n",
    "\n",
    "# Define a class named color\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>All the functions in the Data Acquisitioin section have been tested out and inorporated into the MVP_acquire_ds.py and MVP_acquire_wd.py files. To save space, no extra test is carried out in this notebook.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "# type(first_page_soup)\n",
    "\n",
    "# # Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "# num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "# print(\"Data Type: \", type(num_jobs))\n",
    "# print(\"Name of the Tag: \", num_jobs.name)\n",
    "# print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "# print(\"Text within the Tag: \")\n",
    "# num_jobs.text\n",
    "\n",
    "# # Find the number of the jobs in the text\n",
    "# match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "# match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function job_cards_indeed\n",
    "# job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# # Print the data type of job_cards\n",
    "# type(job_cards)\n",
    "\n",
    "# # How many jobs listed in the 1st page? \n",
    "# len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function: page_soup_indeed\n",
    "\n",
    "# url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "# soup = page_soup_indeed(url)\n",
    "# type(soup)\n",
    "\n",
    "# # Find out the page number\n",
    "# int(page_num_indeed(url))\n",
    "\n",
    "# # Pull the job cards from the soup\n",
    "# type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location, max_page=35):\n",
    "    '''\n",
    "    This function accepts the job title and location and return the job information (35 pages by default) \n",
    "    pulled from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <= max_page:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        df_jobs.to_csv(\"df_jobs_backup.csv\")\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Daily Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>The functions defined in this section are used to:</b>\n",
    "    <ul>\n",
    "        <li>Compute the date when a job is posted</li>\n",
    "        <li>Remove the duplicated job postings</li>\n",
    "        <li>Clean the job titles</li>\n",
    "        <li>Add new job positings</li>\n",
    "        <li>Clean the text in the job description</li>\n",
    "        <li>Delete the redudant columns</li>\n",
    "        <li>Adjust the data type</li>\n",
    "        <li>Save as a JSON file for the front end development</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date when the jobs are posted\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of a job posting based on its post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted' or age == 'Today' or age == 'Active today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted' or age == 'Today' or age == 'Active today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicated job postings\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe based on title, location, \n",
    "    company, job_link, and job_description\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the job titles for analysis\n",
    "\n",
    "def clean_job_title(title):\n",
    "    '''\n",
    "    This function removes the \"\\nnew\" and \"...\" in the job title.\n",
    "    '''\n",
    "    title = title.split(sep=\"\\nnew\")[0]\n",
    "#     title = title.split(sep=' -')[0]\n",
    "#     title = title.split(sep=' (')[0]\n",
    "#     title = title.split(sep=',')[0]\n",
    "    title = title.split(sep='...')[0]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Notes</b>\n",
    "    <p>1. The locations further break down to city, state, and zipcode in the daily update. According to observations, if the values in the city and zipcode columns are missing, the jobs are in remote.</p>\n",
    "    <p>2. Nearly 20% company do not have ratings. The missing values are replaced with the mean of the company rating.</p>\n",
    "    <p>3. 3.3% of the job postings has no description.</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add the new job new postings\n",
    "\n",
    "def daily_update(df_new):\n",
    "    '''\n",
    "    This function updates and prepares the job posts by adding daily new job postings \n",
    "    and save as JSON file. \n",
    "    '''\n",
    "    # Load the job posts based on the inputted job title\n",
    "    database = env_Shi.database\n",
    "    print(\"Enter the INITIALS of the job title:\")\n",
    "    initials = input()\n",
    "    df = pd.read_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    num_jobs = df.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    \n",
    "    # Add the new job postings\n",
    "    df_new = compute_post_date(df_new)\n",
    "    df = pd.concat([df, df_new]).sort_index(ascending=False)\n",
    "    \n",
    "    # Remove the duplicates\n",
    "    df = remove_duplicates(df)\n",
    "    \n",
    "    # Back up the jop postings as a csv file before data cleaning phase I\n",
    "    df.to_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    num_new_jobs = df.shape[0] - num_jobs\n",
    "    print(\"New Jobs of Posted Today: \", num_new_jobs)\n",
    "    \n",
    "    # Re-Load the dataset\n",
    "    df = pd.read_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    \n",
    "    # Clean the location data: break the location to city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    # Imputation\n",
    "    location.city = location.city.apply(lambda i: 'Remote' if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 'Remote' if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                              else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    \n",
    "    # Replace the missing values in the company rating with the mean\n",
    "    df.company_rating = df.company_rating.replace('missing', np.NaN).astype(float)\n",
    "    mean_rating = df.company_rating.mean()\n",
    "    df.company_rating.fillna(mean_rating, inplace=True)\n",
    "    df.company_rating = df.company_rating.round(2)\n",
    "    \n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    \n",
    "    # Clean the job title\n",
    "    df.title = df.title.apply(clean_job_title)\n",
    "    \n",
    "    # Drop the redundant columns post_age and location\n",
    "    redundant_cols = ['post_age', 'location', 'tokenized', 'stemmed', 'lemmatized']\n",
    "    df = df.drop(columns=redundant_cols)\n",
    "    \n",
    "    # Alther the data type of zipcode\n",
    "    df.zipcode = df.zipcode.apply(lambda i: int(i))\n",
    "    \n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_{initials}_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Notes:</b>\n",
    "    <p>1. When a dataframe with datetime index is saved as a csv file by default and then loaded again in the notebook, the datetime index is converted to a column and its data type is object. Then the new dataframe is saved as json file and then loading the json in the notebook, the datatype of the date is converted back to datetime64.<p>\n",
    "    <p>2. When a datetime object column is saved as in json, the datetime will be represented a series of number.</p>\n",
    "    <p>3. Detailed physical address can't be found on the Indeed website. It has to be obtained through other approaches.</p> \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load job posts of web developer in TX on Feb.12 2021\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_wd_old = pd.read_csv(f\"{database}web_developer_tx_indeed_021221.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_wd_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_wd_old = transform_old_file(df_wd_old, '2021-02-12')\n",
    "# df_wd_old.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Update of Job Postings of Data Scientist and Web Developer Positions in TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter FULL NAME of the job title in ALL lowercase: \n",
      "web developer\n",
      "Enter the date in the format mmddyy\n",
      "061021\n",
      "(195, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr. Front End Angular Developer\\nnew</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>SAM NETWORK SYSTEMS LLC.</td>\n",
       "      <td>missing</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/company/SAM-NETWORK-SYS...</td>\n",
       "      <td>Senior Level Frontend developer with experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software Engineer (J2EE), VP\\nnew</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=c8781155927af...</td>\n",
       "      <td>As an experienced member of our Software Engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Java Developer / Java UI Developer\\nnew</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>CTS</td>\n",
       "      <td>missing</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/company/CTS/jobs/Java-D...</td>\n",
       "      <td>Position – 1Title – Java Developer with Java 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr. Software Engineer\\nnew</td>\n",
       "      <td>United States</td>\n",
       "      <td>Decisiv</td>\n",
       "      <td>missing</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Decisiv is looking for a senior software engin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Software Engineer\\nnew</td>\n",
       "      <td>Austin, TX 78730</td>\n",
       "      <td>HotSchedules</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=b6b3d1338fc9a...</td>\n",
       "      <td>The New Tech for Restaurant and Hospitality\\nF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title          location  \\\n",
       "0     Sr. Front End Angular Developer\\nnew         Plano, TX   \n",
       "1        Software Engineer (J2EE), VP\\nnew         Plano, TX   \n",
       "2  Java Developer / Java UI Developer\\nnew        Irving, TX   \n",
       "3               Sr. Software Engineer\\nnew     United States   \n",
       "4            Senior Software Engineer\\nnew  Austin, TX 78730   \n",
       "\n",
       "                     company company_rating post_age  \\\n",
       "0   SAM NETWORK SYSTEMS LLC.        missing    Today   \n",
       "1  JPMorgan Chase Bank, N.A.            3.9    Today   \n",
       "2                        CTS        missing    Today   \n",
       "3                    Decisiv        missing    Today   \n",
       "4               HotSchedules            3.4    Today   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/company/SAM-NETWORK-SYS...   \n",
       "1  https://www.indeed.com/rc/clk?jk=c8781155927af...   \n",
       "2  https://www.indeed.com/company/CTS/jobs/Java-D...   \n",
       "3  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "4  https://www.indeed.com/rc/clk?jk=b6b3d1338fc9a...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Senior Level Frontend developer with experienc...  \n",
       "1  As an experienced member of our Software Engin...  \n",
       "2  Position – 1Title – Java Developer with Java 8...  \n",
       "3  Decisiv is looking for a senior software engin...  \n",
       "4  The New Tech for Restaurant and Hospitality\\nF...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the job postings in TX on March 12 2021\n",
    "\n",
    "# Read the daily job postings\n",
    "print(\"Enter FULL NAME of the job title in ALL lowercase: \")\n",
    "job_title = input()\n",
    "print(\"Enter the date in the format mmddyy\")\n",
    "date = input()\n",
    "job_title = job_title.split()\n",
    "df_new = pd.read_csv(f\"{database}{job_title[0]}_{job_title[1]}_tx_indeed_{date}.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_new.shape)\n",
    "\n",
    "# Print the first 5 rows\n",
    "df_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the INITIALS of the job title:\n",
      "wd\n",
      "New Jobs of Posted Today:  80\n",
      "CPU times: user 2min 37s, sys: 1.99 s, total: 2min 39s\n",
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>Sr. Front End Angular Developer</td>\n",
       "      <td>SAM NETWORK SYSTEMS LLC.</td>\n",
       "      <td>3.74</td>\n",
       "      <td>https://www.indeed.com/company/SAM-NETWORK-SYS...</td>\n",
       "      <td>Senior Level Frontend developer with experienc...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>senior level frontend developer experience mob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>Lead Software Developer</td>\n",
       "      <td>Suretys Inc</td>\n",
       "      <td>3.74</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Lead Software Developer/Architect6/9/21Respons...</td>\n",
       "      <td>Remote</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>lead software developerarchitect6921responsibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>J2EE Spring Micro Services Engineer(Software E...</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.90</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=40ba481aa9187...</td>\n",
       "      <td>As a member of our Software Engineering Group,...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>member software engineering group look first f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>UI/UX Developer</td>\n",
       "      <td>Sunnova Energy Corp</td>\n",
       "      <td>2.80</td>\n",
       "      <td>https://www.indeed.com/company/Sunnova-Energy-...</td>\n",
       "      <td>The PositionSunnova is looking for a full time...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>77046</td>\n",
       "      <td>positionsunnova looking full time uiux develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>Lead Info Security Engineer (Remote Optional)</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.90</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=c6a32e832a1cc...</td>\n",
       "      <td>Purpose of Job\\nWe are seeking a talented Lead...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>78288</td>\n",
       "      <td>purpose job seeking talented lead information ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2021-06-10                    Sr. Front End Angular Developer   \n",
       "1  2021-06-10                            Lead Software Developer   \n",
       "2  2021-06-10  J2EE Spring Micro Services Engineer(Software E...   \n",
       "3  2021-06-10                                    UI/UX Developer   \n",
       "4  2021-06-10      Lead Info Security Engineer (Remote Optional)   \n",
       "\n",
       "                     company  company_rating  \\\n",
       "0   SAM NETWORK SYSTEMS LLC.            3.74   \n",
       "1                Suretys Inc            3.74   \n",
       "2  JPMorgan Chase Bank, N.A.            3.90   \n",
       "3        Sunnova Energy Corp            2.80   \n",
       "4                       USAA            3.90   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/company/SAM-NETWORK-SYS...   \n",
       "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2  https://www.indeed.com/rc/clk?jk=40ba481aa9187...   \n",
       "3  https://www.indeed.com/company/Sunnova-Energy-...   \n",
       "4  https://www.indeed.com/rc/clk?jk=c6a32e832a1cc...   \n",
       "\n",
       "                                     job_description         city state  \\\n",
       "0  Senior Level Frontend developer with experienc...        Plano    TX   \n",
       "1  Lead Software Developer/Architect6/9/21Respons...       Remote    TX   \n",
       "2  As a member of our Software Engineering Group,...        Plano    TX   \n",
       "3  The PositionSunnova is looking for a full time...      Houston    TX   \n",
       "4  Purpose of Job\\nWe are seeking a talented Lead...  San Antonio    TX   \n",
       "\n",
       "   zipcode                                              clean  \n",
       "0        0  senior level frontend developer experience mob...  \n",
       "1        0  lead software developerarchitect6921responsibi...  \n",
       "2        0  member software engineering group look first f...  \n",
       "3    77046  positionsunnova looking full time uiux develop...  \n",
       "4    78288  purpose job seeking talented lead information ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Perform the daily updates\n",
    "\n",
    "df_current = daily_update(df_new)\n",
    "df_current.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12017 entries, 0 to 12016\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date             12017 non-null  object \n",
      " 1   title            12017 non-null  object \n",
      " 2   company          12017 non-null  object \n",
      " 3   company_rating   12017 non-null  float64\n",
      " 4   job_link         12017 non-null  object \n",
      " 5   job_description  12017 non-null  object \n",
      " 6   city             12017 non-null  object \n",
      " 7   state            12017 non-null  object \n",
      " 8   zipcode          12017 non-null  int64  \n",
      " 9   clean            12017 non-null  object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 939.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the dataframe\n",
    "df_current.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_current.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Takeaways:</b>\n",
    "After the job titles are cleaned, the duplicates of job postings starts to appear. It suggests that when the same job is re-posted, the job title changes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Read the json file\n",
    "\n",
    "# result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "# parsed = json.load(result)\n",
    "# parsed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Prepared JSON Files to AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Upload the json file to AWS\n",
    "\n",
    "# # Create the s3 resource object\n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "# # Create the job title initials\n",
    "# print(\"Enter the INITIALS of the job title:\")\n",
    "# initials = input()\n",
    "\n",
    "# # Upload df_ds_tx_backup.csv file\n",
    "# s3.Bucket(f'{initials}rawjobpostings').upload_file(f\"{database}df_{initials}_tx_backup.csv\", \n",
    "#                                                    f\"df_{initials}_tx_backup.csv\")\n",
    "\n",
    "# # Upload df_ds_tx_prepared_backup.json file\n",
    "# s3.Bucket(f'{initials}preparedjobpostings').upload_file(f\"{database}df_{initials}_tx_prepared_backup.json\", \n",
    "#                                                         f\"df_{initials}_tx_prepared_backup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>In this section, the data in the job postings are further explored in the following areas.</b>\n",
    "    <ul>\n",
    "        <li>Geospacial analysis</li>\n",
    "        <li>Text analysis</li>\n",
    "        <li>Time aeries analysis</li>\n",
    "        <li>More...</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire the Prepared Dataset From AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List All Buckets in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the s3 resource object\n",
    "# s3 = boto3.resource('s3')   # Notes: the datatype of s3 is s3.ServiceResource\n",
    "\n",
    "# # Print the bucket names\n",
    "# print(color.UNDERLINE + color.BOLD + \"List of Buckets in S3:\" + color.END)\n",
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List All the Files in a Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Input the name of the bucket\n",
    "# bucket_name = input()\n",
    "\n",
    "# # Create the bucket object\n",
    "# bucket = s3.Bucket(bucket_name) # Note: the data type of the bucket is s3.Bucket\n",
    "\n",
    "# # List all the files inside the bucket\n",
    "\n",
    "# print(color.UNDERLINE + color.BOLD + f\"List of Files in Bucket {bucket_name}:\" + color.END)\n",
    "# for page in bucket.objects.pages():\n",
    "#     for obj in page:\n",
    "#         print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Last Modified Datetime for the Prepared JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the time when the prepared json file is last modified\n",
    "\n",
    "# s3 = boto3.resource(\"s3\")\n",
    "# prepared_json = s3.Object('dspreparedjobpostings', 'df_ds_tx_prepared_backup.json')\n",
    "# print(\"Last modified time of JSON for data scientist position: \", prepared_json.last_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # When is the df_wd_tx_prepared.json last modified?\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "# prepared_json = s3.Object('wdpreparedjobpostings', 'df_wd_tx_prepared_backup.json')\n",
    "# print(\"Last modified time of JSON for web developer position: \", prepared_json.last_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Down Load JSON Files from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download JSON file from AWS \n",
    "# database = env_Shi.database\n",
    "# s3.Bucket('dsrawjobpostings').download_file('df_ds_tx_backup.csv', \n",
    "#                                             f\"{database}df_ds_tx_aws.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire the Prepared Job Postings of Data Science Positions from the Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load JSON file of job postings\n",
    "\n",
    "def read_job_postings_json():\n",
    "    '''\n",
    "    This function reads the JSON file of prepared job postings into a pandas dataframe \n",
    "    based on a job title and set the date as the index.\n",
    "    '''\n",
    "    # Load the file path of the local database\n",
    "    database = env_Shi.database\n",
    "    # Create the file name\n",
    "    print(\"Enter the INITIALS of the job title:\")\n",
    "    initials = input()\n",
    "    file_name = 'df_' + initials + '_tx_prepared_backup.json'\n",
    "    \n",
    "    # Read the JSON file into a pandas dataframe\n",
    "    df = pd.read_json(f'{database}{file_name}')\n",
    "    # Print the numbr of job posts\n",
    "    print(\"Number of Job Postings To Date: \", df.shape[0])\n",
    "    \n",
    "    # Ask: do you need to set the date as the index? \n",
    "    print(\"Do you want to set the date as the index? (Y/N)\")\n",
    "    answer = input()\n",
    "    if answer == 'Y' or answer == 'y':\n",
    "        # Convert the string date to datetime\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "        # Set the date as the index and sort the dataframe\n",
    "        df = df.set_index('date').sort_index(ascending=False)\n",
    "    elif answer == 'N' or answer == 'n':\n",
    "            print(\"You can manually set it up later or re-run this function.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the INITIALS of the job title:\n",
      "ds\n",
      "Number of Job Postings To Date:  5949\n",
      "Do you want to set the date as the index? (Y/N)\n",
      "y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Sr. Data Scientist - Medicare Claims Analytics...</td>\n",
       "      <td>Qlarant</td>\n",
       "      <td>3.00</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0821f17e77598...</td>\n",
       "      <td>Qlarant, Inc., is a not-for-profit corporation...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>qlarant inc notforprofit corporation partner p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Senior Data Scientist (NLP)</td>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0f6bf79544baa...</td>\n",
       "      <td>Tiger Analytics is looking for experienced Dat...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>tiger analytics looking experienced data scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amiga Informatics</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/company/Amiga-Informati...</td>\n",
       "      <td>Data Scientist Location: Dallas, TX (Remote ti...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>data scientist location dallas tx remote till ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Data Operations Specialist</td>\n",
       "      <td>DISCO</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=c6063dacc3266...</td>\n",
       "      <td>We are seeking a customer-focused individual w...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>seeking customerfocused individual strong data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>realtor.com</td>\n",
       "      <td>3.30</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=66e3c49029ff2...</td>\n",
       "      <td>We have the most comprehensive and accurate co...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>comprehensive accurate coverage real estate li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-06-08  Sr. Data Scientist - Medicare Claims Analytics...   \n",
       "2021-06-08                        Senior Data Scientist (NLP)   \n",
       "2021-06-08                                     Data Scientist   \n",
       "2021-06-08                         Data Operations Specialist   \n",
       "2021-06-08                              Senior Data Scientist   \n",
       "\n",
       "                      company  company_rating  \\\n",
       "date                                            \n",
       "2021-06-08            Qlarant            3.00   \n",
       "2021-06-08    Tiger Analytics            3.78   \n",
       "2021-06-08  Amiga Informatics            3.78   \n",
       "2021-06-08              DISCO            3.78   \n",
       "2021-06-08        realtor.com            3.30   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=0821f17e77598...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=0f6bf79544baa...   \n",
       "2021-06-08  https://www.indeed.com/company/Amiga-Informati...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=c6063dacc3266...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=66e3c49029ff2...   \n",
       "\n",
       "                                              job_description    city state  \\\n",
       "date                                                                          \n",
       "2021-06-08  Qlarant, Inc., is a not-for-profit corporation...  Dallas    TX   \n",
       "2021-06-08  Tiger Analytics is looking for experienced Dat...  Austin    TX   \n",
       "2021-06-08  Data Scientist Location: Dallas, TX (Remote ti...  Dallas    TX   \n",
       "2021-06-08  We are seeking a customer-focused individual w...  Austin    TX   \n",
       "2021-06-08  We have the most comprehensive and accurate co...  Austin    TX   \n",
       "\n",
       "            zipcode                                              clean  \n",
       "date                                                                    \n",
       "2021-06-08        0  qlarant inc notforprofit corporation partner p...  \n",
       "2021-06-08        0  tiger analytics looking experienced data scien...  \n",
       "2021-06-08        0  data scientist location dallas tx remote till ...  \n",
       "2021-06-08        0  seeking customerfocused individual strong data...  \n",
       "2021-06-08        0  comprehensive accurate coverage real estate li...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the job postings for the data scientists\n",
    "df_ds_tx = read_job_postings_json()\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 5949 entries, 2021-06-08 to 2020-12-22\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            5949 non-null   object \n",
      " 1   company          5949 non-null   object \n",
      " 2   company_rating   5949 non-null   float64\n",
      " 3   job_link         5949 non-null   object \n",
      " 4   job_description  5949 non-null   object \n",
      " 5   city             5949 non-null   object \n",
      " 6   state            5949 non-null   object \n",
      " 7   zipcode          5949 non-null   int64  \n",
      " 8   clean            5949 non-null   object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 593.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title              Healthcare Data Scientist - Watson Health Fede...\n",
       "company                                                          IBM\n",
       "company_rating                                                   3.9\n",
       "job_link           https://www.indeed.com/rc/clk?jk=c2a250b10984d...\n",
       "job_description    Introduction\\nAs a Data Scientist at IBM, you ...\n",
       "city                                                          Austin\n",
       "state                                                             TX\n",
       "zipcode                                                        73301\n",
       "clean              introduction data scientist ibm help transform...\n",
       "Name: 2021-06-07 00:00:00, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask = df_ds_tx.loc['2021-06-09'].title == 'Data Scientist'\n",
    "# mask = df_ds_tx.loc['2021-06-02'].city == 'San Antonio'\n",
    "# mask = df_ds_tx.loc['2021-06-02'].company == 'American Airlines'\n",
    "# df = df_ds_tx.loc['2021-06-09'][mask]\n",
    "df = df_ds_tx.loc['2021-06-07'].iloc[-1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/rc/clk?jk=c2a250b10984de7d&fccid=de71a49b535e21cb&vjs=3'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.job_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the job postings for web developers\n",
    "df_wd_tx = read_job_postings_json()\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df_wd_tx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breif Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scientist Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot the top 10 companies and top 5 cities by the number of the job posts\n",
    "\n",
    "plt.figure(figsize=(13,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "top_companies = df_ds_tx.company.value_counts().head(10)\n",
    "top_companies.sort_values().plot(kind='barh')\n",
    "plt.title(\"Which Companies Need Data Scientist?\", fontweight='bold')\n",
    "\n",
    "plt.subplot(122)\n",
    "top_cities = df_ds_tx.city.value_counts().head(5)\n",
    "top_cities.sort_values().plot(kind='barh')\n",
    "plt.title(\"Which Cities Need Data Scientist?\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot the popular titles of the job postings\n",
    "\n",
    "top_title = df_ds_tx.title.value_counts().head(5)\n",
    "top_title.sort_values().plot(kind='barh')\n",
    "plt.title(\"What Are the Popular Titles?\", fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospacial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the data types of the df_ds_tx\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a copy and add the city_state column\n",
    "\n",
    "df_ds_tx_copy = df_ds_tx.copy()\n",
    "df_ds_tx_copy = df_ds_tx_copy.assign(city_state = \n",
    "                                     df_ds_tx_copy['city'] + ', ' + df_ds_tx_copy['state'])\n",
    "\n",
    "df_ds_tx_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate coordinates for all the citys in the df_ds_tx\n",
    "\n",
    "df_coordinates = MVP_Ortiz.get_geodata(df_ds_tx_copy)\n",
    "print(\"Number of cities: \", df_coordinates.shape[0])\n",
    "df_coordinates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save as the json and upload to AWS s3 bucket\n",
    "\n",
    "# file_name = \"geospatial_data.json\"\n",
    "# df_coordinates.to_json(f\"{database}{file_name}\", orient=\"records\")\n",
    "\n",
    "# # Upload to AWS S3 bucket\n",
    "# s3 = boto3.resource('s3')\n",
    "# s3.Bucket(\"additionaljobinfo\").upload_file(f\"{database}{file_name}\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add coordinates to the original dataframe\n",
    "# df_ds_tx_copy = df_ds_tx_copy.merge(df_coordinates, how='left', on='city_state')\n",
    "# df_ds_tx_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Notes:</b><br>\n",
    "    The rows may be re-sorted according to key order when two dataframes are merged, depending on what type of merge is performed.<br>\n",
    "    If joining columns on column, the dataframe indexes will be ignored.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the date to string date\n",
    "# df_ds_tx_copy.date = df_ds_tx_copy.date.apply(lambda i: i.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ds_tx_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Save as the JSON file for data visualization and analysis in Tableau\n",
    "# df_ds_tx_copy.to_json(f\"{database}df_ds_tx_tableau.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qlarant inc notforprofit corporation partner public private sector create high quality safe efficien'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the words variable\n",
    "d_words = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print the first characters in the words variable\n",
    "d_words['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the frequence of the mono-, bi-, and tri-grams of the job description\n",
    "\n",
    "def everygram_frequency_v1(d_words, max_len=3):\n",
    "    '''\n",
    "    This function accetps the dictionary produced by the function `words_variables_v1` and \n",
    "    return mono-, bi-, and tri-grams along with their frequency. \n",
    "    '''\n",
    "    # Generate mono-, bi-, and tri-grams\n",
    "    grams = nltk.everygrams(d_words['frequency'].split(), max_len=max_len) # dtype of grams: <class 'genertor'>\n",
    "    # Convert to a list of tuples\n",
    "    grams = list(grams)\n",
    "    # Create an empty list to hold mono-, bi-, and tri-grams\n",
    "    everygram = []\n",
    "    # For loop the list of tuples and convert the grams to strings\n",
    "    for gram in grams:\n",
    "        str_gram = gram[0]\n",
    "        for i in gram[1:]:\n",
    "            str_gram = str_gram + ' ' + i\n",
    "        everygram.append(str_gram)\n",
    "    # Compute the frequency of the everygrams\n",
    "    everygram = pd.Series(everygram).value_counts()\n",
    "    return everygram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.13 s, sys: 707 ms, total: 7.84 s\n",
      "Wall time: 7.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data          58992\n",
       "experience    34483\n",
       "business      23319\n",
       "team          20470\n",
       "work          18153\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the frequency of mono-, bi-, and tri-grams\n",
    "gram_frequency = everygram_frequency_v1(d_words)\n",
    "\n",
    "# Print the top 5 most frequent word(s)\n",
    "gram_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Top 10 Skills for Data Scientists in TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pick up the top k skills from a skill library\n",
    "\n",
    "def top_skills_v1(df, k, library):\n",
    "    '''\n",
    "    This function accepts a positive integer k and a skillset library and \n",
    "    returns a dataframe containing the top k skills needed for data scientist positions.\n",
    "    '''\n",
    "    # Confirm library type\n",
    "    print(\"Please confirm library type: general or tech or soft?\")\n",
    "    library_type = input()\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    everygram_frequency = everygram_frequency_v1(dic)\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (everygram_frequency.index == skill)\n",
    "        df =  everygram_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.columns = dic.keys()\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    # Reset the index\n",
    "    df_skills.reset_index(inplace=True)\n",
    "    # Rename the column name\n",
    "    df_skills.rename(columns={'index': f'top{k}_{library_type}_skills'}, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in tech skill library:  71\n",
      "Please confirm library type: general or tech or soft?\n",
      "tech\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_tech_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>9319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "      <td>5075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sql</td>\n",
       "      <td>3662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r</td>\n",
       "      <td>2642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aws</td>\n",
       "      <td>2412.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>big data</td>\n",
       "      <td>2035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spark</td>\n",
       "      <td>1780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deep learning</td>\n",
       "      <td>1714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>azure</td>\n",
       "      <td>1535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hadoop</td>\n",
       "      <td>1459.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_tech_skills  frequency\n",
       "0  machine learning     9319.0\n",
       "1            python     5075.0\n",
       "2               sql     3662.0\n",
       "3                 r     2642.0\n",
       "4               aws     2412.0\n",
       "5          big data     2035.0\n",
       "6             spark     1780.0\n",
       "7     deep learning     1714.0\n",
       "8             azure     1535.0\n",
       "9            hadoop     1459.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the tech library\n",
    "print(\"Number of skills in tech skill library: \", len(library_ds_tech))\n",
    "\n",
    "# Compute top 10 technical skills for data scientist position in TX\n",
    "df_ds_top_tech = top_skills_v1(df_ds_tx, 10, library_ds_tech)\n",
    "df_ds_top_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  19\n",
      "Please confirm library type: general or tech or soft?\n",
      "soft\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_soft_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>communication</td>\n",
       "      <td>3978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leadership</td>\n",
       "      <td>2926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collaboration</td>\n",
       "      <td>1366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>problem solving</td>\n",
       "      <td>753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decision making</td>\n",
       "      <td>648.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>written communication</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>verbal communication</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>curiosity</td>\n",
       "      <td>323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>business acumen</td>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>creativity</td>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       top10_soft_skills  frequency\n",
       "0          communication     3978.0\n",
       "1             leadership     2926.0\n",
       "2          collaboration     1366.0\n",
       "3        problem solving      753.0\n",
       "4        decision making      648.0\n",
       "5  written communication      563.0\n",
       "6   verbal communication      500.0\n",
       "7              curiosity      323.0\n",
       "8        business acumen      321.0\n",
       "9             creativity      321.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the soft library\n",
    "print(\"Number of skills in soft skill library: \", len(library_ds_soft))\n",
    "\n",
    "# Compute top 10 soft skills for data scientists in TX\n",
    "df_ds_top_soft = top_skills_v1(df_ds_tx, 10, library_ds_soft)\n",
    "df_ds_top_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  89\n",
      "Please confirm library type: general or tech or soft?\n",
      "general\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_general_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>machine learning</td>\n",
       "      <td>9319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "      <td>5075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>communication</td>\n",
       "      <td>3978.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sql</td>\n",
       "      <td>3662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leadership</td>\n",
       "      <td>2926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r</td>\n",
       "      <td>2642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aws</td>\n",
       "      <td>2412.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>big data</td>\n",
       "      <td>2035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark</td>\n",
       "      <td>1780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deep learning</td>\n",
       "      <td>1714.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_general_skills  frequency\n",
       "0     machine learning     9319.0\n",
       "1               python     5075.0\n",
       "2        communication     3978.0\n",
       "3                  sql     3662.0\n",
       "4           leadership     2926.0\n",
       "5                    r     2642.0\n",
       "6                  aws     2412.0\n",
       "7             big data     2035.0\n",
       "8                spark     1780.0\n",
       "9        deep learning     1714.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the soft library\n",
    "print(\"Number of skills in soft skill library: \", len(library_ds_general))\n",
    "\n",
    "# Compute top 10 soft skills for data scientists in TX\n",
    "df_ds_top_general = top_skills_v1(df_ds_tx, 10, library_ds_general)\n",
    "df_ds_top_general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 10 Skills for Web Developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in tech skill library:  65\n",
      "Please confirm library type: general or tech or soft?\n",
      "tech\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_tech_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>javascript</td>\n",
       "      <td>9309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>java</td>\n",
       "      <td>6084.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sql</td>\n",
       "      <td>5600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>react</td>\n",
       "      <td>5206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>4692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>net</td>\n",
       "      <td>4519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>html</td>\n",
       "      <td>4187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>angular</td>\n",
       "      <td>4103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aws</td>\n",
       "      <td>3822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ui</td>\n",
       "      <td>3574.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_tech_skills  frequency\n",
       "0        javascript     9309.0\n",
       "1              java     6084.0\n",
       "2               sql     5600.0\n",
       "3             react     5206.0\n",
       "4                 c     4692.0\n",
       "5               net     4519.0\n",
       "6              html     4187.0\n",
       "7           angular     4103.0\n",
       "8               aws     3822.0\n",
       "9                ui     3574.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the tech library\n",
    "print(\"Number of skills in tech skill library: \", len(library_wd_tech))\n",
    "\n",
    "# Compute top 10 technical skills for web developers in TX\n",
    "df_wd_top_tech = top_skills_v1(df_wd_tx, 10, library_wd_tech)\n",
    "df_wd_top_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  19\n",
      "Please confirm library type: general or tech or soft?\n",
      "soft\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_soft_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>communication</td>\n",
       "      <td>6981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leadership</td>\n",
       "      <td>2613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>collaboration</td>\n",
       "      <td>1933.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>problem solving</td>\n",
       "      <td>1331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>written communication</td>\n",
       "      <td>1051.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>verbal communication</td>\n",
       "      <td>759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>team player</td>\n",
       "      <td>752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>teamwork</td>\n",
       "      <td>588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>creativity</td>\n",
       "      <td>491.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time management</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       top10_soft_skills  frequency\n",
       "0          communication     6981.0\n",
       "1             leadership     2613.0\n",
       "2          collaboration     1933.0\n",
       "3        problem solving     1331.0\n",
       "4  written communication     1051.0\n",
       "5   verbal communication      759.0\n",
       "6            team player      752.0\n",
       "7               teamwork      588.0\n",
       "8             creativity      491.0\n",
       "9        time management      430.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the soft library\n",
    "print(\"Number of skills in soft skill library: \", len(library_wd_soft))\n",
    "\n",
    "# Compute top 5 soft skills for web devlopers in TX\n",
    "df_wd_top_soft = top_skills_v1(df_wd_tx, 10, library_wd_soft)\n",
    "df_wd_top_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  84\n",
      "Please confirm library type: general or tech or soft?\n",
      "general\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_general_skills</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>javascript</td>\n",
       "      <td>9309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>communication</td>\n",
       "      <td>6981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>java</td>\n",
       "      <td>6084.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sql</td>\n",
       "      <td>5600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>react</td>\n",
       "      <td>5206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c</td>\n",
       "      <td>4692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>net</td>\n",
       "      <td>4519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>html</td>\n",
       "      <td>4187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>angular</td>\n",
       "      <td>4103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aws</td>\n",
       "      <td>3822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  top10_general_skills  frequency\n",
       "0           javascript     9309.0\n",
       "1        communication     6981.0\n",
       "2                 java     6084.0\n",
       "3                  sql     5600.0\n",
       "4                react     5206.0\n",
       "5                    c     4692.0\n",
       "6                  net     4519.0\n",
       "7                 html     4187.0\n",
       "8              angular     4103.0\n",
       "9                  aws     3822.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of skills in the soft library\n",
    "print(\"Number of skills in soft skill library: \", len(library_wd_general))\n",
    "\n",
    "# Compute top 5 soft skills for web developers in TX\n",
    "df_wd_top_general = top_skills_v1(df_wd_tx, 10, library_wd_general)\n",
    "df_wd_top_general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis\n",
    "- Centered on Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the frequencies of the top 5 skills in each observation\n",
    "\n",
    "def top_skill_frequency(df, df_top):\n",
    "    '''\n",
    "    This function accepts the dataframe of the prepared job postings and the top k skills and provides\n",
    "    three options: \n",
    "    - to save the frequencies of the top skills over time as a JSON file and upload to AWS.\n",
    "    - to return a dataframe containing only the frequecies of the top skills over time.\n",
    "    - to add the frequencies of the top skills over time to the original dataframe\n",
    "    - to save the merged dataframe as JSON file in the database. \n",
    "    '''\n",
    "    # Reminder\n",
    "    print(\"Please insure the date is in the right format\")\n",
    "    \n",
    "    # Confirm the library type\n",
    "    print(\"Confirm TYPE of library: tech or soft or general\")\n",
    "    library_type = input()\n",
    "\n",
    "    # Confirm the job title\n",
    "    print(\"Enter the INITIALS of the job title:\")\n",
    "    initials = input()\n",
    "    \n",
    "    # Create a list of the top k skills\n",
    "    skill_list = df_top.iloc[:, 0].to_list()\n",
    "    \n",
    "    # Create an empty dictionary to hold the frequency of the skill in each observation\n",
    "    dic_frequency = {}    \n",
    "    # Loop through the list of skills to compute its frequency in each observation\n",
    "    for skill in skill_list:\n",
    "        list_frequency = []\n",
    "        for string in df.clean.values:\n",
    "            matches = re.findall(f\" {skill} \", string)\n",
    "            frequency = len(matches)\n",
    "            list_frequency.append(frequency)\n",
    "        dic_frequency[skill]=list_frequency\n",
    "        \n",
    "    # Convert the dictionary into the dataframe and set the index the same as df\n",
    "    df_frequency = pd.DataFrame(dic_frequency, index=df.index)\n",
    "    \n",
    "    # Save as JSON file and upload to AWS\n",
    "    print(\"Do you want to save the dataframe as JSON and upload to AWS? (Y/N)\")\n",
    "    save_option = input()\n",
    "    if save_option == \"Y\" or save_option == 'y':\n",
    "        df_freq_copy = df_frequency.reset_index()\n",
    "        df_freq_copy.date = df_freq_copy.date.apply(lambda i: i.strftime(\"%Y-%m-%d\"))\n",
    "        database = env_Shi.database\n",
    "        file_name = f\"{initials}_top_{library_type}_ts.json\"\n",
    "        df_freq_copy.to_json(f\"{database}{file_name}\", orient=\"records\")\n",
    "        s3 = boto3.resource(\"s3\")\n",
    "        s3.Bucket(\"additionaljobinfo\").upload_file(f\"{database}{file_name}\", file_name)\n",
    "    elif save_option == \"N\" or save_option == 'n':\n",
    "        print(\"You can manually save it by yourself\")\n",
    "        \n",
    "    # Merge two dataframe together\n",
    "    print(\"Do you want to merge the dataframes? (Y/N)\")\n",
    "    merge_option = input()\n",
    "    if merge_option == \"Y\" or merge_option == 'y':\n",
    "        df_frequency = pd.concat([df, df_frequency], axis=1)\n",
    "        \n",
    "        # Save the merged dataframe as JSON\n",
    "        print(\"Do you want to save the merged dataframe? (Y/N)\")\n",
    "        save_option = input()\n",
    "        if save_option == 'Y' or save_option == 'y':\n",
    "            df_freq_copy = df_frequency.reset_index()\n",
    "            df_freq_copy.date = df_freq_copy.date.apply(lambda i: i.strftime(\"%Y-%m-%d\"))\n",
    "            database = env_Shi.database\n",
    "            file_name = f\"df_{initials}_tx_top_{library_type}_ts.json\"\n",
    "            df_freq_copy.to_json(f\"{database}{file_name}\", orient=\"records\")\n",
    "        elif save_option == 'N' or save_option == 'n':\n",
    "            print(\"You can manually save it by yourself\")\n",
    "\n",
    "    elif merge_option == \"N\" or merge_option == 'n':\n",
    "        print(\"You can manually merge it by yourself\")\n",
    "    \n",
    "    return df_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please insure the date is in the right format\n",
      "Confirm TYPE of library: tech or soft or general\n",
      "general\n",
      "Enter the INITIALS of the job title:\n",
      "ds\n",
      "Do you want to save the dataframe as JSON and upload to AWS? (Y/N)\n",
      "n\n",
      "You can manually save it by yourself\n",
      "Do you want to merge the dataframes? (Y/N)\n",
      "y\n",
      "Do you want to save the merged dataframe? (Y/N)\n",
      "y\n",
      "CPU times: user 648 ms, sys: 84.1 ms, total: 732 ms\n",
      "Wall time: 9.61 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>machine learning</th>\n",
       "      <th>python</th>\n",
       "      <th>communication</th>\n",
       "      <th>sql</th>\n",
       "      <th>leadership</th>\n",
       "      <th>r</th>\n",
       "      <th>aws</th>\n",
       "      <th>big data</th>\n",
       "      <th>spark</th>\n",
       "      <th>deep learning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Sr. Data Scientist - Medicare Claims Analytics...</td>\n",
       "      <td>Qlarant</td>\n",
       "      <td>3.00</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0821f17e77598...</td>\n",
       "      <td>Qlarant, Inc., is a not-for-profit corporation...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>qlarant inc notforprofit corporation partner p...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Senior Data Scientist (NLP)</td>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=0f6bf79544baa...</td>\n",
       "      <td>Tiger Analytics is looking for experienced Dat...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>tiger analytics looking experienced data scien...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amiga Informatics</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/company/Amiga-Informati...</td>\n",
       "      <td>Data Scientist Location: Dallas, TX (Remote ti...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>data scientist location dallas tx remote till ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Data Operations Specialist</td>\n",
       "      <td>DISCO</td>\n",
       "      <td>3.78</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=c6063dacc3266...</td>\n",
       "      <td>We are seeking a customer-focused individual w...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>seeking customerfocused individual strong data...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-08</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>realtor.com</td>\n",
       "      <td>3.30</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=66e3c49029ff2...</td>\n",
       "      <td>We have the most comprehensive and accurate co...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>comprehensive accurate coverage real estate li...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-06-08  Sr. Data Scientist - Medicare Claims Analytics...   \n",
       "2021-06-08                        Senior Data Scientist (NLP)   \n",
       "2021-06-08                                     Data Scientist   \n",
       "2021-06-08                         Data Operations Specialist   \n",
       "2021-06-08                              Senior Data Scientist   \n",
       "\n",
       "                      company  company_rating  \\\n",
       "date                                            \n",
       "2021-06-08            Qlarant            3.00   \n",
       "2021-06-08    Tiger Analytics            3.78   \n",
       "2021-06-08  Amiga Informatics            3.78   \n",
       "2021-06-08              DISCO            3.78   \n",
       "2021-06-08        realtor.com            3.30   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=0821f17e77598...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=0f6bf79544baa...   \n",
       "2021-06-08  https://www.indeed.com/company/Amiga-Informati...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=c6063dacc3266...   \n",
       "2021-06-08  https://www.indeed.com/rc/clk?jk=66e3c49029ff2...   \n",
       "\n",
       "                                              job_description    city state  \\\n",
       "date                                                                          \n",
       "2021-06-08  Qlarant, Inc., is a not-for-profit corporation...  Dallas    TX   \n",
       "2021-06-08  Tiger Analytics is looking for experienced Dat...  Austin    TX   \n",
       "2021-06-08  Data Scientist Location: Dallas, TX (Remote ti...  Dallas    TX   \n",
       "2021-06-08  We are seeking a customer-focused individual w...  Austin    TX   \n",
       "2021-06-08  We have the most comprehensive and accurate co...  Austin    TX   \n",
       "\n",
       "            zipcode                                              clean  \\\n",
       "date                                                                     \n",
       "2021-06-08        0  qlarant inc notforprofit corporation partner p...   \n",
       "2021-06-08        0  tiger analytics looking experienced data scien...   \n",
       "2021-06-08        0  data scientist location dallas tx remote till ...   \n",
       "2021-06-08        0  seeking customerfocused individual strong data...   \n",
       "2021-06-08        0  comprehensive accurate coverage real estate li...   \n",
       "\n",
       "            machine learning  python  communication  sql  leadership  r  aws  \\\n",
       "date                                                                           \n",
       "2021-06-08                 0       3              0    1           0  3    0   \n",
       "2021-06-08                 4       1              1    1           1  1    0   \n",
       "2021-06-08                 2       1              1    0           0  1    0   \n",
       "2021-06-08                 0       0              2    1           0  0    0   \n",
       "2021-06-08                 0       1              1    2           0  1    0   \n",
       "\n",
       "            big data  spark  deep learning  \n",
       "date                                        \n",
       "2021-06-08         0      0              0  \n",
       "2021-06-08         0      0              0  \n",
       "2021-06-08         1      1              0  \n",
       "2021-06-08         0      0              0  \n",
       "2021-06-08         0      0              0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_test = top_skill_frequency(df_ds_tx, df_ds_top_general)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_skill_ts(df, df_top):\n",
    "    '''\n",
    "    This function accetps the dataframe of preapred job postings with the frequencies of the skills\n",
    "    and plot how popular each skill changes over time. \n",
    "    '''\n",
    "    # Set up the size of the plot\n",
    "    plt.figure(figsize=(11, 8))\n",
    "    # Create a list of the top skills\n",
    "    skill_list = df_top.iloc[:, 0].to_list()\n",
    "    # Resample the dataset by week and plot the mean of the frequency of each skill per job posting\n",
    "    for skill in skill_list:\n",
    "        df.resample('W')[skill].mean().plot(label=f'{skill} Weekly')\n",
    "    \n",
    "    # Name the plot\n",
    "    plt.title(\"How Popular the Top 5 Skills Are Over Time\", fontweight='bold')\n",
    "    # Position the legend\n",
    "    plt.legend(bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_skill_ts(df_ds_tx, df_ds_top_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the job position that match the skills of a applicant\n",
    "\n",
    "def skills_match_search(skills):\n",
    "    '''\n",
    "    '''  \n",
    "    # Create the initials of the job_title\n",
    "    print(\"Enter the INITIALS of the job title:\")\n",
    "    initials = input()\n",
    "    # Load the file path\n",
    "    database = env_Shi.database\n",
    "    # Create the file name\n",
    "    file_name = 'df_' + initials + '_tx_prepared_backup.json'\n",
    "    # Load the job postings file\n",
    "    df = pd.read_json(f'{database}{file_name}')\n",
    "    # Create a list variable to hold the boolean values\n",
    "    mask = []\n",
    "    # For loop \n",
    "    for clean in df.clean:\n",
    "        if all(skill in clean for skill in skills):\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    df_match = df[mask]\n",
    "    cols = ['job_description', 'clean']\n",
    "    df_match.drop(columns=cols, inplace=True)\n",
    "    print(\"Number of the Matched Companies: \", df_match.shape[0])\n",
    "    return df_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the skillset I own\n",
    "skills = ['python', 'sql', 'tableau']\n",
    "\n",
    "df_search = skills_match_search(skills)\n",
    "df_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_search.title == 'Data Scientist')\n",
    "df_search = df_search[mask]\n",
    "df_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "\n",
    "print(df_search.job_link.iloc[i])\n",
    "\n",
    "df_search.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau & mask_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
