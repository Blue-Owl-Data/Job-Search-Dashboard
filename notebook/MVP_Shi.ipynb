{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Geospatial Libraries\n",
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import folium\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>All the functions in the Data Acquisitioin section have been tested out and inorporated into the MVP_acquire_ds.py and MVP_acquire_wd.py files. To save space, no extra test is carried out in this notebook.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "# type(first_page_soup)\n",
    "\n",
    "# # Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "# num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "# print(\"Data Type: \", type(num_jobs))\n",
    "# print(\"Name of the Tag: \", num_jobs.name)\n",
    "# print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "# print(\"Text within the Tag: \")\n",
    "# num_jobs.text\n",
    "\n",
    "# # Find the number of the jobs in the text\n",
    "# match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "# match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function job_cards_indeed\n",
    "# job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# # Print the data type of job_cards\n",
    "# type(job_cards)\n",
    "\n",
    "# # How many jobs listed in the 1st page? \n",
    "# len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function: page_soup_indeed\n",
    "\n",
    "# url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "# soup = page_soup_indeed(url)\n",
    "# type(soup)\n",
    "\n",
    "# # Find out the page number\n",
    "# int(page_num_indeed(url))\n",
    "\n",
    "# # Pull the job cards from the soup\n",
    "# type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location, max_page=35):\n",
    "    '''\n",
    "    This function accepts the job title and location and return the job information (35 pages by default) \n",
    "    pulled from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <= max_page:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        df_jobs.to_csv(\"df_jobs_backup.csv\")\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date of the job posts\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of the job post based on post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>Data Analyst / Data Science Analyst\\nnew</td>\n",
       "      <td>Conduent</td>\n",
       "      <td>2.7</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=3b84a6dcb5263...</td>\n",
       "      <td>About Conduent:\\n\\nThrough our dedicated assoc...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>78230</td>\n",
       "      <td>conduent dedicated associate conduent delivers...</td>\n",
       "      <td>about conduent\\n\\nthrough our dedicated associ...</td>\n",
       "      <td>about conduent through our dedic associ condue...</td>\n",
       "      <td>about conduent through our dedicated associate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>Data Scientist\\nnew</td>\n",
       "      <td>Luminant Generation Company LLC</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=89fc5bb63c4a5...</td>\n",
       "      <td>Key Roles &amp; Responsibilities Other duties may ...</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>key role responsibility duty may assigned dire...</td>\n",
       "      <td>key roles responsibilities other duties may be...</td>\n",
       "      <td>key role respons other duti may be assign dire...</td>\n",
       "      <td>key role responsibility other duty may be assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>Lead Decision Science Analyst – AML (Remote Wo...</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=abdf03ef29c2d...</td>\n",
       "      <td>Purpose of Job\\nWe are currently seeking a tal...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>78288</td>\n",
       "      <td>purpose job currently seeking talented decisio...</td>\n",
       "      <td>purpose of job\\nwe are currently seeking a tal...</td>\n",
       "      <td>purpos of job we are current seek a talent dec...</td>\n",
       "      <td>purpose of job we are currently seeking a tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>Data Scientist - ProServe\\nnew</td>\n",
       "      <td>Amazon Web Services, Inc.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e36df471c42ae...</td>\n",
       "      <td>\\nBachelor’s degree in a highly quantitative f...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>bachelor degree highly quantitative field comp...</td>\n",
       "      <td>bachelors degree in a highly quantitative fiel...</td>\n",
       "      <td>bachelor degre in a highli quantit field compu...</td>\n",
       "      <td>bachelor degree in a highly quantitative field...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>Sr. Data Scientist\\nnew</td>\n",
       "      <td>Amazon Web Services, Inc.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d85b9952344d8...</td>\n",
       "      <td>\\nMasters with 4 years of experience or a Bach...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>master 4 year experience bachelor degree stati...</td>\n",
       "      <td>masters with 4 years of experience or a bachel...</td>\n",
       "      <td>master with 4 year of experi or a bachelor deg...</td>\n",
       "      <td>master with 4 year of experience or a bachelor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                              title  \\\n",
       "0 2021-02-11           Data Analyst / Data Science Analyst\\nnew   \n",
       "1 2021-02-11                                Data Scientist\\nnew   \n",
       "2 2021-02-11  Lead Decision Science Analyst – AML (Remote Wo...   \n",
       "3 2021-02-11                     Data Scientist - ProServe\\nnew   \n",
       "4 2021-02-11                            Sr. Data Scientist\\nnew   \n",
       "\n",
       "                           company  company_rating  \\\n",
       "0                         Conduent             2.7   \n",
       "1  Luminant Generation Company LLC             3.8   \n",
       "2                             USAA             3.9   \n",
       "3        Amazon Web Services, Inc.             3.6   \n",
       "4        Amazon Web Services, Inc.             3.6   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=3b84a6dcb5263...   \n",
       "1  https://www.indeed.com/rc/clk?jk=89fc5bb63c4a5...   \n",
       "2  https://www.indeed.com/rc/clk?jk=abdf03ef29c2d...   \n",
       "3  https://www.indeed.com/rc/clk?jk=e36df471c42ae...   \n",
       "4  https://www.indeed.com/rc/clk?jk=d85b9952344d8...   \n",
       "\n",
       "                                     job_description         city state  \\\n",
       "0  About Conduent:\\n\\nThrough our dedicated assoc...  San Antonio    TX   \n",
       "1  Key Roles & Responsibilities Other duties may ...       Irving    TX   \n",
       "2  Purpose of Job\\nWe are currently seeking a tal...  San Antonio    TX   \n",
       "3  \\nBachelor’s degree in a highly quantitative f...  San Antonio    TX   \n",
       "4  \\nMasters with 4 years of experience or a Bach...       Dallas    TX   \n",
       "\n",
       "   zipcode                                              clean  \\\n",
       "0    78230  conduent dedicated associate conduent delivers...   \n",
       "1        0  key role responsibility duty may assigned dire...   \n",
       "2    78288  purpose job currently seeking talented decisio...   \n",
       "3        0  bachelor degree highly quantitative field comp...   \n",
       "4        0  master 4 year experience bachelor degree stati...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  about conduent\\n\\nthrough our dedicated associ...   \n",
       "1  key roles responsibilities other duties may be...   \n",
       "2  purpose of job\\nwe are currently seeking a tal...   \n",
       "3  bachelors degree in a highly quantitative fiel...   \n",
       "4  masters with 4 years of experience or a bachel...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  about conduent through our dedic associ condue...   \n",
       "1  key role respons other duti may be assign dire...   \n",
       "2  purpos of job we are current seek a talent dec...   \n",
       "3  bachelor degre in a highli quantit field compu...   \n",
       "4  master with 4 year of experi or a bachelor deg...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  about conduent through our dedicated associate...  \n",
       "1  key role responsibility other duty may be assi...  \n",
       "2  purpose of job we are currently seeking a tale...  \n",
       "3  bachelor degree in a highly quantitative field...  \n",
       "4  master with 4 year of experience or a bachelor...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = env_Shi.database\n",
    "df_ds = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "df_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Deveopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 2, 14, 2, 15, 14, tzinfo=tzutc())"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When is the df_wd_tx_prepared.json last modified?\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "object_json = s3.Object('wdpreparedjobpostings', 'df_wd_tx_prepared_backup.json')\n",
    "object_json.last_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load job posts of web developer in TX on Feb.12 2021\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_wd_old = pd.read_csv(f\"{database}web_developer_tx_indeed_021221.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_wd_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_wd_old = transform_old_file(df_wd_old, '2021-02-12')\n",
    "# df_wd_old.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Front End UI Developer\\nnew</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vue Developer, PHP Environment\\nnew</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 title         location               company  \\\n",
       "0          Front End UI Developer\\nnew  San Antonio, TX  Tri-Starr Technology   \n",
       "1  Vue Developer, PHP Environment\\nnew  San Antonio, TX  Tri-Starr Technology   \n",
       "\n",
       "  company_rating     post_age  \\\n",
       "0        missing  Just posted   \n",
       "1        missing  Just posted   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "1  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Tri-Starr Technology is seeking to build relat...  \n",
       "1  Tri-Starr Technology is seeking to build relat...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load web developer job posts in TX on Feb 13 2021\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_wd_new = pd.read_csv(f\"{database}web_developer_tx_indeed_021321.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_wd_new.shape)\n",
    "\n",
    "# Print the first two rows\n",
    "df_wd_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_wd(df):\n",
    "    '''\n",
    "    This function updates job posts of web developer in TX by adding the daily acquring\n",
    "    of web developer job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df_wd_tx = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_jobs = df_wd_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_wd_tx.date = pd.to_datetime(df_wd_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_wd_tx = df_wd_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_wd_tx = pd.concat([df_wd_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_wd_tx = remove_duplicates(df_wd_tx)\n",
    "    # Save as csv file\n",
    "    df_wd_tx.to_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_new_jobs = df_wd_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_wd_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Jobs Posted Today:  119\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Front End UI Developer\\nnew</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Vue Developer, PHP Environment\\nnew</td>\n",
       "      <td>San Antonio, TX</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title         location  \\\n",
       "date                                                               \n",
       "2021-02-13          Front End UI Developer\\nnew  San Antonio, TX   \n",
       "2021-02-13  Vue Developer, PHP Environment\\nnew  San Antonio, TX   \n",
       "\n",
       "                         company company_rating     post_age  \\\n",
       "date                                                           \n",
       "2021-02-13  Tri-Starr Technology        missing  Just posted   \n",
       "2021-02-13  Tri-Starr Technology        missing  Just posted   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-13  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "2021-02-13  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-13  Tri-Starr Technology is seeking to build relat...  \n",
       "2021-02-13  Tri-Starr Technology is seeking to build relat...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function: daily_update_wd\n",
    "\n",
    "df_test = daily_update_wd(df_wd_new)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3476 entries, 2021-02-13 to 2021-01-04 00:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            3476 non-null   object\n",
      " 1   location         3476 non-null   object\n",
      " 2   company          3476 non-null   object\n",
      " 3   company_rating   3476 non-null   object\n",
      " 4   post_age         3476 non-null   object\n",
      " 5   job_link         3476 non-null   object\n",
      " 6   job_description  3476 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 217.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job posts of web developer\n",
    "\n",
    "def prepare_job_posts_indeed_wd():\n",
    "    '''\n",
    "   The function cleans the csv file of web developer job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_wd_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.5 s, sys: 309 ms, total: 39.8 s\n",
      "Wall time: 40 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-13</td>\n",
       "      <td>Front End UI Developer\\nnew</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>tristarr technology seeking build relationship...</td>\n",
       "      <td>tristarr technology is seeking to build relati...</td>\n",
       "      <td>tristarr technolog is seek to build relationsh...</td>\n",
       "      <td>tristarr technology is seeking to build relati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-13</td>\n",
       "      <td>Vue Developer, PHP Environment\\nnew</td>\n",
       "      <td>Tri-Starr Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.indeed.com/company/Tri--Starr-Tech...</td>\n",
       "      <td>Tri-Starr Technology is seeking to build relat...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>tristarr technology seeking build relationship...</td>\n",
       "      <td>tristarr technology is seeking to build relati...</td>\n",
       "      <td>tristarr technolog is seek to build relationsh...</td>\n",
       "      <td>tristarr technology is seeking to build relati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                title               company  \\\n",
       "0  2021-02-13          Front End UI Developer\\nnew  Tri-Starr Technology   \n",
       "1  2021-02-13  Vue Developer, PHP Environment\\nnew  Tri-Starr Technology   \n",
       "\n",
       "  company_rating                                           job_link  \\\n",
       "0              0  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "1              0  https://www.indeed.com/company/Tri--Starr-Tech...   \n",
       "\n",
       "                                     job_description         city state  \\\n",
       "0  Tri-Starr Technology is seeking to build relat...  San Antonio    TX   \n",
       "1  Tri-Starr Technology is seeking to build relat...  San Antonio    TX   \n",
       "\n",
       "  zipcode                                              clean  \\\n",
       "0       0  tristarr technology seeking build relationship...   \n",
       "1       0  tristarr technology seeking build relationship...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  tristarr technology is seeking to build relati...   \n",
       "1  tristarr technology is seeking to build relati...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  tristarr technolog is seek to build relationsh...   \n",
       "1  tristarr technolog is seek to build relationsh...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  tristarr technology is seeking to build relati...  \n",
       "1  tristarr technology is seeking to build relati...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test the function: prepare_job_posts_indeed_wd\n",
    "df_test = prepare_job_posts_indeed_wd()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3476 entries, 0 to 3475\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   date             3476 non-null   object\n",
      " 1   title            3476 non-null   object\n",
      " 2   company          3476 non-null   object\n",
      " 3   company_rating   3476 non-null   object\n",
      " 4   job_link         3476 non-null   object\n",
      " 5   job_description  3476 non-null   object\n",
      " 6   city             3476 non-null   object\n",
      " 7   state            3476 non-null   object\n",
      " 8   zipcode          3476 non-null   object\n",
      " 9   clean            3476 non-null   object\n",
      " 10  tokenized        3476 non-null   object\n",
      " 11  stemmed          3476 non-null   object\n",
      " 12  lemmatized       3476 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 353.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                object\n",
       "title               object\n",
       "company             object\n",
       "company_rating     float16\n",
       "job_link            object\n",
       "job_description     object\n",
       "city                object\n",
       "state               object\n",
       "zipcode              int16\n",
       "clean               object\n",
       "tokenized           object\n",
       "stemmed             object\n",
       "lemmatized          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the data types\n",
    "\n",
    "dtypes = {'company_rating': 'float16', \n",
    "          'zipcode': 'int16'}\n",
    "df_test.astype(dtypes).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "\n",
    "result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "parsed = json.load(result)\n",
    "parsed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 1.27 s, total: 2.79 s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Upload the json file to AWS\n",
    "\n",
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket('wdrawjobpostings').upload_file(f\"{database}df_wd_tx_backup.csv\", \"df_wd_tx_backup.csv\")\n",
    "\n",
    "# Upload df_ds_tx_prepared_backup.json file\n",
    "s3.Bucket('wdpreparedjobpostings').upload_file(f\"{database}df_wd_tx_prepared_backup.json\", \n",
    "                                               \"df_wd_tx_prepared_backup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 2, 13, 22, 38, 7, tzinfo=tzutc())"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the time when the prepared json file is last modified\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "prepared_json = s3.Object('dspreparedjobpostings', 'df_ds_tx_prepared_backup.json')\n",
    "prepared_json.last_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load old data scientist job posts in TX\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_ds_old = pd.read_csv(f\"{database}data_scientist_tx_indeed_020821.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_ds_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_test = transform_old_file(df_ds_old, '2021-02-08')\n",
    "# df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5dcee53399304...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist III\\nnew</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>DCP Midstream</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=995b42aedf08b...</td>\n",
       "      <td>Work with data to identify patterns, uses judg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     location  \\\n",
       "0  Risk - Corporate Risk - Wholesale Credit Solut...    Plano, TX   \n",
       "1                            Data Scientist III\\nnew  Houston, TX   \n",
       "\n",
       "                     company company_rating     post_age  \\\n",
       "0  JPMorgan Chase Bank, N.A.            3.9  Just posted   \n",
       "1              DCP Midstream            3.5        Today   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=5dcee53399304...   \n",
       "1  https://www.indeed.com/rc/clk?jk=995b42aedf08b...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  \n",
       "1  Work with data to identify patterns, uses judg...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data scientist job posts in TX on 2021-02-13\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_ds_new = pd.read_csv(f\"{database}data_scientist_tx_indeed_021321.csv\", index_col=0)\n",
    "\n",
    "# Inspect the first 2 rows of the new posts\n",
    "df_ds_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5dcee53399304...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>North America Market Specialist, Community Ope...</td>\n",
       "      <td>Austin, TX (Downtown area)</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=31a5deea02dbb...</td>\n",
       "      <td>Global Operations' focus is on supporting our ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-13  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "2021-02-13  North America Market Specialist, Community Ope...   \n",
       "\n",
       "                              location                    company  \\\n",
       "date                                                                \n",
       "2021-02-13                   Plano, TX  JPMorgan Chase Bank, N.A.   \n",
       "2021-02-13  Austin, TX (Downtown area)                   Facebook   \n",
       "\n",
       "           company_rating     post_age  \\\n",
       "date                                     \n",
       "2021-02-13            3.9  Just posted   \n",
       "2021-02-13            4.2        Today   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=5dcee53399304...   \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=31a5deea02dbb...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-13  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  \n",
       "2021-02-13  Global Operations' focus is on supporting our ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: compute_post_date\n",
    "\n",
    "df_test = compute_post_date(df_ds_new)\n",
    "df_test.head(2) # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_ds(df):\n",
    "    '''\n",
    "    This function updates job posts of data scientist in TX by adding the daily acquring\n",
    "    of data scientist job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df_ds_tx = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    num_jobs = df_ds_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_ds_tx = pd.concat([df_ds_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_ds_tx = remove_duplicates(df_ds_tx)\n",
    "    # Save as csv file\n",
    "    df_ds_tx.to_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Print the new jobs posted today\n",
    "    num_new_jobs = df_ds_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_ds_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Jobs Posted Today:  61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>North America Market Specialist, Community Ope...</td>\n",
       "      <td>Austin, TX (Downtown area)</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=31a5deea02dbb...</td>\n",
       "      <td>Global Operations' focus is on supporting our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Auto Actuary Data Science\\nnew</td>\n",
       "      <td>Dallas, TX 75215</td>\n",
       "      <td>Smith Hanley Associates</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Title: Data Science Actuary Auto\\n\\nOne of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5dcee53399304...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Data Engineer, Apple Media Products Data Scien...</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Apple</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5e6423045c035...</td>\n",
       "      <td>Summary\\nPosted: Feb 12, 2021\\nRole Number:200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-13</th>\n",
       "      <td>Sr. Data &amp; ML Engineer\\nnew</td>\n",
       "      <td>Schertz, TX</td>\n",
       "      <td>Amazon Web Services, Inc.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=c93e40b655dd8...</td>\n",
       "      <td>\\nBachelor’s degree in Computer Science, Engin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-13  North America Market Specialist, Community Ope...   \n",
       "2021-02-13                     Auto Actuary Data Science\\nnew   \n",
       "2021-02-13  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "2021-02-13  Data Engineer, Apple Media Products Data Scien...   \n",
       "2021-02-13                        Sr. Data & ML Engineer\\nnew   \n",
       "\n",
       "                              location                    company  \\\n",
       "date                                                                \n",
       "2021-02-13  Austin, TX (Downtown area)                   Facebook   \n",
       "2021-02-13            Dallas, TX 75215    Smith Hanley Associates   \n",
       "2021-02-13                   Plano, TX  JPMorgan Chase Bank, N.A.   \n",
       "2021-02-13                  Austin, TX                      Apple   \n",
       "2021-02-13                 Schertz, TX  Amazon Web Services, Inc.   \n",
       "\n",
       "           company_rating     post_age  \\\n",
       "date                                     \n",
       "2021-02-13            4.2        Today   \n",
       "2021-02-13            4.1        Today   \n",
       "2021-02-13            3.9  Just posted   \n",
       "2021-02-13            4.2        Today   \n",
       "2021-02-13            3.6        Today   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=31a5deea02dbb...   \n",
       "2021-02-13  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=5dcee53399304...   \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=5e6423045c035...   \n",
       "2021-02-13  https://www.indeed.com/rc/clk?jk=c93e40b655dd8...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-13  Global Operations' focus is on supporting our ...  \n",
       "2021-02-13  Title: Data Science Actuary Auto\\n\\nOne of the...  \n",
       "2021-02-13  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  \n",
       "2021-02-13  Summary\\nPosted: Feb 12, 2021\\nRole Number:200...  \n",
       "2021-02-13  \\nBachelor’s degree in Computer Science, Engin...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: daily_update_ds\n",
    "\n",
    "df_test = daily_update_ds(df_ds_new)\n",
    "df_test.head() # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1846 entries, 2021-02-13 to 2020-12-22 00:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            1846 non-null   object\n",
      " 1   location         1846 non-null   object\n",
      " 2   company          1846 non-null   object\n",
      " 3   company_rating   1846 non-null   object\n",
      " 4   post_age         1846 non-null   object\n",
      " 5   job_link         1846 non-null   object\n",
      " 6   job_description  1846 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 115.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the dateframe\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job post for exploration\n",
    "\n",
    "def prepare_job_posts_indeed_ds():\n",
    "    '''\n",
    "    The function cleans the csv file of data scientist job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_ds_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 s, sys: 320 ms, total: 28.5 s\n",
      "Wall time: 28.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-13</td>\n",
       "      <td>North America Market Specialist, Community Ope...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=31a5deea02dbb...</td>\n",
       "      <td>Global Operations' focus is on supporting our ...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>global operation focus supporting user direct ...</td>\n",
       "      <td>global operations focus is on supporting our u...</td>\n",
       "      <td>global oper focu is on support our user throug...</td>\n",
       "      <td>global operation focus is on supporting our us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-13</td>\n",
       "      <td>Auto Actuary Data Science\\nnew</td>\n",
       "      <td>Smith Hanley Associates</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Title: Data Science Actuary Auto\\n\\nOne of the...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75215</td>\n",
       "      <td>title data science actuary auto one country fa...</td>\n",
       "      <td>title data science actuary auto\\n\\none of the ...</td>\n",
       "      <td>titl data scienc actuari auto one of the count...</td>\n",
       "      <td>title data science actuary auto one of the cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2021-02-13  North America Market Specialist, Community Ope...   \n",
       "1  2021-02-13                     Auto Actuary Data Science\\nnew   \n",
       "\n",
       "                   company company_rating  \\\n",
       "0                 Facebook            4.2   \n",
       "1  Smith Hanley Associates            4.1   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=31a5deea02dbb...   \n",
       "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                     job_description    city state zipcode  \\\n",
       "0  Global Operations' focus is on supporting our ...  Austin    TX       0   \n",
       "1  Title: Data Science Actuary Auto\\n\\nOne of the...  Dallas    TX   75215   \n",
       "\n",
       "                                               clean  \\\n",
       "0  global operation focus supporting user direct ...   \n",
       "1  title data science actuary auto one country fa...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  global operations focus is on supporting our u...   \n",
       "1  title data science actuary auto\\n\\none of the ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  global oper focu is on support our user throug...   \n",
       "1  titl data scienc actuari auto one of the count...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  global operation focus is on supporting our us...  \n",
       "1  title data science actuary auto one of the cou...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test the function: prepare_job_posts_indeed\n",
    "\n",
    "df_test = prepare_job_posts_indeed_ds()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1846 entries, 0 to 1845\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   date             1846 non-null   object\n",
      " 1   title            1846 non-null   object\n",
      " 2   company          1846 non-null   object\n",
      " 3   company_rating   1846 non-null   object\n",
      " 4   job_link         1846 non-null   object\n",
      " 5   job_description  1846 non-null   object\n",
      " 6   city             1846 non-null   object\n",
      " 7   state            1846 non-null   object\n",
      " 8   zipcode          1846 non-null   object\n",
      " 9   clean            1846 non-null   object\n",
      " 10  tokenized        1846 non-null   object\n",
      " 11  stemmed          1846 non-null   object\n",
      " 12  lemmatized       1846 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 187.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Files to AWS S3 Bucket\n",
    "- Up-to-date job postings of data scientist positions in TX\n",
    "- Cleaned job postings of data scientist positions in TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket('dsrawjobpostings').upload_file(f\"{database}df_ds_tx_backup.csv\", \"df_ds_tx_backup.csv\")\n",
    "\n",
    "# Upload df_ds_tx_prepared_backup.json file\n",
    "s3.Bucket('dspreparedjobpostings').upload_file(f\"{database}df_ds_tx_prepared_backup.json\", \n",
    "                                               \"df_ds_tx_prepared_backup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: The two files have been successfully uploaded to the designated bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downlaod Files from AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'boto3.resources.factory.s3.ServiceResource'>\n",
      "additionaljobinfo\n",
      "amplify-jobdashboardfront-dev-180611-deployment\n",
      "dspreparedjobpostings\n",
      "dsrawjobpostings\n",
      "sagemaker-jobsearchdashboard\n",
      "sagemaker-studio-793555146825-q0wiagiqq8r\n",
      "wdpreparedjobpostings\n",
      "wdrawjobpostings\n"
     ]
    }
   ],
   "source": [
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print the data type of s3\n",
    "print(type(s3))\n",
    "\n",
    "# Print the bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.factory.s3.Bucket"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bucket object\n",
    "ds_raw = s3.Bucket('dsrawjobpostings')\n",
    "\n",
    "# Print the data type of ds_raw\n",
    "type(ds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ds_tx.csv\n",
      "df_ds_tx_backup.csv\n",
      "ds_tx_indeed_02052021.csv\n"
     ]
    }
   ],
   "source": [
    "# List all the files inside the bucket\n",
    "\n",
    "for page in ds_raw.objects.pages():\n",
    "    for obj in page:\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file\n",
    "database = env_Shi.database\n",
    "s3.Bucket('dsrawjobpostings').download_file('df_ds_tx_backup.csv', \n",
    "                                            f\"{database}df_ds_tx_aws.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: The file is successfully downloaded and saved in the customized location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Job Post:  1597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Director, Data Science\\nnew</td>\n",
       "      <td>EmployBridge, LLC.</td>\n",
       "      <td>3.2</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=1f65699388974...</td>\n",
       "      <td>Your Opportunity:\\n\\nDevelop a team and mentor...</td>\n",
       "      <td>Farmers Branch</td>\n",
       "      <td>TX</td>\n",
       "      <td>75234</td>\n",
       "      <td>opportunity develop team mentor manager develo...</td>\n",
       "      <td>your opportunity\\n\\ndevelop a team and mentor ...</td>\n",
       "      <td>your opportun develop a team and mentor manag ...</td>\n",
       "      <td>your opportunity develop a team and mentor man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Associate, Data Scientist, Intelligent Forecas...</td>\n",
       "      <td>KPMG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5980735216bf6...</td>\n",
       "      <td>Innovate. Collaborate. Build. Create. Solve. T...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>innovate collaborate build create solve kpmg d...</td>\n",
       "      <td>innovate collaborate build create solve the kp...</td>\n",
       "      <td>innov collabor build creat solv the kpmg digit...</td>\n",
       "      <td>innovate collaborate build create solve the kp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Data Scientist/Machine Learning Engineer\\nnew</td>\n",
       "      <td>ConnectedX Inc.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.indeed.com/company/Connectedx,-Inc...</td>\n",
       "      <td>Machine Learning/ Data ScientistLocation: Plan...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75207</td>\n",
       "      <td>machine learning data scientistlocation plano ...</td>\n",
       "      <td>machine learning data scientistlocation plano ...</td>\n",
       "      <td>machin learn data scientistloc plano txdurat l...</td>\n",
       "      <td>machine learning data scientistlocation plano ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Head of Cancer Artificial Intelligence\\nnew</td>\n",
       "      <td>Larvol</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.indeed.com/company/The-Larvol-Grou...</td>\n",
       "      <td>Head of Cancer Artificial IntelligenceFull-tim...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>head cancer artificial intelligencefulltime 10...</td>\n",
       "      <td>head of cancer artificial intelligencefulltime...</td>\n",
       "      <td>head of cancer artifici intelligencefulltim 10...</td>\n",
       "      <td>head of cancer artificial intelligencefulltime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Applied Researcher, NLP\\nnew</td>\n",
       "      <td>eBay Inc.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=f1c17d175e718...</td>\n",
       "      <td>NLP Applied Researcher, Job Description\\nDo yo...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>nlp applied researcher job description want hu...</td>\n",
       "      <td>nlp applied researcher job description\\ndo you...</td>\n",
       "      <td>nlp appli research job descript do you want to...</td>\n",
       "      <td>nlp applied researcher job description do you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-09                        Director, Data Science\\nnew   \n",
       "2021-02-09  Associate, Data Scientist, Intelligent Forecas...   \n",
       "2021-02-09      Data Scientist/Machine Learning Engineer\\nnew   \n",
       "2021-02-09        Head of Cancer Artificial Intelligence\\nnew   \n",
       "2021-02-09                       Applied Researcher, NLP\\nnew   \n",
       "\n",
       "                       company  company_rating  \\\n",
       "date                                             \n",
       "2021-02-09  EmployBridge, LLC.             3.2   \n",
       "2021-02-09                KPMG             4.0   \n",
       "2021-02-09     ConnectedX Inc.             0.0   \n",
       "2021-02-09              Larvol             0.0   \n",
       "2021-02-09           eBay Inc.             3.9   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-09  https://www.indeed.com/rc/clk?jk=1f65699388974...   \n",
       "2021-02-09  https://www.indeed.com/rc/clk?jk=5980735216bf6...   \n",
       "2021-02-09  https://www.indeed.com/company/Connectedx,-Inc...   \n",
       "2021-02-09  https://www.indeed.com/company/The-Larvol-Grou...   \n",
       "2021-02-09  https://www.indeed.com/rc/clk?jk=f1c17d175e718...   \n",
       "\n",
       "                                              job_description            city  \\\n",
       "date                                                                            \n",
       "2021-02-09  Your Opportunity:\\n\\nDevelop a team and mentor...  Farmers Branch   \n",
       "2021-02-09  Innovate. Collaborate. Build. Create. Solve. T...          Dallas   \n",
       "2021-02-09  Machine Learning/ Data ScientistLocation: Plan...          Dallas   \n",
       "2021-02-09  Head of Cancer Artificial IntelligenceFull-tim...          Austin   \n",
       "2021-02-09  NLP Applied Researcher, Job Description\\nDo yo...          Austin   \n",
       "\n",
       "           state  zipcode                                              clean  \\\n",
       "date                                                                           \n",
       "2021-02-09    TX    75234  opportunity develop team mentor manager develo...   \n",
       "2021-02-09    TX        0  innovate collaborate build create solve kpmg d...   \n",
       "2021-02-09    TX    75207  machine learning data scientistlocation plano ...   \n",
       "2021-02-09    TX        0  head cancer artificial intelligencefulltime 10...   \n",
       "2021-02-09    TX        0  nlp applied researcher job description want hu...   \n",
       "\n",
       "                                                    tokenized  \\\n",
       "date                                                            \n",
       "2021-02-09  your opportunity\\n\\ndevelop a team and mentor ...   \n",
       "2021-02-09  innovate collaborate build create solve the kp...   \n",
       "2021-02-09  machine learning data scientistlocation plano ...   \n",
       "2021-02-09  head of cancer artificial intelligencefulltime...   \n",
       "2021-02-09  nlp applied researcher job description\\ndo you...   \n",
       "\n",
       "                                                      stemmed  \\\n",
       "date                                                            \n",
       "2021-02-09  your opportun develop a team and mentor manag ...   \n",
       "2021-02-09  innov collabor build creat solv the kpmg digit...   \n",
       "2021-02-09  machin learn data scientistloc plano txdurat l...   \n",
       "2021-02-09  head of cancer artifici intelligencefulltim 10...   \n",
       "2021-02-09  nlp appli research job descript do you want to...   \n",
       "\n",
       "                                                   lemmatized  \n",
       "date                                                           \n",
       "2021-02-09  your opportunity develop a team and mentor man...  \n",
       "2021-02-09  innovate collaborate build create solve the kp...  \n",
       "2021-02-09  machine learning data scientistlocation plano ...  \n",
       "2021-02-09  head of cancer artificial intelligencefulltime...  \n",
       "2021-02-09  nlp applied researcher job description do you ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "df_ds_tx = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the number of job posts\n",
    "print(\"Number of Job Post: \", df_ds_tx.shape[0])\n",
    "\n",
    "# Conver the string date to datetime object\n",
    "df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "\n",
    "# Set the date as the index and sort the dataframe in descending order\n",
    "df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1597 entries, 2021-02-09 to 2020-12-22\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            1597 non-null   object \n",
      " 1   company          1597 non-null   object \n",
      " 2   company_rating   1597 non-null   float64\n",
      " 3   job_link         1597 non-null   object \n",
      " 4   job_description  1597 non-null   object \n",
      " 5   city             1597 non-null   object \n",
      " 6   state            1597 non-null   object \n",
      " 7   zipcode          1597 non-null   int64  \n",
      " 8   clean            1597 non-null   object \n",
      " 9   tokenized        1597 non-null   object \n",
      " 10  stemmed          1597 non-null   object \n",
      " 11  lemmatized       1597 non-null   object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 162.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the df_ds_tx\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cognizant Technology Solutions    57\n",
       "Dell Technologies                 41\n",
       "Facebook                          34\n",
       "Deloitte                          32\n",
       "USAA                              31\n",
       "StataCorp                         24\n",
       "KPMG                              20\n",
       "JPMorgan Chase Bank, N.A.         20\n",
       "Advanced Micro Devices, Inc.      18\n",
       "Apple                             18\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 companies by the number of posts\n",
    "df_ds_tx.company.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin         450\n",
       "Dallas         261\n",
       "Houston        203\n",
       "Plano          124\n",
       "San Antonio    122\n",
       "Irving          86\n",
       "0               78\n",
       "Fort Worth      41\n",
       "Round Rock      37\n",
       "Frisco          25\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 cities by the number of posts\n",
    "df_ds_tx.city.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-12-27    392\n",
       "2021-01-03    136\n",
       "2021-01-10    212\n",
       "2021-01-17    130\n",
       "2021-01-24    302\n",
       "2021-01-31    258\n",
       "2021-02-07    143\n",
       "2021-02-14     24\n",
       "Freq: W-SUN, Name: title, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: the dataframe has datetime index\n",
    "df_ds_tx.resample(\"W\").title.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Job Requirements by Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random job link\n",
    "\n",
    "job_url = df_ds.job_link.sample(1, random_state=1)[0]\n",
    "job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rquest\n",
    "\n",
    "response = requests.get(job_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a soup to hold the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of all the words appear in the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frequency'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opportunity develop team mentor manager develop innovative data science solution utilize machine lea'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v1\n",
    "dic = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic.keys())\n",
    "\n",
    "# Print the first 100 characters of the value\n",
    "dic['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the helper function: words_variables_v2\n",
    "\n",
    "companies = ['Apple']\n",
    "dic_v2 = words_variables_v2(df_ds_tx, companies)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic_v2.keys())\n",
    "\n",
    "# Print the first 100 characters of the value of `Apple`\n",
    "dic_v2['Apple'][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(d_words['frequency'].split()).value_counts()\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = d_words.keys()\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade `word_frequency_v1`\n",
    "\n",
    "def word_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Added 'Bigram' column to dataframe\n",
    "# df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "# df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 2))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 3))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Mono-, Bi- and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Simple concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2:  Use nltk.util.everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the frequence of the mono-, bi-, and tri-grams of the job description\n",
    "\n",
    "def everygram_frequency_v1(d_words, max_len=3):\n",
    "    '''\n",
    "    This function accetps the dictionary produced by the function `words_variables_v1` and \n",
    "    return mono-, bi-, and tri-grams along with their frequency. \n",
    "    '''\n",
    "    # Generate mono-, bi-, and tri-grams\n",
    "    grams = nltk.everygrams(d_words['frequency'].split(), max_len=max_len) # dtype of grams: <class 'genertor'>\n",
    "    # Convert to a list of tuples\n",
    "    grams = list(grams)\n",
    "    # Create an empty list to hold mono-, bi-, and tri-grams\n",
    "    everygram = []\n",
    "    # For loop the list of tuples and convert the grams to strings\n",
    "    for gram in grams:\n",
    "        str_gram = gram[0]\n",
    "        for i in gram[1:]:\n",
    "            str_gram = str_gram + ' ' + i\n",
    "        everygram.append(str_gram)\n",
    "    # Compute the frequency of the everygrams\n",
    "    everygram = pd.Series(everygram).value_counts()\n",
    "    return everygram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          16501\n",
       "experience     9129\n",
       "business       5749\n",
       "team           5112\n",
       "work           4516\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function above\n",
    "\n",
    "everygram = everygram_frequency_v1(dic)\n",
    "everygram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ds_grams as json file\n",
    "everygram.to_json(f\"{database}ds_grams.json\", orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploade to AWS s3 bucket\n",
    "\n",
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket('additionaljobinfo').upload_file(f\"{database}ds_grams.json\", \"ds_grams.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pick up the top k skills of a data scientict from a skillset library\n",
    "\n",
    "def top_skills_ds_v1(k, library):\n",
    "    '''\n",
    "    This function accepts a positive integer k and a skillset library and \n",
    "    returns a dataframe containing the top k skills needed for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    everygram_frequency = everygram_frequency_v1(dic)\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = ( everygram_frequency.index == skill)\n",
    "        df =  everygram_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.columns = dic.keys()\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in tech skill library:  63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine learning</th>\n",
       "      <td>2521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>1329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>1012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aws</th>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big data</th>\n",
       "      <td>622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark</th>\n",
       "      <td>569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop</th>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agile</th>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep learning</th>\n",
       "      <td>386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tableau</th>\n",
       "      <td>374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azure</th>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data visualization</th>\n",
       "      <td>358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data mining</th>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etl</th>\n",
       "      <td>307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dashboard</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    frequency\n",
       "machine learning       2521.0\n",
       "python                 1329.0\n",
       "sql                    1012.0\n",
       "r                       760.0\n",
       "aws                     689.0\n",
       "big data                622.0\n",
       "spark                   569.0\n",
       "hadoop                  539.0\n",
       "c                       442.0\n",
       "java                    439.0\n",
       "agile                   393.0\n",
       "deep learning           386.0\n",
       "tableau                 374.0\n",
       "azure                   372.0\n",
       "data visualization      358.0\n",
       "data mining             349.0\n",
       "etl                     307.0\n",
       "dashboard               268.0\n",
       "math                    266.0\n",
       "nlp                     266.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tech library\n",
    "tech_library = ['python', 'sql', 'pandas','numpy','matplotlib','scikit learn','spark','hadoop',\n",
    "           'aws','amazon web services','azure','microsoft word','microsoft excel','excel',\n",
    "           'tableau','tensor flow','pytorch','hive', 'impala', 'matlab','etl',\n",
    "           'statistics','exploration', 'extraction', 'data wrangling','math',\n",
    "           'machine learning','data visualization','java','js',\n",
    "           'javascript','scala','r','c','c++','power bi','dashboard','linear algebra',\n",
    "           'calculus','neural networks','eda','big data','frameworks','database management',\n",
    "           'testing hypotheses','probability','data mining','perl','nosql','saas','git',\n",
    "           'github','natural language processing','nlp','deep learning','agile','kanban',\n",
    "           'project management','julia','devops','google cloud','pytorch','computer vision']\n",
    "\n",
    "# Print the number of skills in the library\n",
    "print(\"Number of skills in tech skill library: \", len(tech_library))\n",
    "\n",
    "# Test function: top_skills_ds_v1\n",
    "df_test = top_skills_ds_v1(20, tech_library)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>3576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communication</th>\n",
       "      <td>1122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leadership</th>\n",
       "      <td>707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collaboration</th>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem solving</th>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>written communication</th>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision making</th>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbal communication</th>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity</th>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team player</th>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time management</th>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business acumen</th>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critical thinking</th>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teamwork</th>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storytelling</th>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain knowledge</th>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaptability</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       frequency\n",
       "learning                  3576.0\n",
       "communication             1122.0\n",
       "leadership                 707.0\n",
       "collaboration              348.0\n",
       "problem solving            244.0\n",
       "written communication      199.0\n",
       "decision making            146.0\n",
       "verbal communication       109.0\n",
       "curiosity                   93.0\n",
       "creativity                  91.0\n",
       "team player                 84.0\n",
       "time management             79.0\n",
       "business acumen             73.0\n",
       "critical thinking           70.0\n",
       "teamwork                    53.0\n",
       "storytelling                17.0\n",
       "domain knowledge            16.0\n",
       "adaptability                12.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick up the top 20 soft skills\n",
    "soft_library = ['critical thinking','communication','problem solving','teamwork','ethics','business acumen',\n",
    "           'interpersonal skills','curiosity','storytelling','adaptability','team player','collaboration',\n",
    "                'time management','leadership','domain knowledge','creativity','decision making',\n",
    "           'verbal communication','written communication']\n",
    "\n",
    "# Print the number of skills in the library\n",
    "print(\"Number of skills in soft skill library: \", len(soft_library))\n",
    "\n",
    "top_soft_skills = top_skills_ds_v1(20, soft_library)\n",
    "top_soft_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'python')\n",
    "df_word_frequency[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'r')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'aws')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'sql')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_match():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
