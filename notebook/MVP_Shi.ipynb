{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "url = first_page_url_indeed('data scientist', 'al')\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "type(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "print(\"Data Type: \", type(num_jobs))\n",
    "print(\"Name of the Tag: \", num_jobs.name)\n",
    "print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "print(\"Text within the Tag: \")\n",
    "num_jobs.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of the jobs in the text\n",
    "match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function num_jobs_indeed\n",
    "num_jobs_indeed(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function num_jobs_indeed\n",
    "page_num_indeed(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function job_cards_indeed\n",
    "job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# Print the data type of job_cards\n",
    "type(job_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Note**: job_cards is an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many jobs listed in the 1st page? \n",
    "len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = job_titles_indeed(job_cards)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: comany_names_indeed\n",
    "company_names = company_names_indeed(job_cards)\n",
    "company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: post_ages_indeed\n",
    "ages = post_ages_indeed(job_cards)\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: job_locations_indeed\n",
    "locations = job_locations_indeed(job_cards)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = company_rating_indeed(job_cards)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: job_links_and_contents_indeed\n",
    "links, descriptions = job_links_and_contents_indeed(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: page_soup_indeed\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "soup = page_soup_indeed(url)\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the page number\n",
    "int(page_num_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the job cards from the soup\n",
    "type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function acquire_page_indeed\n",
    "page_num, df = acquire_page_indeed(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location):\n",
    "    '''\n",
    "    This function accepts the job title and location and return \n",
    "    the job information pull from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <=35:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date of the job posts\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of the job post based on post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Deveopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load web developer job posts in TX today\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_wd_new = pd.read_csv(f\"{database}web_developer_tx_indeed_020721.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_wd_new.shape)\n",
    "\n",
    "# Print the first two rows\n",
    "df_wd_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_wd(df):\n",
    "    '''\n",
    "    This function updates job posts of web developer in TX by adding the daily acquring\n",
    "    of web developer job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df_wd_tx = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_jobs = df_wd_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_wd_tx.date = pd.to_datetime(df_wd_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_wd_tx = df_wd_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_wd_tx = pd.concat([df_wd_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_wd_tx = remove_duplicates(df_wd_tx)\n",
    "    # Save as csv file\n",
    "    df_wd_tx.to_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_new_jobs = df_wd_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_wd_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: daily_update_wd\n",
    "\n",
    "df_test = daily_update_wd(df_wd_new)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job posts of web developer\n",
    "\n",
    "def prepare_job_posts_indeed_wd():\n",
    "    '''\n",
    "   The function cleans the csv file of web developer job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_wd_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Test the function: prepare_job_posts_indeed_wd\n",
    "df_test = prepare_job_posts_indeed_wd()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "\n",
    "result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load old data scientist job posts in TX\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_ds_old = pd.read_csv(f\"{database}data_scientist_tx_indeed_020221.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_ds_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "\n",
    "# df_test = transform_old_file(df_ds_old, '2021-02-02')\n",
    "# df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data scientist job posts in TX on 2021-02-03\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_ds_new = pd.read_csv(f\"{database}data_scientist_tx_indeed_020621.csv\", index_col=0)\n",
    "\n",
    "# Inspect the first 2 rows of the new posts\n",
    "df_ds_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: compute_post_date\n",
    "\n",
    "df_test = compute_post_date(df_ds_new)\n",
    "df_test.head(2) # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_ds(df):\n",
    "    '''\n",
    "    This function updates job posts of data scientist in TX by adding the daily acquring\n",
    "    of data scientist job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df_ds_tx = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    num_jobs = df_ds_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_ds_tx = pd.concat([df_ds_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_ds_tx = remove_duplicates(df_ds_tx)\n",
    "    # Save as csv file\n",
    "    df_ds_tx.to_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Print the new jobs posted today\n",
    "    num_new_jobs = df_ds_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_ds_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test the function: daily_update_ds\n",
    "\n",
    "df_test = daily_update_ds(df_ds_new)\n",
    "df_test.head() # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of the dateframe\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job post for exploration\n",
    "\n",
    "def prepare_job_posts_indeed_ds():\n",
    "    '''\n",
    "    The function cleans the csv file of data scientist job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_ds_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test the function: prepare_job_posts_indeed\n",
    "\n",
    "df_test = prepare_job_posts_indeed_ds()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "result = open(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the type of the file\n",
    "print(type(result))\n",
    "\n",
    "# \n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print the data type of s3\n",
    "print(type(s3))\n",
    "\n",
    "# Print the bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bucket object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Job Post:  1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Wholesale Payments - Small Business Data Scien...</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=fbd85e1549e4f...</td>\n",
       "      <td>CIB Wholesale Payments Data and Analytics - Da...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>cib wholesale payment data analytics data scie...</td>\n",
       "      <td>cib wholesale payments data and analytics data...</td>\n",
       "      <td>cib wholesal payment data and analyt data scie...</td>\n",
       "      <td>cib wholesale payment data and analytics data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Data Scientist\\nnew</td>\n",
       "      <td>Vistra Corporate Services Company</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d96310969eba0...</td>\n",
       "      <td>Responsibilities\\n• A person in this role is e...</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>responsibility person role expected execute en...</td>\n",
       "      <td>responsibilities\\n a person in this role is ex...</td>\n",
       "      <td>respons a person in thi role is expect to exec...</td>\n",
       "      <td>responsibility a person in this role is expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Manager, Data Science\\nnew</td>\n",
       "      <td>Dell Technologies</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=13daea469fd59...</td>\n",
       "      <td>Senior Manager, Data Science (Round Rock TX or...</td>\n",
       "      <td>Round Rock</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>senior manager data science round rock tx remo...</td>\n",
       "      <td>senior manager data science round rock tx or r...</td>\n",
       "      <td>senior manag data scienc round rock tx or remo...</td>\n",
       "      <td>senior manager data science round rock tx or r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Data Analyst - US Remote\\nnew</td>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=81c33357c602f...</td>\n",
       "      <td>Welcome to one of the toughest and most fulfil...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>welcome one toughest fulfilling way help peopl...</td>\n",
       "      <td>welcome to one of the toughest and most fulfil...</td>\n",
       "      <td>welcom to one of the toughest and most fulfil ...</td>\n",
       "      <td>welcome to one of the toughest and most fulfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Sr. Machine Learning Scientist\\nnew</td>\n",
       "      <td>Amazon.com Services LLC</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2541f249fc43d...</td>\n",
       "      <td>\\nMS or PhD in Artificial Intelligence, Comput...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>m phd artificial intelligence computer science...</td>\n",
       "      <td>ms or phd in artificial intelligence computer ...</td>\n",
       "      <td>ms or phd in artifici intellig comput scienc m...</td>\n",
       "      <td>m or phd in artificial intelligence computer s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-06  Wholesale Payments - Small Business Data Scien...   \n",
       "2021-02-06                         Senior Data Scientist\\nnew   \n",
       "2021-02-06                  Senior Manager, Data Science\\nnew   \n",
       "2021-02-06               Senior Data Analyst - US Remote\\nnew   \n",
       "2021-02-06                Sr. Machine Learning Scientist\\nnew   \n",
       "\n",
       "                                      company  company_rating  \\\n",
       "date                                                            \n",
       "2021-02-06          JPMorgan Chase Bank, N.A.             3.9   \n",
       "2021-02-06  Vistra Corporate Services Company             3.8   \n",
       "2021-02-06                  Dell Technologies             4.0   \n",
       "2021-02-06                 UnitedHealth Group             3.7   \n",
       "2021-02-06            Amazon.com Services LLC             3.6   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=fbd85e1549e4f...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=d96310969eba0...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=13daea469fd59...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=81c33357c602f...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=2541f249fc43d...   \n",
       "\n",
       "                                              job_description        city  \\\n",
       "date                                                                        \n",
       "2021-02-06  CIB Wholesale Payments Data and Analytics - Da...       Plano   \n",
       "2021-02-06  Responsibilities\\n• A person in this role is e...      Irving   \n",
       "2021-02-06  Senior Manager, Data Science (Round Rock TX or...  Round Rock   \n",
       "2021-02-06  Welcome to one of the toughest and most fulfil...      Dallas   \n",
       "2021-02-06  \\nMS or PhD in Artificial Intelligence, Comput...      Austin   \n",
       "\n",
       "           state  zipcode                                              clean  \\\n",
       "date                                                                           \n",
       "2021-02-06    TX        0  cib wholesale payment data analytics data scie...   \n",
       "2021-02-06    TX        0  responsibility person role expected execute en...   \n",
       "2021-02-06    TX        0  senior manager data science round rock tx remo...   \n",
       "2021-02-06    TX    75202  welcome one toughest fulfilling way help peopl...   \n",
       "2021-02-06    TX        0  m phd artificial intelligence computer science...   \n",
       "\n",
       "                                                    tokenized  \\\n",
       "date                                                            \n",
       "2021-02-06  cib wholesale payments data and analytics data...   \n",
       "2021-02-06  responsibilities\\n a person in this role is ex...   \n",
       "2021-02-06  senior manager data science round rock tx or r...   \n",
       "2021-02-06  welcome to one of the toughest and most fulfil...   \n",
       "2021-02-06  ms or phd in artificial intelligence computer ...   \n",
       "\n",
       "                                                      stemmed  \\\n",
       "date                                                            \n",
       "2021-02-06  cib wholesal payment data and analyt data scie...   \n",
       "2021-02-06  respons a person in thi role is expect to exec...   \n",
       "2021-02-06  senior manag data scienc round rock tx or remo...   \n",
       "2021-02-06  welcom to one of the toughest and most fulfil ...   \n",
       "2021-02-06  ms or phd in artifici intellig comput scienc m...   \n",
       "\n",
       "                                                   lemmatized  \n",
       "date                                                           \n",
       "2021-02-06  cib wholesale payment data and analytics data ...  \n",
       "2021-02-06  responsibility a person in this role is expect...  \n",
       "2021-02-06  senior manager data science round rock tx or r...  \n",
       "2021-02-06  welcome to one of the toughest and most fulfil...  \n",
       "2021-02-06  m or phd in artificial intelligence computer s...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "df_ds_tx = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the number of job posts\n",
    "print(\"Number of Job Post: \", df_ds_tx.shape[0])\n",
    "\n",
    "# Conver the string date to datetime object\n",
    "df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "\n",
    "# Set the date as the index and sort the dataframe in descending order\n",
    "df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1430 entries, 2021-02-06 to 2020-12-22\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            1430 non-null   object \n",
      " 1   company          1430 non-null   object \n",
      " 2   company_rating   1430 non-null   float64\n",
      " 3   job_link         1430 non-null   object \n",
      " 4   job_description  1430 non-null   object \n",
      " 5   city             1430 non-null   object \n",
      " 6   state            1430 non-null   object \n",
      " 7   zipcode          1430 non-null   int64  \n",
      " 8   clean            1430 non-null   object \n",
      " 9   tokenized        1430 non-null   object \n",
      " 10  stemmed          1430 non-null   object \n",
      " 11  lemmatized       1430 non-null   object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 145.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the df_ds_tx\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cognizant Technology Solutions    53\n",
       "Dell Technologies                 39\n",
       "Deloitte                          31\n",
       "Facebook                          28\n",
       "USAA                              25\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 companies by the number of posts\n",
    "df_ds_tx.company.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin         408\n",
       "Dallas         223\n",
       "Houston        181\n",
       "San Antonio    112\n",
       "Plano          108\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 cities by the number of posts\n",
    "df_ds_tx.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-12-27    392\n",
       "2021-01-03    136\n",
       "2021-01-10    152\n",
       "2021-01-17    123\n",
       "2021-01-24    288\n",
       "2021-01-31    231\n",
       "2021-02-07    108\n",
       "Freq: W-SUN, Name: title, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: the dataframe has datetime index\n",
    "df_ds_tx.resample(\"W\").title.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Job Requirements by Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random job link\n",
    "\n",
    "job_url = df_ds.job_link.sample(1, random_state=1)[0]\n",
    "job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rquest\n",
    "\n",
    "response = requests.get(job_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a soup to hold the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of all the words appear in the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frequency'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cib wholesale payment data analytics data scientist new rapidly growing team change agent inside jp '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v1\n",
    "dic = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic.keys())\n",
    "\n",
    "# Print the first 100 characters of the value\n",
    "dic['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['all', 'Apple'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'summary posted jan 25 2021 role number200218691 apple great idea way becoming great product service selfmotivated highenergy person afraid challenge looking apple seeking ml advanced research application manager join amp data science analytics covering app store apple music apple tv apple podcasts apple fitness etc amp data science analytics collaborates executive various partner across business p'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v2\n",
    "\n",
    "companies = ['Apple']\n",
    "dic_v2 = words_variables_v2(df_ds_tx, companies)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic_v2.keys())\n",
    "\n",
    "# Print the first 100 characters of the value of `Apple`\n",
    "dic_v2['Apple'][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(d_words['frequency'].split()).value_counts()\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = d_words.keys()\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade `word_frequency_v1`\n",
    "\n",
    "def word_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>14575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>5210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>4605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "data            14575\n",
       "experience       8188\n",
       "business         5210\n",
       "team             4605\n",
       "work             4050"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function word_frequency_v1\n",
    "\n",
    "monogram = word_frequency_v1(dic)\n",
    "monogram.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15001 entries, data to oneknowledge\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   frequency  15001 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "monogram.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function word_frequency_v2\n",
    "\n",
    "df_word_frequency_v2 = word_frequency_v2(dic_v2)\n",
    "df_word_frequency_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 'Bigram' column to dataframe\n",
    "df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 2))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th>learning</th>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <td>1757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>experience</th>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>scientist</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <th>science</th>\n",
       "      <td>926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     frequency\n",
       "machine  learning         2242\n",
       "data     science          1757\n",
       "year     experience       1222\n",
       "data     scientist        1000\n",
       "computer science           926"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = bigrams_frequency_v1(dic)\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute bigrams_frequency\n",
    "\n",
    "bigrams_v2 = bigrams_frequency_v2(dic_v2)\n",
    "bigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 3))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th>color</th>\n",
       "      <th>religion</th>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>employer</th>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <th>identity</th>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <th>regard</th>\n",
       "      <th>race</th>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  frequency\n",
       "sexual      orientation gender          419\n",
       "race        color       religion        413\n",
       "equal       opportunity employer        361\n",
       "orientation gender      identity        345\n",
       "without     regard      race            280"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function: trigrams_frequency_v1\n",
    "\n",
    "trigrams = trigrams_frequency_v1(dic)\n",
    "trigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: trigrams_frequency_v2\n",
    "\n",
    "trigrams_v2 = trigrams_frequency_v2(dic_v2)\n",
    "trigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Mono-, Bi- and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Simple concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is total number of grams? \n",
    "\n",
    "monogram.shape[0]+bigrams.shape[0]+trigrams.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338056, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>14575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>5210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>4605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "data            14575\n",
       "experience       8188\n",
       "business         5210\n",
       "team             4605\n",
       "work             4050"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all three grams\n",
    "\n",
    "everygram = pd.concat([monogram, bigrams, trigrams])\n",
    "print(everygram.shape)\n",
    "everygram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library for all skills\n",
    "\n",
    "library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', \n",
    "           'aws', 'hadoop', 'hive', 'impala', 'matlab', 'model', 'algorithm', \n",
    "           'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "           'sharepoint', 'dashboard']\n",
    "\n",
    "library_tech = ['programming', 'big data', 'wrangling', 'version control', 'visualiztion', ]\n",
    "library_soft = ['communication', 'business acumen', 'storytelling']\n",
    "library_tools = ['python', 'git', 'sql', 'pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "# big data\n",
    "# software engineering\n",
    "# model\n",
    "# models\n",
    "# algorithms\n",
    "# storytelling\n",
    "# statistic\n",
    "# statistical\n",
    "# machine learning\n",
    "# deep learning\n",
    "# etl\n",
    "# extraction\n",
    "# crud\n",
    "# exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_skills_ds_v1(k):\n",
    "    '''\n",
    "    This function accepts a positive integer k and \n",
    "    returns a dataframe containing the top k skills needed\n",
    "    for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_csv(f\"{database}df_tx_ds.csv\", index_col=0)\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    df_word_frequency = word_frequency_v1(dic)\n",
    "    # Define a library that has a complete sillset for data scientist\n",
    "    library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', 'aws', 'hadoop', 'hive', \n",
    "        'impala', 'matlab', 'model', 'algorithm', 'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "        'sharepoint', 'dashboard']\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (df_word_frequency.index == skill)\n",
    "        df = df_word_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function top_skills_ds\n",
    "\n",
    "top_skills = top_skills_ds_v1(7)\n",
    "top_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'python')\n",
    "df_word_frequency[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'r')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'aws')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'sql')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
