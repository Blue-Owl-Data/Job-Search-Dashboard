{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>All the functions in the Data Acquisitioin section have been tested out and inorporated into the MVP_acquire_ds.py and MVP_acquire_wd.py files. To save space, no extra test is carried out in this notebook.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "# type(first_page_soup)\n",
    "\n",
    "# # Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "# num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "# print(\"Data Type: \", type(num_jobs))\n",
    "# print(\"Name of the Tag: \", num_jobs.name)\n",
    "# print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "# print(\"Text within the Tag: \")\n",
    "# num_jobs.text\n",
    "\n",
    "# # Find the number of the jobs in the text\n",
    "# match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "# match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function job_cards_indeed\n",
    "# job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# # Print the data type of job_cards\n",
    "# type(job_cards)\n",
    "\n",
    "# # How many jobs listed in the 1st page? \n",
    "# len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function: page_soup_indeed\n",
    "\n",
    "# url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "# soup = page_soup_indeed(url)\n",
    "# type(soup)\n",
    "\n",
    "# # Find out the page number\n",
    "# int(page_num_indeed(url))\n",
    "\n",
    "# # Pull the job cards from the soup\n",
    "# type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location):\n",
    "    '''\n",
    "    This function accepts the job title and location and return \n",
    "    the job information pull from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <=35:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date of the job posts\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of the job post based on post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Deveopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Software Engineer, Front End\\nnew</td>\n",
       "      <td>Austin, TX 78701 (Downtown area)</td>\n",
       "      <td>eBay Inc.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>At eBay, you will be part of a purpose driven ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWS Developer\\nnew</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>TCS</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=210bc31caa4af...</td>\n",
       "      <td>Mandatory Technical Skills* C# , .Net Core, We...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title                          location  \\\n",
       "0  Senior Software Engineer, Front End\\nnew  Austin, TX 78701 (Downtown area)   \n",
       "1                        AWS Developer\\nnew                       Houston, TX   \n",
       "\n",
       "     company company_rating     post_age  \\\n",
       "0  eBay Inc.            3.9  Just posted   \n",
       "1        TCS            3.8  Just posted   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "1  https://www.indeed.com/rc/clk?jk=210bc31caa4af...   \n",
       "\n",
       "                                     job_description  \n",
       "0  At eBay, you will be part of a purpose driven ...  \n",
       "1  Mandatory Technical Skills* C# , .Net Core, We...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load web developer job posts in TX today\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_wd_new = pd.read_csv(f\"{database}web_developer_tx_indeed_020921.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_wd_new.shape)\n",
    "\n",
    "# Print the first two rows\n",
    "df_wd_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_wd(df):\n",
    "    '''\n",
    "    This function updates job posts of web developer in TX by adding the daily acquring\n",
    "    of web developer job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df_wd_tx = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_jobs = df_wd_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_wd_tx.date = pd.to_datetime(df_wd_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_wd_tx = df_wd_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_wd_tx = pd.concat([df_wd_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_wd_tx = remove_duplicates(df_wd_tx)\n",
    "    # Save as csv file\n",
    "    df_wd_tx.to_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_new_jobs = df_wd_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_wd_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Jobs Posted Today:  189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Senior Software Developer\\nnew</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Emerson</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=805ea6fc9112a...</td>\n",
       "      <td>Senior Software Developer\\n\\nRequisition ID: (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-09</th>\n",
       "      <td>Web site Developer/Manager\\nnew</td>\n",
       "      <td>San Antonio, TX 78216</td>\n",
       "      <td>Save Merchant Services</td>\n",
       "      <td>missing</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/company/Save-Merchant-S...</td>\n",
       "      <td>We are seeking a highly qualified web site des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title               location  \\\n",
       "date                                                                 \n",
       "2021-02-09   Senior Software Developer\\nnew             Austin, TX   \n",
       "2021-02-09  Web site Developer/Manager\\nnew  San Antonio, TX 78216   \n",
       "\n",
       "                           company company_rating post_age  \\\n",
       "date                                                         \n",
       "2021-02-09                 Emerson            3.9    Today   \n",
       "2021-02-09  Save Merchant Services        missing    Today   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-09  https://www.indeed.com/rc/clk?jk=805ea6fc9112a...   \n",
       "2021-02-09  https://www.indeed.com/company/Save-Merchant-S...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-09  Senior Software Developer\\n\\nRequisition ID: (...  \n",
       "2021-02-09  We are seeking a highly qualified web site des...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function: daily_update_wd\n",
    "\n",
    "df_test = daily_update_wd(df_wd_new)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2842 entries, 2021-02-09 to 2021-01-04 00:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            2842 non-null   object\n",
      " 1   location         2842 non-null   object\n",
      " 2   company          2842 non-null   object\n",
      " 3   company_rating   2842 non-null   object\n",
      " 4   post_age         2842 non-null   object\n",
      " 5   job_link         2842 non-null   object\n",
      " 6   job_description  2842 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 177.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job posts of web developer\n",
    "\n",
    "def prepare_job_posts_indeed_wd():\n",
    "    '''\n",
    "   The function cleans the csv file of web developer job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_wd_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.6 s, sys: 346 ms, total: 33 s\n",
      "Wall time: 33.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>Senior Software Developer\\nnew</td>\n",
       "      <td>Emerson</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=805ea6fc9112a...</td>\n",
       "      <td>Senior Software Developer\\n\\nRequisition ID: (...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>senior software developer requisition id 20008...</td>\n",
       "      <td>senior software developer\\n\\nrequisition id 20...</td>\n",
       "      <td>senior softwar develop requisit id 20008701 pr...</td>\n",
       "      <td>senior software developer requisition id 20008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>Web site Developer/Manager\\nnew</td>\n",
       "      <td>Save Merchant Services</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.indeed.com/company/Save-Merchant-S...</td>\n",
       "      <td>We are seeking a highly qualified web site des...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>78216</td>\n",
       "      <td>seeking highly qualified web site designer hig...</td>\n",
       "      <td>we are seeking a highly qualified web site des...</td>\n",
       "      <td>we are seek a highli qualifi web site design w...</td>\n",
       "      <td>we are seeking a highly qualified web site des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                            title                 company  \\\n",
       "0  2021-02-09   Senior Software Developer\\nnew                 Emerson   \n",
       "1  2021-02-09  Web site Developer/Manager\\nnew  Save Merchant Services   \n",
       "\n",
       "  company_rating                                           job_link  \\\n",
       "0            3.9  https://www.indeed.com/rc/clk?jk=805ea6fc9112a...   \n",
       "1              0  https://www.indeed.com/company/Save-Merchant-S...   \n",
       "\n",
       "                                     job_description         city state  \\\n",
       "0  Senior Software Developer\\n\\nRequisition ID: (...       Austin    TX   \n",
       "1  We are seeking a highly qualified web site des...  San Antonio    TX   \n",
       "\n",
       "  zipcode                                              clean  \\\n",
       "0       0  senior software developer requisition id 20008...   \n",
       "1   78216  seeking highly qualified web site designer hig...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  senior software developer\\n\\nrequisition id 20...   \n",
       "1  we are seeking a highly qualified web site des...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  senior softwar develop requisit id 20008701 pr...   \n",
       "1  we are seek a highli qualifi web site design w...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  senior software developer requisition id 20008...  \n",
       "1  we are seeking a highly qualified web site des...  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test the function: prepare_job_posts_indeed_wd\n",
    "df_test = prepare_job_posts_indeed_wd()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2842 entries, 0 to 2841\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   date             2842 non-null   object\n",
      " 1   title            2842 non-null   object\n",
      " 2   company          2842 non-null   object\n",
      " 3   company_rating   2842 non-null   object\n",
      " 4   job_link         2842 non-null   object\n",
      " 5   job_description  2842 non-null   object\n",
      " 6   city             2842 non-null   object\n",
      " 7   state            2842 non-null   object\n",
      " 8   zipcode          2842 non-null   object\n",
      " 9   clean            2842 non-null   object\n",
      " 10  tokenized        2842 non-null   object\n",
      " 11  stemmed          2842 non-null   object\n",
      " 12  lemmatized       2842 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 288.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "\n",
    "result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load old data scientist job posts in TX\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_ds_old = pd.read_csv(f\"{database}data_scientist_tx_indeed_020821.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_ds_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_test = transform_old_file(df_ds_old, '2021-02-08')\n",
    "# df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data scientist job posts in TX on 2021-02-08\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_ds_new = pd.read_csv(f\"{database}data_scientist_tx_indeed_020821.csv\", index_col=0)\n",
    "\n",
    "# Inspect the first 2 rows of the new posts\n",
    "df_ds_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: compute_post_date\n",
    "\n",
    "df_test = compute_post_date(df_ds_new)\n",
    "df_test.head(2) # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_ds(df):\n",
    "    '''\n",
    "    This function updates job posts of data scientist in TX by adding the daily acquring\n",
    "    of data scientist job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df_ds_tx = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    num_jobs = df_ds_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_ds_tx = pd.concat([df_ds_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_ds_tx = remove_duplicates(df_ds_tx)\n",
    "    # Save as csv file\n",
    "    df_ds_tx.to_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Print the new jobs posted today\n",
    "    num_new_jobs = df_ds_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_ds_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test the function: daily_update_ds\n",
    "\n",
    "df_test = daily_update_ds(df_ds_new)\n",
    "df_test.head() # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of the dateframe\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job post for exploration\n",
    "\n",
    "def prepare_job_posts_indeed_ds():\n",
    "    '''\n",
    "    The function cleans the csv file of data scientist job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_ds_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test the function: prepare_job_posts_indeed\n",
    "\n",
    "df_test = prepare_job_posts_indeed_ds()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "result = open(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the type of the file\n",
    "print(type(result))\n",
    "\n",
    "# \n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import \n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "# # Print the data type of s3\n",
    "# print(type(s3))\n",
    "\n",
    "# # Print the bucket names\n",
    "# for bucket in s3.buckets.all():\n",
    "#     print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Job Post:  1501\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Senior Data Analyst\\nnew</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>3.4</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Description\\nThe Senior Data Analyst is adept ...</td>\n",
       "      <td>Hurst</td>\n",
       "      <td>TX</td>\n",
       "      <td>76054</td>\n",
       "      <td>description senior data analyst adept working ...</td>\n",
       "      <td>description\\nthe senior data analyst is adept ...</td>\n",
       "      <td>descript the senior data analyst is adept at w...</td>\n",
       "      <td>description the senior data analyst is adept a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Data Scientist Senior - Computer Vision/Deep L...</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a tale...</td>\n",
       "      <td>LaCoste</td>\n",
       "      <td>TX</td>\n",
       "      <td>78039</td>\n",
       "      <td>purpose job currently seeking talented data sc...</td>\n",
       "      <td>purpose of job we are currently seeking a tale...</td>\n",
       "      <td>purpos of job we are current seek a talent dat...</td>\n",
       "      <td>purpose of job we are currently seeking a tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Senior Catastrophe Modeling Analyst\\nnew</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a Seni...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>TX</td>\n",
       "      <td>78206</td>\n",
       "      <td>purpose job currently seeking senior catastrop...</td>\n",
       "      <td>purpose of job we are currently seeking a seni...</td>\n",
       "      <td>purpos of job we are current seek a senior cat...</td>\n",
       "      <td>purpose of job we are currently seeking a seni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Data Scientist - 100% Remote Available\\nnew</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/cmp/Usaa</td>\n",
       "      <td>error</td>\n",
       "      <td>Shavano Park</td>\n",
       "      <td>TX</td>\n",
       "      <td>78231</td>\n",
       "      <td>error</td>\n",
       "      <td>error</td>\n",
       "      <td>error</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>organization jpmorgan chase co nyse jpm leadin...</td>\n",
       "      <td>organization\\njpmorgan chase co nyse jpm is a ...</td>\n",
       "      <td>organ jpmorgan chase co nyse jpm is a lead glo...</td>\n",
       "      <td>organization jpmorgan chase co nyse jpm is a l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-08                           Senior Data Analyst\\nnew   \n",
       "2021-02-08  Data Scientist Senior - Computer Vision/Deep L...   \n",
       "2021-02-08           Senior Catastrophe Modeling Analyst\\nnew   \n",
       "2021-02-08        Data Scientist - 100% Remote Available\\nnew   \n",
       "2021-02-08  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "\n",
       "                              company  company_rating  \\\n",
       "date                                                    \n",
       "2021-02-08                   Citizens             3.4   \n",
       "2021-02-08                       USAA             3.9   \n",
       "2021-02-08                       USAA             3.9   \n",
       "2021-02-08                       USAA             3.9   \n",
       "2021-02-08  JPMorgan Chase Bank, N.A.             3.9   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08                    https://www.indeed.com/cmp/Usaa   \n",
       "2021-02-08  https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...   \n",
       "\n",
       "                                              job_description          city  \\\n",
       "date                                                                          \n",
       "2021-02-08  Description\\nThe Senior Data Analyst is adept ...         Hurst   \n",
       "2021-02-08  Purpose of Job We are currently seeking a tale...       LaCoste   \n",
       "2021-02-08  Purpose of Job We are currently seeking a Seni...   San Antonio   \n",
       "2021-02-08                                              error  Shavano Park   \n",
       "2021-02-08  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...         Plano   \n",
       "\n",
       "           state  zipcode                                              clean  \\\n",
       "date                                                                           \n",
       "2021-02-08    TX    76054  description senior data analyst adept working ...   \n",
       "2021-02-08    TX    78039  purpose job currently seeking talented data sc...   \n",
       "2021-02-08    TX    78206  purpose job currently seeking senior catastrop...   \n",
       "2021-02-08    TX    78231                                              error   \n",
       "2021-02-08    TX        0  organization jpmorgan chase co nyse jpm leadin...   \n",
       "\n",
       "                                                    tokenized  \\\n",
       "date                                                            \n",
       "2021-02-08  description\\nthe senior data analyst is adept ...   \n",
       "2021-02-08  purpose of job we are currently seeking a tale...   \n",
       "2021-02-08  purpose of job we are currently seeking a seni...   \n",
       "2021-02-08                                              error   \n",
       "2021-02-08  organization\\njpmorgan chase co nyse jpm is a ...   \n",
       "\n",
       "                                                      stemmed  \\\n",
       "date                                                            \n",
       "2021-02-08  descript the senior data analyst is adept at w...   \n",
       "2021-02-08  purpos of job we are current seek a talent dat...   \n",
       "2021-02-08  purpos of job we are current seek a senior cat...   \n",
       "2021-02-08                                              error   \n",
       "2021-02-08  organ jpmorgan chase co nyse jpm is a lead glo...   \n",
       "\n",
       "                                                   lemmatized  \n",
       "date                                                           \n",
       "2021-02-08  description the senior data analyst is adept a...  \n",
       "2021-02-08  purpose of job we are currently seeking a tale...  \n",
       "2021-02-08  purpose of job we are currently seeking a seni...  \n",
       "2021-02-08                                              error  \n",
       "2021-02-08  organization jpmorgan chase co nyse jpm is a l...  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "df_ds_tx = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the number of job posts\n",
    "print(\"Number of Job Post: \", df_ds_tx.shape[0])\n",
    "\n",
    "# Conver the string date to datetime object\n",
    "df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "\n",
    "# Set the date as the index and sort the dataframe in descending order\n",
    "df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1501 entries, 2021-02-08 to 2020-12-22\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            1501 non-null   object \n",
      " 1   company          1501 non-null   object \n",
      " 2   company_rating   1501 non-null   float64\n",
      " 3   job_link         1501 non-null   object \n",
      " 4   job_description  1501 non-null   object \n",
      " 5   city             1501 non-null   object \n",
      " 6   state            1501 non-null   object \n",
      " 7   zipcode          1501 non-null   int64  \n",
      " 8   clean            1501 non-null   object \n",
      " 9   tokenized        1501 non-null   object \n",
      " 10  stemmed          1501 non-null   object \n",
      " 11  lemmatized       1501 non-null   object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 152.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the df_ds_tx\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cognizant Technology Solutions    55\n",
       "Dell Technologies                 40\n",
       "Deloitte                          32\n",
       "Facebook                          30\n",
       "USAA                              28\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 companies by the number of posts\n",
    "df_ds_tx.company.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin         420\n",
       "Dallas         239\n",
       "Houston        190\n",
       "Plano          116\n",
       "San Antonio    116\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 cities by the number of posts\n",
    "df_ds_tx.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-12-27    392\n",
       "2021-01-03    136\n",
       "2021-01-10    179\n",
       "2021-01-17    124\n",
       "2021-01-24    291\n",
       "2021-01-31    251\n",
       "2021-02-07    123\n",
       "2021-02-14      5\n",
       "Freq: W-SUN, Name: title, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: the dataframe has datetime index\n",
    "df_ds_tx.resample(\"W\").title.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Job Requirements by Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random job link\n",
    "\n",
    "job_url = df_ds.job_link.sample(1, random_state=1)[0]\n",
    "job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rquest\n",
    "\n",
    "response = requests.get(job_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a soup to hold the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of all the words appear in the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frequency'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'description senior data analyst adept working business system partner turn project requirement techn'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v1\n",
    "dic = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic.keys())\n",
    "\n",
    "# Print the first 100 characters of the value\n",
    "dic['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the helper function: words_variables_v2\n",
    "\n",
    "companies = ['Apple']\n",
    "dic_v2 = words_variables_v2(df_ds_tx, companies)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic_v2.keys())\n",
    "\n",
    "# Print the first 100 characters of the value of `Apple`\n",
    "dic_v2['Apple'][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(d_words['frequency'].split()).value_counts()\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = d_words.keys()\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade `word_frequency_v1`\n",
    "\n",
    "def word_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function word_frequency_v1\n",
    "\n",
    "monogram = word_frequency_v1(dic)\n",
    "monogram.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monogram.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function word_frequency_v2\n",
    "\n",
    "df_word_frequency_v2 = word_frequency_v2(dic_v2)\n",
    "df_word_frequency_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 'Bigram' column to dataframe\n",
    "df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 2))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = bigrams_frequency_v1(dic)\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute bigrams_frequency\n",
    "\n",
    "bigrams_v2 = bigrams_frequency_v2(dic_v2)\n",
    "bigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 3))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: trigrams_frequency_v1\n",
    "\n",
    "trigrams = trigrams_frequency_v1(dic)\n",
    "trigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: trigrams_frequency_v2\n",
    "\n",
    "trigrams_v2 = trigrams_frequency_v2(dic_v2)\n",
    "trigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Mono-, Bi- and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Simple concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is total number of grams? \n",
    "\n",
    "monogram.shape[0]+bigrams.shape[0]+trigrams.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all three grams\n",
    "\n",
    "everygram = pd.concat([monogram, bigrams, trigrams])\n",
    "print(everygram.shape)\n",
    "everygram.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2:  Use nltk.util.everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the frequence of the mono-, bi-, and tri-grams of the job description\n",
    "\n",
    "def everygram_frequency_v1(d_words, max_len=3):\n",
    "    '''\n",
    "    This function accetps the dictionary produced by the function `words_variables_v1` and \n",
    "    return mono-, bi-, and tri-grams along with their frequency. \n",
    "    '''\n",
    "    # Generate mono-, bi-, and tri-grams\n",
    "    grams = nltk.everygrams(d_words['frequency'].split(), max_len=max_len) # dtype of grams: <class 'genertor'>\n",
    "    # Convert to a list of tuples\n",
    "    grams = list(grams)\n",
    "    # Create an empty list to hold mono-, bi-, and tri-grams\n",
    "    everygram = []\n",
    "    # For loop the list of tuples and convert the grams to strings\n",
    "    for gram in grams:\n",
    "        str_gram = gram[0]\n",
    "        for i in gram[1:]:\n",
    "            str_gram = str_gram + ' ' + i\n",
    "        everygram.append(str_gram)\n",
    "    # Compute the frequency of the everygrams\n",
    "    everygram = pd.Series(everygram).value_counts()\n",
    "    return everygram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          15473\n",
       "experience     8540\n",
       "business       5449\n",
       "team           4783\n",
       "work           4219\n",
       "dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function above\n",
    "\n",
    "everygram = everygram_frequency_v1(dic)\n",
    "everygram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pick up the top k skills of a data scientict from a skillset library\n",
    "\n",
    "def top_skills_ds_v1(k, library):\n",
    "    '''\n",
    "    This function accepts a positive integer k and a skillset library and \n",
    "    returns a dataframe containing the top k skills needed for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    everygram_frequency = everygram_frequency_v1(dic)\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = ( everygram_frequency.index == skill)\n",
    "        df =  everygram_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.columns = dic.keys()\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>1249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data scientist</th>\n",
       "      <td>1043.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbal written communication</th>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critical thinking</th>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              frequency\n",
       "python                           1249.0\n",
       "data scientist                   1043.0\n",
       "sql                               948.0\n",
       "verbal written communication      123.0\n",
       "critical thinking                  69.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sample library of skills for data scientist\n",
    "library = ['python', 'sql', 'critical thinking', 'data scientist', 'verbal written communication']\n",
    "\n",
    "# Test function: top_skills_ds_v1\n",
    "df_test = top_skills_ds_v1(5, library)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library for all skills\n",
    "\n",
    "library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', \n",
    "           'aws', 'hadoop', 'hive', 'impala', 'matlab', 'model', 'algorithm', \n",
    "           'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "           'sharepoint', 'dashboard']\n",
    "\n",
    "library_tech = ['programming', 'big data', 'wrangling', 'version control', 'visualiztion', ]\n",
    "library_soft = ['communication', 'business acumen', 'storytelling']\n",
    "library_tools = ['python', 'git', 'sql', 'pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "# big data\n",
    "# software engineering\n",
    "# model\n",
    "# models\n",
    "# algorithms\n",
    "# storytelling\n",
    "# statistic\n",
    "# statistical\n",
    "# machine learning\n",
    "# deep learning\n",
    "# etl\n",
    "# extraction\n",
    "# crud\n",
    "# exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_skills_ds_v1(k):\n",
    "    '''\n",
    "    This function accepts a positive integer k and \n",
    "    returns a dataframe containing the top k skills needed\n",
    "    for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_csv(f\"{database}df_tx_ds.csv\", index_col=0)\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    df_word_frequency = word_frequency_v1(dic)\n",
    "    # Define a library that has a complete sillset for data scientist\n",
    "    library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', 'aws', 'hadoop', 'hive', \n",
    "        'impala', 'matlab', 'model', 'algorithm', 'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "        'sharepoint', 'dashboard']\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (df_word_frequency.index == skill)\n",
    "        df = df_word_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function top_skills_ds\n",
    "\n",
    "top_skills = top_skills_ds_v1(7)\n",
    "top_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'python')\n",
    "df_word_frequency[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'r')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'aws')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'sql')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
