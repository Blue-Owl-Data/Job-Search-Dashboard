{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "url = first_page_url_indeed('data scientist', 'al')\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code of the request:  200\n",
      "Document type:  <!DOCTYPE html>\n",
      "Title of the response:  Data Scientist Jobs, Employment in Alabama | Indeed.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "type(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type:  <class 'bs4.element.Tag'>\n",
      "Name of the Tag:  div\n",
      "Attributes of the Tag:  {'id': 'searchCountPages'}\n",
      "Text within the Tag: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n                    Page 1 of 48 jobs'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "print(\"Data Type: \", type(num_jobs))\n",
    "print(\"Name of the Tag: \", num_jobs.name)\n",
    "print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "print(\"Text within the Tag: \")\n",
    "num_jobs.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of the jobs in the text\n",
    "match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function num_jobs_indeed\n",
    "num_jobs_indeed(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function job_cards_indeed\n",
    "job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# Print the data type of job_cards\n",
    "type(job_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Note**: job_cards is an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many jobs listed in the 1st page? \n",
    "len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Statistical Programmer\\nnew',\n",
       " 'Summer Intern - Data Science\\nnew',\n",
       " 'Senior Machine Learning Software Engineer\\nnew',\n",
       " 'Technology and Enterprise Ops Data Science Intern (Summer 20...\\nnew',\n",
       " 'Fellow, Data Engineer\\nnew',\n",
       " 'Machine Learning Data Scientist (#1663600)\\nnew',\n",
       " 'Artificial Intelligence (AI) Business Development Manager\\nnew',\n",
       " 'Data Science Application Administrator\\nnew',\n",
       " 'Senior Incentives and Proficiency Data Analyst\\nnew',\n",
       " 'Statistician/Applied Mathematician (Part-Time)',\n",
       " 'AI Cognitive-Architect',\n",
       " 'Solution Architect - Data & Analytics',\n",
       " 'Senior Sensor Data Analyst',\n",
       " 'Machine Learning / Artificial Intelligence Engineer',\n",
       " 'Data Analyst - Microsoft Stack (mid-senior)']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = job_titles_indeed(job_cards)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Translational Drug Development (TD2)',\n",
       " 'COLSA',\n",
       " 'Wind River',\n",
       " 'Regions Bank',\n",
       " 'American Heart Association',\n",
       " 'PeopleTec',\n",
       " 'Dynetics',\n",
       " 'SAIC',\n",
       " 'Prescient Edge Federal',\n",
       " 'Goldbelt, Inc.',\n",
       " 'Wipro LTD',\n",
       " 'Deloitte',\n",
       " 'Whitney, Bradley and Brown',\n",
       " 'Dynetics',\n",
       " 'Vaco']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: comany_names_indeed\n",
    "company_names = company_names_indeed(job_cards)\n",
    "company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 days ago',\n",
       " '3 days ago',\n",
       " '4 days ago',\n",
       " '5 days ago',\n",
       " '5 days ago',\n",
       " '5 days ago',\n",
       " '5 days ago',\n",
       " '6 days ago',\n",
       " '6 days ago',\n",
       " '11 days ago',\n",
       " '10 days ago',\n",
       " '13 days ago',\n",
       " '11 days ago',\n",
       " '12 days ago',\n",
       " '20 days ago']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: post_ages_indeed\n",
    "ages = post_ages_indeed(job_cards)\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United States',\n",
       " 'Huntsville, AL',\n",
       " 'Huntsville, AL',\n",
       " 'Birmingham, AL',\n",
       " 'Birmingham, AL 35298',\n",
       " 'Huntsville, AL 35805',\n",
       " 'Huntsville, AL',\n",
       " 'Huntsville, AL 35898',\n",
       " 'Birmingham, AL 35201',\n",
       " 'Fort Rucker, AL',\n",
       " 'Louisville, AL',\n",
       " 'Huntsville, AL 35806',\n",
       " 'Huntsville, AL 35801',\n",
       " 'Huntsville, AL',\n",
       " 'Hartselle, AL']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function: job_locations_indeed\n",
    "locations = job_locations_indeed(job_cards)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['missing',\n",
       " '3.9',\n",
       " '3.7',\n",
       " '3.6',\n",
       " '3.8',\n",
       " '4.5',\n",
       " '4.3',\n",
       " '4.0',\n",
       " 'missing',\n",
       " '3.3',\n",
       " '3.8',\n",
       " '4.0',\n",
       " '3.8',\n",
       " '4.3',\n",
       " '3.7']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = company_rating_indeed(job_cards)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: job_links_and_contents_indeed\n",
    "links, descriptions = job_links_and_contents_indeed(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code of the request:  200\n",
      "Document type:  <!DOCTYPE html>\n",
      "Title of the response:  Data Scientist Jobs, Employment in Alabama | Indeed.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: page_soup_indeed\n",
    "\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "soup = page_soup_indeed(url)\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code of the request:  200\n",
      "Document type:  <!DOCTYPE html>\n",
      "Title of the response:  Data Scientist Jobs, Employment in Alabama | Indeed.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the page number\n",
    "int(page_num_indeed(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull the job cards from the soup\n",
    "type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function acquire_page_indeed\n",
    "page_num, df = acquire_page_indeed(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location):\n",
    "    '''\n",
    "    This function accepts the job title and location and return \n",
    "    the job information pull from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <=35:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date of the job posts\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of the job post based on post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Deveopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load web developer job posts in TX today\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_wd_new = pd.read_csv(f\"{database}web_developer_tx_indeed_020721.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_wd_new.shape)\n",
    "\n",
    "# Print the first two rows\n",
    "df_wd_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_wd(df):\n",
    "    '''\n",
    "    This function updates job posts of web developer in TX by adding the daily acquring\n",
    "    of web developer job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df_wd_tx = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_jobs = df_wd_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_wd_tx.date = pd.to_datetime(df_wd_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_wd_tx = df_wd_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_wd_tx = pd.concat([df_wd_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_wd_tx = remove_duplicates(df_wd_tx)\n",
    "    # Save as csv file\n",
    "    df_wd_tx.to_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    num_new_jobs = df_wd_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_wd_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: daily_update_wd\n",
    "\n",
    "df_test = daily_update_wd(df_wd_new)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job posts of web developer\n",
    "\n",
    "def prepare_job_posts_indeed_wd():\n",
    "    '''\n",
    "   The function cleans the csv file of web developer job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_wd_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_wd_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Test the function: prepare_job_posts_indeed_wd\n",
    "df_test = prepare_job_posts_indeed_wd()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the json file\n",
    "\n",
    "result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Senior Data Analyst\\nnew</td>\n",
       "      <td>Hurst, TX 76054</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Description\\nThe Senior Data Analyst is adept ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-08  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "2021-02-08                           Senior Data Analyst\\nnew   \n",
       "\n",
       "                   location                    company company_rating  \\\n",
       "date                                                                    \n",
       "2021-02-08        Plano, TX  JPMorgan Chase Bank, N.A.            3.9   \n",
       "2021-02-08  Hurst, TX 76054                   Citizens            3.4   \n",
       "\n",
       "               post_age                                           job_link  \\\n",
       "date                                                                         \n",
       "2021-02-08  Just posted  https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...   \n",
       "2021-02-08        Today  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-08  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  \n",
       "2021-02-08  Description\\nThe Senior Data Analyst is adept ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load old data scientist job posts in TX\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_ds_old = pd.read_csv(f\"{database}data_scientist_tx_indeed_020821.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_ds_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_test = transform_old_file(df_ds_old, '2021-02-08')\n",
    "# df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>Senior Data Analyst\\nnew</td>\n",
       "      <td>Hurst, TX 76054</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Description\\nThe Senior Data Analyst is adept ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2021-02-08  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "1  2021-02-08                           Senior Data Analyst\\nnew   \n",
       "\n",
       "          location                    company company_rating     post_age  \\\n",
       "0        Plano, TX  JPMorgan Chase Bank, N.A.            3.9  Just posted   \n",
       "1  Hurst, TX 76054                   Citizens            3.4        Today   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...   \n",
       "1  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  \n",
       "1  Description\\nThe Senior Data Analyst is adept ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data scientist job posts in TX on 2021-02-08\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "df_ds_new = pd.read_csv(f\"{database}data_scientist_tx_indeed_020821.csv\", index_col=0)\n",
    "\n",
    "# Inspect the first 2 rows of the new posts\n",
    "df_ds_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function: compute_post_date\n",
    "\n",
    "df_test = compute_post_date(df_ds_new)\n",
    "df_test.head(2) # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update_ds(df):\n",
    "    '''\n",
    "    This function updates job posts of data scientist in TX by adding the daily acquring\n",
    "    of data scientist job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df_ds_tx = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    num_jobs = df_ds_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_ds_tx = pd.concat([df_ds_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_ds_tx = remove_duplicates(df_ds_tx)\n",
    "    # Save as csv file\n",
    "    df_ds_tx.to_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Print the new jobs posted today\n",
    "    num_new_jobs = df_ds_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs Posted Today: \", num_new_jobs)\n",
    "    return df_ds_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Jobs Posted Today:  71\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Senior Data Analyst\\nnew</td>\n",
       "      <td>Hurst, TX 76054</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Description\\nThe Senior Data Analyst is adept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Data Scientist Senior - Computer Vision/Deep L...</td>\n",
       "      <td>LaCoste, TX 78039</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Senior Catastrophe Modeling Analyst\\nnew</td>\n",
       "      <td>San Antonio, TX 78206 (King William area)</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a Seni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Data Scientist - 100% Remote Available\\nnew</td>\n",
       "      <td>Shavano Park, TX 78231</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/cmp/Usaa</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-08</th>\n",
       "      <td>Risk - Corporate Risk - Wholesale Credit Solut...</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...</td>\n",
       "      <td>Organization\\nJPMorgan Chase &amp; Co. (NYSE: JPM)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-08                           Senior Data Analyst\\nnew   \n",
       "2021-02-08  Data Scientist Senior - Computer Vision/Deep L...   \n",
       "2021-02-08           Senior Catastrophe Modeling Analyst\\nnew   \n",
       "2021-02-08        Data Scientist - 100% Remote Available\\nnew   \n",
       "2021-02-08  Risk - Corporate Risk - Wholesale Credit Solut...   \n",
       "\n",
       "                                             location  \\\n",
       "date                                                    \n",
       "2021-02-08                            Hurst, TX 76054   \n",
       "2021-02-08                          LaCoste, TX 78039   \n",
       "2021-02-08  San Antonio, TX 78206 (King William area)   \n",
       "2021-02-08                     Shavano Park, TX 78231   \n",
       "2021-02-08                                  Plano, TX   \n",
       "\n",
       "                              company company_rating     post_age  \\\n",
       "date                                                                \n",
       "2021-02-08                   Citizens            3.4        Today   \n",
       "2021-02-08                       USAA            3.9        Today   \n",
       "2021-02-08                       USAA            3.9        Today   \n",
       "2021-02-08                       USAA            3.9        Today   \n",
       "2021-02-08  JPMorgan Chase Bank, N.A.            3.9  Just posted   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "2021-02-08                    https://www.indeed.com/cmp/Usaa   \n",
       "2021-02-08  https://www.indeed.com/rc/clk?jk=2f4b1ed986adf...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-08  Description\\nThe Senior Data Analyst is adept ...  \n",
       "2021-02-08  Purpose of Job We are currently seeking a tale...  \n",
       "2021-02-08  Purpose of Job We are currently seeking a Seni...  \n",
       "2021-02-08                                              error  \n",
       "2021-02-08  Organization\\nJPMorgan Chase & Co. (NYSE: JPM)...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function: daily_update_ds\n",
    "\n",
    "df_test = daily_update_ds(df_ds_new)\n",
    "df_test.head() # Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1501 entries, 2021-02-08 to 2020-12-22\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            1501 non-null   object\n",
      " 1   location         1501 non-null   object\n",
      " 2   company          1501 non-null   object\n",
      " 3   company_rating   1501 non-null   object\n",
      " 4   post_age         1501 non-null   object\n",
      " 5   job_link         1501 non-null   object\n",
      " 6   job_description  1501 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the dateframe\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job post for exploration\n",
    "\n",
    "def prepare_job_posts_indeed_ds():\n",
    "    '''\n",
    "    The function cleans the csv file of data scientist job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of data scientist in TX\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_csv(f\"{database}df_ds_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Drop the column post_age and location\n",
    "    df = df.drop(columns=['post_age', 'location'])\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_ds_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 356 ms, total: 25.6 s\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>Senior Data Analyst\\nnew</td>\n",
       "      <td>Citizens</td>\n",
       "      <td>3.4</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Description\\nThe Senior Data Analyst is adept ...</td>\n",
       "      <td>Hurst</td>\n",
       "      <td>TX</td>\n",
       "      <td>76054</td>\n",
       "      <td>description senior data analyst adept working ...</td>\n",
       "      <td>description\\nthe senior data analyst is adept ...</td>\n",
       "      <td>descript the senior data analyst is adept at w...</td>\n",
       "      <td>description the senior data analyst is adept a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>Data Scientist Senior - Computer Vision/Deep L...</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a tale...</td>\n",
       "      <td>LaCoste</td>\n",
       "      <td>TX</td>\n",
       "      <td>78039</td>\n",
       "      <td>purpose job currently seeking talented data sc...</td>\n",
       "      <td>purpose of job we are currently seeking a tale...</td>\n",
       "      <td>purpos of job we are current seek a talent dat...</td>\n",
       "      <td>purpose of job we are currently seeking a tale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title   company  \\\n",
       "0  2021-02-08                           Senior Data Analyst\\nnew  Citizens   \n",
       "1  2021-02-08  Data Scientist Senior - Computer Vision/Deep L...      USAA   \n",
       "\n",
       "  company_rating                                           job_link  \\\n",
       "0            3.4  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "1            3.9  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                     job_description     city state zipcode  \\\n",
       "0  Description\\nThe Senior Data Analyst is adept ...    Hurst    TX   76054   \n",
       "1  Purpose of Job We are currently seeking a tale...  LaCoste    TX   78039   \n",
       "\n",
       "                                               clean  \\\n",
       "0  description senior data analyst adept working ...   \n",
       "1  purpose job currently seeking talented data sc...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  description\\nthe senior data analyst is adept ...   \n",
       "1  purpose of job we are currently seeking a tale...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  descript the senior data analyst is adept at w...   \n",
       "1  purpos of job we are current seek a talent dat...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  description the senior data analyst is adept a...  \n",
       "1  purpose of job we are currently seeking a tale...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test the function: prepare_job_posts_indeed\n",
    "\n",
    "df_test = prepare_job_posts_indeed_ds()\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1501 entries, 0 to 1500\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   date             1501 non-null   object\n",
      " 1   title            1501 non-null   object\n",
      " 2   company          1501 non-null   object\n",
      " 3   company_rating   1501 non-null   object\n",
      " 4   job_link         1501 non-null   object\n",
      " 5   job_description  1501 non-null   object\n",
      " 6   city             1501 non-null   object\n",
      " 7   state            1501 non-null   object\n",
      " 8   zipcode          1501 non-null   object\n",
      " 9   clean            1501 non-null   object\n",
      " 10  tokenized        1501 non-null   object\n",
      " 11  stemmed          1501 non-null   object\n",
      " 12  lemmatized       1501 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 152.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'date': '2021-02-08',\n",
       " 'title': 'Senior Data Analyst\\nnew',\n",
       " 'company': 'Citizens',\n",
       " 'company_rating': '3.4',\n",
       " 'job_link': 'https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DbY87xTe1ZMhhjQ9k8R965brWLRw8vo5R_taDvbDEPJsMlZ2V6QwSwPAjpFnpq1NJDc6Sn0imHZiNZxmFbMoGggUVwDm0GXDwhtRqY6D5NOtv_To4nikhBhrrEO-gDRucGBjQ99z3jQrXN6RZcvbadXk3mok-jozvnlUAp_okzBi_9rq24mMOPgGKVAjQ9TUftorNQ7C-7F9hIncgZdBbVDhB_61nvVSs7TNPXEpu6Al-ABfD0I8kRnGsZ-slkNTsjYUoIa8BgW09OmxqohJ4sD6V4z7yxfw_AIfe7wkgP-2oy5mXs_AEi6vN-2ubRVGRLYoiv5jV3ejXihapBy4fA9XE45bp74_vqLLHIALgqgtfYCQosYr9y&p=2&fvj=0&vjs=3',\n",
       " 'job_description': \"Description\\nThe Senior Data Analyst is adept at working with business and systems partners to turn project requirements into technical specifications to build custom-formatted, fit-for-purpose reports, data sets, data services and data interfaces. The candidate applies a strong knowledge of life-cycle data generation to be able to outline critical information, analyze business procedures, and recommend specific types of data than can be used to improve them.\\nPrimary responsibilities will be to:\\nUse stakeholder knowledge and other available resources to research, to facilitate discovery, and to analyze and evaluate options for data solutions. Provide recommendations to team and project leads.\\nProduce consistent, accurate and complete data requirements and definitions for complex projects.\\nDevelop current state processmaps and identify digitization opportunities with data, analytics and Master data management solutions\\nWork with business users and others to understand their business functions and processes in order to help them define business intelligence, analytic and reporting requirements. Assist in crafting data and metadata requirements for systems that are complete and unambiguous.\\nUse statistical methods to analyze data and generate useful data sets and reports.\\nWork with management team and other leads to prioritize needs for business segments.\\nIdentify and recommend new, innovative ways to streamline data-centric processes.\\nUse data to create models that depict trends in the customer base or the consumer population, or within a specified segment or business processing area.\\nWork with departmental managers and business stake holders to outline data needs for analytical, reporting and data integration projects.\\nDefine and manage test data sets for integration testing, problem analysis, business and technical analysis.\\nQualifications\\nRequired Skills/Experience:\\nDemonstrated understanding of consumer baking/financial products and business processes\\nExperience doing data discovery and analysis work with one or more commonly-used business intelligence tools, such as Tableau, SAS, Qlik, Oracle, Cognos, Microsoft, Information Builders, etc.\\nExperience working with various operating systems (Windows, Unix/Linux) and database management systems (Redshift, Oracle, MS-SQL, etc).\\nAbility to collaborate effectively, take initiative and work as part of a team\\nStrong attention to detail\\nDesireable Skills/Experience:\\nData exploration experience using both SQL and not-only-SQL tools and techniques.\\nData design or data management experience is a plus.\\nExperience with Agile process is preferred\\nExperience in the financial services industry, especially banking.\\nEducational, Certifications and/or Other Professional Credentials:\\n7 years of experience as described above and a Bachelors Degree in Business, finance, Mathematics, Statistics or Computer science\\nOr 5 years of experience as described above and a Masters Degree in those disciplines\\nHours & Work Schedule\\nHours per Week: 40\\nWork Schedule: Monday through Friday; 8:30AM  5:00PM\\nWhy Work for Us\\nAt Citizens, you'll find a customer-centric culture built around helping our customers and giving back to our local communities. When you join our team, you are part of a supportive and collaborative workforce, with access to training and tools to accelerate your potential and maximize your career growth.\\nEqual Employment Opportunity\\nIt is the policy of Citizens to provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to race, color, ethnicity, religion, gender, pregnancy/childbirth, colleague or a dependents reproductive health decision making, age, national origin, sexual orientation, gender identity or expression, disability or perceived disability, genetic information, genetic characteristic, citizenship, veteran or military status, marital or domestic partner status, family status/parenthood, victim of domestic violence, or any other category protected by federal, state and/or local laws.\\nEqual Employment and Opportunity Employer/Disabled/Veteran\\nCitizens is a brand name of Citizens Bank, N.A. and each of its respective affiliates.\",\n",
       " 'city': 'Hurst',\n",
       " 'state': 'TX',\n",
       " 'zipcode': '76054',\n",
       " 'clean': 'description senior data analyst adept working business system partner turn project requirement technical specification build customformatted fitforpurpose report data set data service data interface candidate applies strong knowledge lifecycle data generation able outline critical information analyze business procedure recommend specific type data used improve primary responsibility use stakeholder knowledge available resource research facilitate discovery analyze evaluate option data solution provide recommendation team project lead produce consistent accurate complete data requirement definition complex project develop current state processmaps identify digitization opportunity data analytics master data management solution work business user others understand business function process order help define business intelligence analytic reporting requirement assist crafting data metadata requirement system complete unambiguous use statistical method analyze data generate useful data set report work management team lead prioritize need business segment identify recommend new innovative way streamline datacentric process use data create model depict trend customer base consumer population within specified segment business processing area work departmental manager business stake holder outline data need analytical reporting data integration project define manage test data set integration testing problem analysis business technical analysis qualification required skillsexperience demonstrated understanding consumer bakingfinancial product business process experience data discovery analysis work one commonlyused business intelligence tool tableau sa qlik oracle cognos microsoft information builder etc experience working various operating system window unixlinux database management system redshift oracle mssql etc ability collaborate effectively take initiative work part team strong attention detail desireable skillsexperience data exploration experience using sql notonlysql tool technique data design data management experience plus experience agile process preferred experience financial service industry especially banking educational certification andor professional credential 7 year experience described bachelor degree business finance mathematics statistic computer science 5 year experience described master degree discipline hour work schedule hour per week 40 work schedule monday friday 830am 500pm work u citizen youll find customercentric culture built around helping customer giving back local community join team part supportive collaborative workforce access training tool accelerate potential maximize career growth equal employment opportunity policy citizen provide equal employment advancement opportunity colleague applicant employment without regard race color ethnicity religion gender pregnancychildbirth colleague dependent reproductive health decision making age national origin sexual orientation gender identity expression disability perceived disability genetic information genetic characteristic citizenship veteran military status marital domestic partner status family statusparenthood victim domestic violence category protected federal state andor local law equal employment opportunity employerdisabledveteran citizen brand name citizen bank na respective affiliate',\n",
       " 'tokenized': 'description\\nthe senior data analyst is adept at working with business and systems partners to turn project requirements into technical specifications to build customformatted fitforpurpose reports data sets data services and data interfaces the candidate applies a strong knowledge of lifecycle data generation to be able to outline critical information analyze business procedures and recommend specific types of data than can be used to improve them\\nprimary responsibilities will be to\\nuse stakeholder knowledge and other available resources to research to facilitate discovery and to analyze and evaluate options for data solutions provide recommendations to team and project leads\\nproduce consistent accurate and complete data requirements and definitions for complex projects\\ndevelop current state processmaps and identify digitization opportunities with data analytics and master data management solutions\\nwork with business users and others to understand their business functions and processes in order to help them define business intelligence analytic and reporting requirements assist in crafting data and metadata requirements for systems that are complete and unambiguous\\nuse statistical methods to analyze data and generate useful data sets and reports\\nwork with management team and other leads to prioritize needs for business segments\\nidentify and recommend new innovative ways to streamline datacentric processes\\nuse data to create models that depict trends in the customer base or the consumer population or within a specified segment or business processing area\\nwork with departmental managers and business stake holders to outline data needs for analytical reporting and data integration projects\\ndefine and manage test data sets for integration testing problem analysis business and technical analysis\\nqualifications\\nrequired skillsexperience\\ndemonstrated understanding of consumer bakingfinancial products and business processes\\nexperience doing data discovery and analysis work with one or more commonlyused business intelligence tools such as tableau sas qlik oracle cognos microsoft information builders etc\\nexperience working with various operating systems windows unixlinux and database management systems redshift oracle mssql etc\\nability to collaborate effectively take initiative and work as part of a team\\nstrong attention to detail\\ndesireable skillsexperience\\ndata exploration experience using both sql and notonlysql tools and techniques\\ndata design or data management experience is a plus\\nexperience with agile process is preferred\\nexperience in the financial services industry especially banking\\neducational certifications andor other professional credentials\\n7 years of experience as described above and a bachelors degree in business finance mathematics statistics or computer science\\nor 5 years of experience as described above and a masters degree in those disciplines\\nhours work schedule\\nhours per week 40\\nwork schedule monday through friday 830am 500pm\\nwhy work for us\\nat citizens youll find a customercentric culture built around helping our customers and giving back to our local communities when you join our team you are part of a supportive and collaborative workforce with access to training and tools to accelerate your potential and maximize your career growth\\nequal employment opportunity\\nit is the policy of citizens to provide equal employment and advancement opportunities to all colleagues and applicants for employment without regard to race color ethnicity religion gender pregnancychildbirth colleague or a dependents reproductive health decision making age national origin sexual orientation gender identity or expression disability or perceived disability genetic information genetic characteristic citizenship veteran or military status marital or domestic partner status family statusparenthood victim of domestic violence or any other category protected by federal state andor local laws\\nequal employment and opportunity employerdisabledveteran\\ncitizens is a brand name of citizens bank na and each of its respective affiliates',\n",
       " 'stemmed': 'descript the senior data analyst is adept at work with busi and system partner to turn project requir into technic specif to build customformat fitforpurpos report data set data servic and data interfac the candid appli a strong knowledg of lifecycl data gener to be abl to outlin critic inform analyz busi procedur and recommend specif type of data than can be use to improv them primari respons will be to use stakehold knowledg and other avail resourc to research to facilit discoveri and to analyz and evalu option for data solut provid recommend to team and project lead produc consist accur and complet data requir and definit for complex project develop current state processmap and identifi digit opportun with data analyt and master data manag solut work with busi user and other to understand their busi function and process in order to help them defin busi intellig analyt and report requir assist in craft data and metadata requir for system that are complet and unambigu use statist method to analyz data and gener use data set and report work with manag team and other lead to priorit need for busi segment identifi and recommend new innov way to streamlin datacentr process use data to creat model that depict trend in the custom base or the consum popul or within a specifi segment or busi process area work with department manag and busi stake holder to outlin data need for analyt report and data integr project defin and manag test data set for integr test problem analysi busi and technic analysi qualif requir skillsexperi demonstr understand of consum bakingfinanci product and busi process experi do data discoveri and analysi work with one or more commonlyus busi intellig tool such as tableau sa qlik oracl cogno microsoft inform builder etc experi work with variou oper system window unixlinux and databas manag system redshift oracl mssql etc abil to collabor effect take initi and work as part of a team strong attent to detail desir skillsexperi data explor experi use both sql and notonlysql tool and techniqu data design or data manag experi is a plu experi with agil process is prefer experi in the financi servic industri especi bank educ certif andor other profession credenti 7 year of experi as describ abov and a bachelor degre in busi financ mathemat statist or comput scienc or 5 year of experi as describ abov and a master degre in those disciplin hour work schedul hour per week 40 work schedul monday through friday 830am 500pm whi work for us at citizen youll find a customercentr cultur built around help our custom and give back to our local commun when you join our team you are part of a support and collabor workforc with access to train and tool to acceler your potenti and maxim your career growth equal employ opportun it is the polici of citizen to provid equal employ and advanc opportun to all colleagu and applic for employ without regard to race color ethnic religion gender pregnancychildbirth colleagu or a depend reproduct health decis make age nation origin sexual orient gender ident or express disabl or perceiv disabl genet inform genet characterist citizenship veteran or militari statu marit or domest partner statu famili statusparenthood victim of domest violenc or ani other categori protect by feder state andor local law equal employ and opportun employerdisabledveteran citizen is a brand name of citizen bank na and each of it respect affili',\n",
       " 'lemmatized': 'description the senior data analyst is adept at working with business and system partner to turn project requirement into technical specification to build customformatted fitforpurpose report data set data service and data interface the candidate applies a strong knowledge of lifecycle data generation to be able to outline critical information analyze business procedure and recommend specific type of data than can be used to improve them primary responsibility will be to use stakeholder knowledge and other available resource to research to facilitate discovery and to analyze and evaluate option for data solution provide recommendation to team and project lead produce consistent accurate and complete data requirement and definition for complex project develop current state processmaps and identify digitization opportunity with data analytics and master data management solution work with business user and others to understand their business function and process in order to help them define business intelligence analytic and reporting requirement assist in crafting data and metadata requirement for system that are complete and unambiguous use statistical method to analyze data and generate useful data set and report work with management team and other lead to prioritize need for business segment identify and recommend new innovative way to streamline datacentric process use data to create model that depict trend in the customer base or the consumer population or within a specified segment or business processing area work with departmental manager and business stake holder to outline data need for analytical reporting and data integration project define and manage test data set for integration testing problem analysis business and technical analysis qualification required skillsexperience demonstrated understanding of consumer bakingfinancial product and business process experience doing data discovery and analysis work with one or more commonlyused business intelligence tool such a tableau sa qlik oracle cognos microsoft information builder etc experience working with various operating system window unixlinux and database management system redshift oracle mssql etc ability to collaborate effectively take initiative and work a part of a team strong attention to detail desireable skillsexperience data exploration experience using both sql and notonlysql tool and technique data design or data management experience is a plus experience with agile process is preferred experience in the financial service industry especially banking educational certification andor other professional credential 7 year of experience a described above and a bachelor degree in business finance mathematics statistic or computer science or 5 year of experience a described above and a master degree in those discipline hour work schedule hour per week 40 work schedule monday through friday 830am 500pm why work for u at citizen youll find a customercentric culture built around helping our customer and giving back to our local community when you join our team you are part of a supportive and collaborative workforce with access to training and tool to accelerate your potential and maximize your career growth equal employment opportunity it is the policy of citizen to provide equal employment and advancement opportunity to all colleague and applicant for employment without regard to race color ethnicity religion gender pregnancychildbirth colleague or a dependent reproductive health decision making age national origin sexual orientation gender identity or expression disability or perceived disability genetic information genetic characteristic citizenship veteran or military status marital or domestic partner status family statusparenthood victim of domestic violence or any other category protected by federal state andor local law equal employment and opportunity employerdisabledveteran citizen is a brand name of citizen bank na and each of it respective affiliate'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "result = open(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the type of the file\n",
    "print(type(result))\n",
    "\n",
    "# \n",
    "parsed = json.load(result)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print the data type of s3\n",
    "print(type(s3))\n",
    "\n",
    "# Print the bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bucket object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Job Post:  1430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Wholesale Payments - Small Business Data Scien...</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=fbd85e1549e4f...</td>\n",
       "      <td>CIB Wholesale Payments Data and Analytics - Da...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>cib wholesale payment data analytics data scie...</td>\n",
       "      <td>cib wholesale payments data and analytics data...</td>\n",
       "      <td>cib wholesal payment data and analyt data scie...</td>\n",
       "      <td>cib wholesale payment data and analytics data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Data Scientist\\nnew</td>\n",
       "      <td>Vistra Corporate Services Company</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d96310969eba0...</td>\n",
       "      <td>Responsibilities\\n A person in this role is e...</td>\n",
       "      <td>Irving</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>responsibility person role expected execute en...</td>\n",
       "      <td>responsibilities\\n a person in this role is ex...</td>\n",
       "      <td>respons a person in thi role is expect to exec...</td>\n",
       "      <td>responsibility a person in this role is expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Manager, Data Science\\nnew</td>\n",
       "      <td>Dell Technologies</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=13daea469fd59...</td>\n",
       "      <td>Senior Manager, Data Science (Round Rock TX or...</td>\n",
       "      <td>Round Rock</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>senior manager data science round rock tx remo...</td>\n",
       "      <td>senior manager data science round rock tx or r...</td>\n",
       "      <td>senior manag data scienc round rock tx or remo...</td>\n",
       "      <td>senior manager data science round rock tx or r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Senior Data Analyst - US Remote\\nnew</td>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=81c33357c602f...</td>\n",
       "      <td>Welcome to one of the toughest and most fulfil...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>75202</td>\n",
       "      <td>welcome one toughest fulfilling way help peopl...</td>\n",
       "      <td>welcome to one of the toughest and most fulfil...</td>\n",
       "      <td>welcom to one of the toughest and most fulfil ...</td>\n",
       "      <td>welcome to one of the toughest and most fulfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-06</th>\n",
       "      <td>Sr. Machine Learning Scientist\\nnew</td>\n",
       "      <td>Amazon.com Services LLC</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2541f249fc43d...</td>\n",
       "      <td>\\nMS or PhD in Artificial Intelligence, Comput...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>m phd artificial intelligence computer science...</td>\n",
       "      <td>ms or phd in artificial intelligence computer ...</td>\n",
       "      <td>ms or phd in artifici intellig comput scienc m...</td>\n",
       "      <td>m or phd in artificial intelligence computer s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-06  Wholesale Payments - Small Business Data Scien...   \n",
       "2021-02-06                         Senior Data Scientist\\nnew   \n",
       "2021-02-06                  Senior Manager, Data Science\\nnew   \n",
       "2021-02-06               Senior Data Analyst - US Remote\\nnew   \n",
       "2021-02-06                Sr. Machine Learning Scientist\\nnew   \n",
       "\n",
       "                                      company  company_rating  \\\n",
       "date                                                            \n",
       "2021-02-06          JPMorgan Chase Bank, N.A.             3.9   \n",
       "2021-02-06  Vistra Corporate Services Company             3.8   \n",
       "2021-02-06                  Dell Technologies             4.0   \n",
       "2021-02-06                 UnitedHealth Group             3.7   \n",
       "2021-02-06            Amazon.com Services LLC             3.6   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=fbd85e1549e4f...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=d96310969eba0...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=13daea469fd59...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=81c33357c602f...   \n",
       "2021-02-06  https://www.indeed.com/rc/clk?jk=2541f249fc43d...   \n",
       "\n",
       "                                              job_description        city  \\\n",
       "date                                                                        \n",
       "2021-02-06  CIB Wholesale Payments Data and Analytics - Da...       Plano   \n",
       "2021-02-06  Responsibilities\\n A person in this role is e...      Irving   \n",
       "2021-02-06  Senior Manager, Data Science (Round Rock TX or...  Round Rock   \n",
       "2021-02-06  Welcome to one of the toughest and most fulfil...      Dallas   \n",
       "2021-02-06  \\nMS or PhD in Artificial Intelligence, Comput...      Austin   \n",
       "\n",
       "           state  zipcode                                              clean  \\\n",
       "date                                                                           \n",
       "2021-02-06    TX        0  cib wholesale payment data analytics data scie...   \n",
       "2021-02-06    TX        0  responsibility person role expected execute en...   \n",
       "2021-02-06    TX        0  senior manager data science round rock tx remo...   \n",
       "2021-02-06    TX    75202  welcome one toughest fulfilling way help peopl...   \n",
       "2021-02-06    TX        0  m phd artificial intelligence computer science...   \n",
       "\n",
       "                                                    tokenized  \\\n",
       "date                                                            \n",
       "2021-02-06  cib wholesale payments data and analytics data...   \n",
       "2021-02-06  responsibilities\\n a person in this role is ex...   \n",
       "2021-02-06  senior manager data science round rock tx or r...   \n",
       "2021-02-06  welcome to one of the toughest and most fulfil...   \n",
       "2021-02-06  ms or phd in artificial intelligence computer ...   \n",
       "\n",
       "                                                      stemmed  \\\n",
       "date                                                            \n",
       "2021-02-06  cib wholesal payment data and analyt data scie...   \n",
       "2021-02-06  respons a person in thi role is expect to exec...   \n",
       "2021-02-06  senior manag data scienc round rock tx or remo...   \n",
       "2021-02-06  welcom to one of the toughest and most fulfil ...   \n",
       "2021-02-06  ms or phd in artifici intellig comput scienc m...   \n",
       "\n",
       "                                                   lemmatized  \n",
       "date                                                           \n",
       "2021-02-06  cib wholesale payment data and analytics data ...  \n",
       "2021-02-06  responsibility a person in this role is expect...  \n",
       "2021-02-06  senior manager data science round rock tx or r...  \n",
       "2021-02-06  welcome to one of the toughest and most fulfil...  \n",
       "2021-02-06  m or phd in artificial intelligence computer s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json file\n",
    "database = env_Shi.database\n",
    "df_ds_tx = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "\n",
    "# Print the number of job posts\n",
    "print(\"Number of Job Post: \", df_ds_tx.shape[0])\n",
    "\n",
    "# Conver the string date to datetime object\n",
    "df_ds_tx.date = pd.to_datetime(df_ds_tx.date)\n",
    "\n",
    "# Set the date as the index and sort the dataframe in descending order\n",
    "df_ds_tx = df_ds_tx.set_index('date').sort_index(ascending=False)\n",
    "df_ds_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1430 entries, 2021-02-06 to 2020-12-22\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            1430 non-null   object \n",
      " 1   company          1430 non-null   object \n",
      " 2   company_rating   1430 non-null   float64\n",
      " 3   job_link         1430 non-null   object \n",
      " 4   job_description  1430 non-null   object \n",
      " 5   city             1430 non-null   object \n",
      " 6   state            1430 non-null   object \n",
      " 7   zipcode          1430 non-null   int64  \n",
      " 8   clean            1430 non-null   object \n",
      " 9   tokenized        1430 non-null   object \n",
      " 10  stemmed          1430 non-null   object \n",
      " 11  lemmatized       1430 non-null   object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 145.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Print the information of the df_ds_tx\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cognizant Technology Solutions    53\n",
       "Dell Technologies                 39\n",
       "Deloitte                          31\n",
       "Facebook                          28\n",
       "USAA                              25\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 companies by the number of posts\n",
    "df_ds_tx.company.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin         408\n",
       "Dallas         223\n",
       "Houston        181\n",
       "San Antonio    112\n",
       "Plano          108\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 5 cities by the number of posts\n",
    "df_ds_tx.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-12-27    392\n",
       "2021-01-03    136\n",
       "2021-01-10    152\n",
       "2021-01-17    123\n",
       "2021-01-24    288\n",
       "2021-01-31    231\n",
       "2021-02-07    108\n",
       "Freq: W-SUN, Name: title, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: the dataframe has datetime index\n",
    "df_ds_tx.resample(\"W\").title.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Job Requirements by Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random job link\n",
    "\n",
    "job_url = df_ds.job_link.sample(1, random_state=1)[0]\n",
    "job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rquest\n",
    "\n",
    "response = requests.get(job_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a soup to hold the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of all the words appear in the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frequency'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cib wholesale payment data analytics data scientist new rapidly growing team change agent inside jp '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v1\n",
    "dic = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic.keys())\n",
    "\n",
    "# Print the first 100 characters of the value\n",
    "dic['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['all', 'Apple'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'summary posted jan 25 2021 role number200218691 apple great idea way becoming great product service selfmotivated highenergy person afraid challenge looking apple seeking ml advanced research application manager join amp data science analytics covering app store apple music apple tv apple podcasts apple fitness etc amp data science analytics collaborates executive various partner across business p'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v2\n",
    "\n",
    "companies = ['Apple']\n",
    "dic_v2 = words_variables_v2(df_ds_tx, companies)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic_v2.keys())\n",
    "\n",
    "# Print the first 100 characters of the value of `Apple`\n",
    "dic_v2['Apple'][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(d_words['frequency'].split()).value_counts()\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = d_words.keys()\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade `word_frequency_v1`\n",
    "\n",
    "def word_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>14575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>5210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>4605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "data            14575\n",
       "experience       8188\n",
       "business         5210\n",
       "team             4605\n",
       "work             4050"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function word_frequency_v1\n",
    "\n",
    "monogram = word_frequency_v1(dic)\n",
    "monogram.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15001 entries, data to oneknowledge\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   frequency  15001 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "monogram.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function word_frequency_v2\n",
    "\n",
    "df_word_frequency_v2 = word_frequency_v2(dic_v2)\n",
    "df_word_frequency_v2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added 'Bigram' column to dataframe\n",
    "df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 2))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine</th>\n",
       "      <th>learning</th>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <td>1757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>experience</th>\n",
       "      <td>1222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>scientist</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <th>science</th>\n",
       "      <td>926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     frequency\n",
       "machine  learning         2242\n",
       "data     science          1757\n",
       "year     experience       1222\n",
       "data     scientist        1000\n",
       "computer science           926"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = bigrams_frequency_v1(dic)\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute bigrams_frequency\n",
    "\n",
    "bigrams_v2 = bigrams_frequency_v2(dic_v2)\n",
    "bigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 3))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th>color</th>\n",
       "      <th>religion</th>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>employer</th>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <th>identity</th>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <th>regard</th>\n",
       "      <th>race</th>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  frequency\n",
       "sexual      orientation gender          419\n",
       "race        color       religion        413\n",
       "equal       opportunity employer        361\n",
       "orientation gender      identity        345\n",
       "without     regard      race            280"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function: trigrams_frequency_v1\n",
    "\n",
    "trigrams = trigrams_frequency_v1(dic)\n",
    "trigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: trigrams_frequency_v2\n",
    "\n",
    "trigrams_v2 = trigrams_frequency_v2(dic_v2)\n",
    "trigrams_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Mono-, Bi- and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Simple concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is total number of grams? \n",
    "\n",
    "monogram.shape[0]+bigrams.shape[0]+trigrams.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338056, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>14575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>8188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>5210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>4605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "data            14575\n",
       "experience       8188\n",
       "business         5210\n",
       "team             4605\n",
       "work             4050"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all three grams\n",
    "\n",
    "everygram = pd.concat([monogram, bigrams, trigrams])\n",
    "print(everygram.shape)\n",
    "everygram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use nltk.util.everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to \n",
    "\n",
    "def everygram_frequency(max_len=3):\n",
    "    '''\n",
    "    '''\n",
    "    # Read the json file\n",
    "    database = env_Shi.database\n",
    "    df = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "    d_words = words_variables_v1(df)\n",
    "    # Create a dataframe to hold the frequency of mono-, bri-, and tri-grams\n",
    "    gram_frequency = pd.DataFrame()\n",
    "    # Compute the frequency of mono-, bri-, and tri-grams\n",
    "    freq = pd.Series(list(nltk.everygrams(d_words['frequency'].split(), max_len=max_len))).value_counts()\n",
    "    # Add the 'freq' series to 'gram_frequency'\n",
    "    gram_frequency = pd.concat([gram_frequency, freq], axis=1, sort=True)\n",
    "    # Rename the columns\n",
    "    gram_frequency.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values\n",
    "    gram_frequency.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return gram_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 s, sys: 113 ms, total: 4.04 s\n",
      "Wall time: 4.06 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th>color</th>\n",
       "      <th>religion</th>\n",
       "      <td>413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equal</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>employer</th>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orientation</th>\n",
       "      <th>gender</th>\n",
       "      <th>identity</th>\n",
       "      <td>345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <th>regard</th>\n",
       "      <th>race</th>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">zyngas</th>\n",
       "      <th>ability</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dynamic</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338056 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  frequency\n",
       "sexual      orientation gender        419.0\n",
       "race        color       religion      413.0\n",
       "equal       opportunity employer      361.0\n",
       "orientation gender      identity      345.0\n",
       "without     regard      race          280.0\n",
       "...                                     ...\n",
       "zyngas      ability     NaN             NaN\n",
       "            data        NaN             NaN\n",
       "            dynamic     NaN             NaN\n",
       "            game        NaN             NaN\n",
       "            marketing   NaN             NaN\n",
       "\n",
       "[338056 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_test = everygram_frequency()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cib w'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['frequency'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_frequency = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = dic['frequency'].split()\n",
    "test = list(nltk.everygrams(sent, max_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(test).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(data,)</th>\n",
       "      <td>14575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(experience,)</th>\n",
       "      <td>8188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(business,)</th>\n",
       "      <td>5210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(team,)</th>\n",
       "      <td>4605.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(work,)</th>\n",
       "      <td>4050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(preferred, auto, req)</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(alongside, industry)</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(twilio, looking, talented)</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(among, important)</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(job, opportunity, director)</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338056 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "(data,)                       14575.0\n",
       "(experience,)                  8188.0\n",
       "(business,)                    5210.0\n",
       "(team,)                        4605.0\n",
       "(work,)                        4050.0\n",
       "...                               ...\n",
       "(preferred, auto, req)            1.0\n",
       "(alongside, industry)             1.0\n",
       "(twilio, looking, talented)       1.0\n",
       "(among, important)                1.0\n",
       "(job, opportunity, director)      1.0\n",
       "\n",
       "[338056 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_frequency = pd.concat([gram_frequency, test])\n",
    "gram_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a library for all skills\n",
    "\n",
    "library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', \n",
    "           'aws', 'hadoop', 'hive', 'impala', 'matlab', 'model', 'algorithm', \n",
    "           'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "           'sharepoint', 'dashboard']\n",
    "\n",
    "library_tech = ['programming', 'big data', 'wrangling', 'version control', 'visualiztion', ]\n",
    "library_soft = ['communication', 'business acumen', 'storytelling']\n",
    "library_tools = ['python', 'git', 'sql', 'pandas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "# big data\n",
    "# software engineering\n",
    "# model\n",
    "# models\n",
    "# algorithms\n",
    "# storytelling\n",
    "# statistic\n",
    "# statistical\n",
    "# machine learning\n",
    "# deep learning\n",
    "# etl\n",
    "# extraction\n",
    "# crud\n",
    "# exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_skills_ds_v1(k):\n",
    "    '''\n",
    "    This function accepts a positive integer k and \n",
    "    returns a dataframe containing the top k skills needed\n",
    "    for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_csv(f\"{database}df_tx_ds.csv\", index_col=0)\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    df_word_frequency = word_frequency_v1(dic)\n",
    "    # Define a library that has a complete sillset for data scientist\n",
    "    library = ['python', 'r', 'sql', 'tableau', 'scikitlearn', 'tensorflow', 'pytorch', 'aws', 'hadoop', 'hive', \n",
    "        'impala', 'matlab', 'model', 'algorithm', 'storytelling', 'statistic', 'etl', 'exploration', 'extraction', \n",
    "        'sharepoint', 'dashboard']\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (df_word_frequency.index == skill)\n",
    "        df = df_word_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function top_skills_ds\n",
    "\n",
    "top_skills = top_skills_ds_v1(7)\n",
    "top_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'python')\n",
    "df_word_frequency[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'r')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'aws')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (df_word_frequency.index == 'sql')\n",
    "df_word_frequency[mask].sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
