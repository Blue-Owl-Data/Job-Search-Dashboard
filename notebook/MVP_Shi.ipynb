{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Web Scraping Libraries\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Regex Library\n",
    "import re\n",
    "\n",
    "# Time-related Libraries\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# NLP Libraries\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper functions\n",
    "import MVP_Bojado, MVP_Shi\n",
    "\n",
    "# Environment file\n",
    "import env, env_Shi\n",
    "\n",
    "# AWS\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Geospatial Libraries\n",
    "import geopandas as gpd\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import folium\n",
    "\n",
    "\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>All the functions in the Data Acquisitioin section have been tested out and inorporated into the MVP_acquire_ds.py and MVP_acquire_wd.py files. To save space, no extra test is carried out in this notebook.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Indeed.com\n",
    "1. Search chemist in TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=TX\n",
    "2. Search chemist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=chemist&l=San+Antonio%2C+TX\n",
    "3. Search data scientist in San Antonio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX\n",
    "4. Search data scientist intern in San Anotnio, TX<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist+intern&l=San+Antonio%2C+TX\n",
    "5. Sort the data scientist jobs posting by date<br>\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=San+Antonio%2C+TX&sort=date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. q = job title\n",
    "2. l = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Format of Monster.com\n",
    "https://www.monster.com/jobs/search/?q=data-scientist&where=San-Antonio__2C-TX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the URL of a Job Search at Indeed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_url_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a URL of the 1st page of a job search at Indeed.com \n",
    "    based on the job title and the location.\n",
    "    '''\n",
    "    # Create the base URL for a job serch at Indeed.com\n",
    "    base_url = 'https://www.indeed.com/jobs?'\n",
    "    # Create a dictionary to map the keys to the input parameters\n",
    "    dic = {'q': job_title, 'l': location, 'sort': 'date'}\n",
    "    # Convert the dictionary to a query string\n",
    "    relative_url = urllib.parse.urlencode(dic)\n",
    "    # Generate the full URL of the first page\n",
    "    url = base_url + relative_url\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_page_soup_indeed(job_title, location):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of the first page of a request for job searching at Indeed.com\n",
    "    '''\n",
    "    # Generate the URL of the job search based on title and location\n",
    "    url = first_page_url_indeed(job_title, location)\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page_soup = first_page_soup_indeed(\"data scientist\", 'al')\n",
    "# type(first_page_soup)\n",
    "\n",
    "# # Find out the tag that contains the number of the jobs by seaching\n",
    "\n",
    "# num_jobs = first_page_soup.find('div', id='searchCountPages')\n",
    "# print(\"Data Type: \", type(num_jobs))\n",
    "# print(\"Name of the Tag: \", num_jobs.name)\n",
    "# print(\"Attributes of the Tag: \", num_jobs.attrs)\n",
    "# print(\"Text within the Tag: \")\n",
    "# num_jobs.text\n",
    "\n",
    "# # Find the number of the jobs in the text\n",
    "# match = re.findall(r'(\\d+)', num_jobs.text)\n",
    "# match[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_jobs_indeed(first_page_soup):\n",
    "    '''\n",
    "    This function returns the total number of the jobs in the searching result.\n",
    "    '''\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = first_page_soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    num_jobs = re.findall(r'(\\d+)', div.text)[1]\n",
    "    return num_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_num_indeed(url):\n",
    "    '''\n",
    "    This function returns the page number of job searching results. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Find out the section contains total number of jobs  \n",
    "    div = soup.find('div', id='searchCountPages')\n",
    "    # Extract the number\n",
    "    page_num = re.findall(r'(\\d+)', div.text)[0]\n",
    "    return page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract all job cards in a Indeed page\n",
    "\n",
    "def job_cards_indeed(soup):\n",
    "    '''\n",
    "    This function accepts the Soup object of a Indeed page \n",
    "    return an iterator containing the all the job cards in this page.\n",
    "    '''\n",
    "    # Find the appropriate tag that contains all of the job listings in this page\n",
    "    tag = soup.find('td', id=\"resultsCol\")\n",
    "    # Extract all job cards\n",
    "    job_cards = tag.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "    return job_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function job_cards_indeed\n",
    "# job_cards = job_cards_indeed(first_page_soup)\n",
    "\n",
    "# # Print the data type of job_cards\n",
    "# type(job_cards)\n",
    "\n",
    "# # How many jobs listed in the 1st page? \n",
    "# len(job_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_titles_indeed(job_cards):\n",
    "    '''\n",
    "    This function extract the job titles from a job_cards set. \n",
    "    '''\n",
    "    # Create a list to hold the job titles\n",
    "    titles = []\n",
    "    # For Loop throught the job cards to extract the titles\n",
    "    for job in job_cards:\n",
    "        title = job.find('h2', class_='title')\n",
    "        title = title.text.strip()\n",
    "        titles.append(title)\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company names from a set of job cards\n",
    "\n",
    "def company_names_indeed(job_cards):\n",
    "    '''\n",
    "    This function extracts the company names from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the company names\n",
    "    names = []\n",
    "    # For loop through the job cards to pull the company names\n",
    "    for job in job_cards:\n",
    "        name = job.find('span', class_='company')\n",
    "        name = name.text.strip()\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the post ages from a set of job cards\n",
    "\n",
    "def post_ages_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the post ages from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the post ages\n",
    "    ages = []\n",
    "    # For loop through the job cards to pull the post ages\n",
    "    for job in job_cards:\n",
    "        age = job.find('span', class_='date')\n",
    "        age = age.text.strip()\n",
    "        ages.append(age)\n",
    "    return ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the location from a set of job cards\n",
    "\n",
    "def job_locations_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job locations from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    locations = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        location = job.find('div', class_='location accessible-contrast-color-location')\n",
    "        if location == None:\n",
    "            location = job.find('span', class_='location accessible-contrast-color-location')\n",
    "        location = location.text.strip()\n",
    "        locations.append(location)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull the company ratings from a set of job cards\n",
    "\n",
    "def company_rating_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the company rating from a set of job cards.\n",
    "    If the rating is unavailable, it will be marked as 'missing'.\n",
    "    '''\n",
    "    # Create a list to hold the locations\n",
    "    ratings = []\n",
    "    # For loop through the job cards to pull the locations\n",
    "    for job in job_cards:\n",
    "        rating = job.find('span', class_='ratingsContent')\n",
    "        if rating == None:\n",
    "            ratings.append('missing')\n",
    "            continue\n",
    "        rating = rating.text.strip()\n",
    "        ratings.append(rating)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acuqire_indeed_job_description(url):\n",
    "    '''\n",
    "    This function accepts the URL of a job posting and pull its description.\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    request = requests.get(url)\n",
    "    print(\"Status Code: \", request.status_code)\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup variable holding the response content\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    if soup == None:\n",
    "        description = 'error'\n",
    "    else:\n",
    "        # Print the page's title\n",
    "        print(soup.title.string)\n",
    "        # Find the section that contains job description\n",
    "        description = soup.find('div', id=\"jobDescriptionText\")\n",
    "        if description == None:\n",
    "            description = 'error'\n",
    "        else:\n",
    "            description = description.text\n",
    "    return description\n",
    "\n",
    "def job_links_and_contents_indeed(job_cards):\n",
    "    '''\n",
    "    This function pulls the job links and descriptions from a set of job cards.\n",
    "    '''\n",
    "    # Create a list to hold the links and descriptions\n",
    "    links = []\n",
    "    descriptions = []\n",
    "    # For loop through the job cards to pull the links and descriptions\n",
    "    for job in job_cards:\n",
    "        link = job.find('a')['href']\n",
    "        link = 'https://www.indeed.com' + link\n",
    "        link = link.replace(';', '&')\n",
    "        description = acuqire_indeed_job_description(link)\n",
    "        links.append(link)\n",
    "        descriptions.append(description)\n",
    "    return links, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Soup object based on a job search url\n",
    "\n",
    "def page_soup_indeed(url):\n",
    "    '''\n",
    "    This function returns a BeautifulSoup object to hold the content \n",
    "    of a page for a job searching results at Indeed.com\n",
    "    '''\n",
    "    # Make the HTTP request\n",
    "    response = requests.get(url)\n",
    "    # Print the status code of the request\n",
    "    print(\"Status code of the request: \", response.status_code)\n",
    "    # Sanity check to make sure the document type is HTML\n",
    "    print(\"Document type: \", response.text[:15])\n",
    "    # Take a break\n",
    "    time.sleep(5)\n",
    "    # Make a soup to hold the response content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Print out the title of the content\n",
    "    print(\"Title of the response: \", soup.title.string)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the function: page_soup_indeed\n",
    "\n",
    "# url = 'https://www.indeed.com/jobs?q=data+scientist&l=al&sort=date'\n",
    "# soup = page_soup_indeed(url)\n",
    "# type(soup)\n",
    "\n",
    "# # Find out the page number\n",
    "# int(page_num_indeed(url))\n",
    "\n",
    "# # Pull the job cards from the soup\n",
    "# type(job_cards_indeed(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pull job information from a job search URL\n",
    "\n",
    "def acquire_page_indeed(url):\n",
    "    '''\n",
    "    This function accepts a job search URL and returns a pandas dataframe \n",
    "    containing job title, location, company, company rating, post age and description. \n",
    "    '''\n",
    "    # Create a Soup object based on the url\n",
    "    soup = page_soup_indeed(url)\n",
    "    # Pull the job cards\n",
    "    job_cards = job_cards_indeed(soup)\n",
    "    # Pull the job titles\n",
    "    titles = job_titles_indeed(job_cards)   \n",
    "    # Pull the names of the companies\n",
    "    companies = company_names_indeed(job_cards)\n",
    "    # Pull the post ages\n",
    "    ages = post_ages_indeed(job_cards)\n",
    "    # Pull the job locations\n",
    "    locations = job_locations_indeed(job_cards)\n",
    "    # Pull the company ratings\n",
    "    ratings = company_rating_indeed(job_cards)\n",
    "    # Pull the hyperlinks and job description\n",
    "    links, descriptions = job_links_and_contents_indeed(job_cards)    \n",
    "    # Create a dataframe\n",
    "    d = {'title': titles,\n",
    "         'location': locations,\n",
    "         'company': companies, \n",
    "         'company_rating': ratings,\n",
    "         'post_age': ages, \n",
    "         'job_link': links, \n",
    "         'job_description': descriptions}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobs_indeed(job_title, location, max_page=35):\n",
    "    '''\n",
    "    This function accepts the job title and location and return the job information (35 pages by default) \n",
    "    pulled from Indeed.com.\n",
    "    '''\n",
    "    # Generate the urls based on job title and location (state)\n",
    "    url = first_page_url = first_page_url_indeed(job_title, location)\n",
    "    # Set up an counter\n",
    "    counter = 1\n",
    "    # Create an empty dataframe to hold the job information\n",
    "    df_jobs = pd.DataFrame(columns = ['title', 'location', 'company', 'company_rating', \n",
    "                                      'post_age','job_link', 'job_description'])\n",
    "    # Pull the page number\n",
    "    page_num = int(page_num_indeed(url))\n",
    "    # Set up an checker\n",
    "    keep_going = (counter == page_num)   \n",
    "    # For loop through the urls to pull job information\n",
    "    while keep_going and page_num <= max_page:\n",
    "        df = acquire_page_indeed(url)\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Page: \", page_num)\n",
    "        print(\"--------------------------------\")\n",
    "        df_jobs = df_jobs.append(df, ignore_index=True)\n",
    "        df_jobs.to_csv(\"df_jobs_backup.csv\")\n",
    "        time.sleep(180)\n",
    "        dic = {'start': page_num*10}\n",
    "        relative_url = urllib.parse.urlencode(dic)\n",
    "        url = first_page_url + '&' + relative_url\n",
    "        counter = counter + 1\n",
    "        page_num = int(page_num_indeed(url))\n",
    "        keep_going = (counter == page_num)\n",
    "    # Print the total number of jobs\n",
    "    print(f\"Total number of {job_title} positions in {location}: \", df_jobs.shape[0])\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_title_initials():\n",
    "    '''\n",
    "    This function accepts the job title in a string format (all lower case) and \n",
    "    returns the initials of the job titles.\n",
    "    '''\n",
    "    print(\"Enter the job title: \")\n",
    "    job_title = input()\n",
    "    match = re.findall(r'([a-z])\\w+', job_title)\n",
    "    initials = ''.join(match)\n",
    "    return initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove the duplicates\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    '''\n",
    "    This function removes the duplicates in the dataframe\n",
    "    '''\n",
    "    # Define the columns for identifying duplicates\n",
    "    columns = ['title', 'location', 'company', 'job_link', 'job_description']\n",
    "    # Drop the duplicates except for the last occurrence\n",
    "    df.drop_duplicates(subset=columns, inplace=True, keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the date of the job posts\n",
    "\n",
    "def compute_post_date(df):\n",
    "    '''\n",
    "    This function computes the date of the job post based on post age\n",
    "    and set the date as the index of the dataframe.\n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.today()\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.today()\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the job titles for analysis\n",
    "\n",
    "def clean_job_title(title):\n",
    "    '''\n",
    "    This function removes the \"\\nnew\" and \"...\" in the job title.\n",
    "    '''\n",
    "    title = title.split(sep=\"\\nnew\")[0]\n",
    "#     title = title.split(sep=' -')[0]\n",
    "#     title = title.split(sep=' (')[0]\n",
    "#     title = title.split(sep=',')[0]\n",
    "    title = title.split(sep='...')[0]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to transform old job posts files\n",
    "\n",
    "def transform_old_file(df, date_string):\n",
    "    '''\n",
    "    This function accepts old daily job posts and convert the post age to post date. \n",
    "    '''\n",
    "    # Create an empty list to hold the post date\n",
    "    post_date = []\n",
    "    # For loop the column post_age and convert the values to date\n",
    "    for age in df.post_age:\n",
    "        if age == 'Just posted':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        elif age == 'Today':\n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            post_date.append(date)\n",
    "        else:\n",
    "            # Extract the number\n",
    "            num = re.findall(r'(\\d+)', age)[0]\n",
    "            # Cast the string number to integer\n",
    "            num = int(num)\n",
    "            # Convert the integer to timedelta object\n",
    "            num = datetime.timedelta(days=num)\n",
    "            # Compute post date        \n",
    "            date = datetime.date.fromisoformat(date_string)\n",
    "            date = date - num\n",
    "            post_date.append(date)\n",
    "    # Add post date as new column\n",
    "    df['date'] = post_date\n",
    "    # Set the column post_date as the index and sort the values\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load job posts of web developer in TX on Feb.12 2021\n",
    "\n",
    "# # Import the file path\n",
    "# database = env_Shi.database\n",
    "\n",
    "# # Read the daily data scientist jobs in TX\n",
    "# df_wd_old = pd.read_csv(f\"{database}web_developer_tx_indeed_021221.csv\", index_col=0)\n",
    "\n",
    "# # Print the first 2 rows\n",
    "# df_wd_old.head(2)\n",
    "\n",
    "# # Transform old file\n",
    "# df_wd_old = transform_old_file(df_wd_old, '2021-02-12')\n",
    "# df_wd_old.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: \n",
      "web developer\n",
      "(300, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UI AngularJS Bootstrap Developer\\nnew</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>iboss</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=7b562f4350e26...</td>\n",
       "      <td>Company Overview\\niboss is a cloud security co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Infrastructure Engineer\\nnew</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>data.world</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=9e84b017c9cc8...</td>\n",
       "      <td>The data.world team is looking for an Infrastr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title    location     company  \\\n",
       "0  UI AngularJS Bootstrap Developer\\nnew  Austin, TX       iboss   \n",
       "1           Infrastructure Engineer\\nnew  Austin, TX  data.world   \n",
       "\n",
       "  company_rating     post_age  \\\n",
       "0            3.2  Just posted   \n",
       "1        missing  Just posted   \n",
       "\n",
       "                                            job_link  \\\n",
       "0  https://www.indeed.com/rc/clk?jk=7b562f4350e26...   \n",
       "1  https://www.indeed.com/rc/clk?jk=9e84b017c9cc8...   \n",
       "\n",
       "                                     job_description  \n",
       "0  Company Overview\\niboss is a cloud security co...  \n",
       "1  The data.world team is looking for an Infrastr...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load web developer job posts in TX on Feb 23 2021\n",
    "\n",
    "# Import the file path\n",
    "database = env_Shi.database\n",
    "\n",
    "# Read the daily data scientist jobs in TX\n",
    "print(\"Enter the job title: \")\n",
    "job_title = input()\n",
    "job_title = job_title.split()\n",
    "df_new = pd.read_csv(f\"{database}{job_title[0]}_{job_title[1]}_tx_indeed_022321.csv\", index_col=0)\n",
    "\n",
    "# Print the dimentionality\n",
    "print(df_new.shape)\n",
    "\n",
    "# Print the first two rows\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_update(df):\n",
    "    '''\n",
    "    This function updates job posts of web developer in TX by adding the daily acquring\n",
    "    of web developer job posts in TX. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    initials = job_title_initials()\n",
    "    df_tx = pd.read_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    num_jobs = df_tx.shape[0]\n",
    "    # Convert the date column to datetime type\n",
    "    df_tx.date = pd.to_datetime(df_tx.date)\n",
    "    # Set the date column as the index and sort the index\n",
    "    df_tx = df_tx.set_index('date').sort_index(ascending=False)\n",
    "    # Add the daily update\n",
    "    df = compute_post_date(df)\n",
    "    df_tx = pd.concat([df_tx, df]).sort_index(ascending=False)\n",
    "    # Remove the duplicates\n",
    "    df_tx = remove_duplicates(df_tx)\n",
    "    # Save as csv file\n",
    "    df_tx.to_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    num_new_jobs = df_tx.shape[0] - num_jobs\n",
    "    print(\"New Jobs of Posted Today: \", num_new_jobs)\n",
    "    return df_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: \n",
      "web developer\n",
      "New Jobs of Posted Today:  108\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>post_age</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-23</th>\n",
       "      <td>UI AngularJS Bootstrap Developer\\nnew</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>iboss</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Just posted</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=7b562f4350e26...</td>\n",
       "      <td>Company Overview\\niboss is a cloud security co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-23</th>\n",
       "      <td>Senior UI Developer – React\\nnew</td>\n",
       "      <td>Dallas-Fort Worth, TX</td>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>missing</td>\n",
       "      <td>Today</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Senior UI Developer – React Cognizant Interact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title               location  \\\n",
       "date                                                                       \n",
       "2021-02-23  UI AngularJS Bootstrap Developer\\nnew             Austin, TX   \n",
       "2021-02-23       Senior UI Developer – React\\nnew  Dallas-Fort Worth, TX   \n",
       "\n",
       "                                   company company_rating     post_age  \\\n",
       "date                                                                     \n",
       "2021-02-23                           iboss            3.2  Just posted   \n",
       "2021-02-23  Cognizant Technology Solutions        missing        Today   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-23  https://www.indeed.com/rc/clk?jk=7b562f4350e26...   \n",
       "2021-02-23  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                              job_description  \n",
       "date                                                           \n",
       "2021-02-23  Company Overview\\niboss is a cloud security co...  \n",
       "2021-02-23  Senior UI Developer – React Cognizant Interact...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the daily updates\n",
    "\n",
    "df_test = daily_update(df_new)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4285 entries, 2021-02-23 to 2021-01-04 00:00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   title            4285 non-null   object\n",
      " 1   location         4285 non-null   object\n",
      " 2   company          4285 non-null   object\n",
      " 3   company_rating   4285 non-null   object\n",
      " 4   post_age         4285 non-null   object\n",
      " 5   job_link         4285 non-null   object\n",
      " 6   job_description  4285 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 267.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the job posts of web developer\n",
    "\n",
    "def prepare_job_posts_indeed():\n",
    "    '''\n",
    "   The function cleans the csv file of web developer job posts and save as json. \n",
    "    '''\n",
    "    # Read the job posts of web developer in TX\n",
    "    database = env_Shi.database\n",
    "    initials = job_title_initials()\n",
    "    df = pd.read_csv(f\"{database}df_{initials}_tx_backup.csv\")\n",
    "    # Create columns of city, state, and zipcode\n",
    "    location = df.location.str.split(', ', expand=True)\n",
    "    location.columns = ['city', 'zipcode']\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'United States' else i)\n",
    "    location.city = location.city.apply(lambda i: 0 if i == 'Texas' else i)\n",
    "    location.zipcode = location.zipcode.apply(lambda i: 0 if re.findall(r\"(\\d+)\", str(i)) == [] \n",
    "                                          else re.findall(r\"(\\d+)\", str(i))[0])\n",
    "    df['city'] = location.city\n",
    "    df['state'] = 'TX'\n",
    "    df['zipcode'] = location.zipcode\n",
    "    # Replace the missing values in the company rating with 0\n",
    "    df.company_rating = df.company_rating.apply(lambda i: 0 if i == 'missing' else i)\n",
    "    # Clean the text in the job description\n",
    "    df = MVP_Bojado.prep_job_description_data(df, 'job_description')\n",
    "    # Clean the job title\n",
    "    df.title = df.title.apply(clean_job_title)\n",
    "    # Drop the redundant columns post_age and location\n",
    "    redundant_cols = ['post_age', 'location', 'tokenized', 'stemmed', 'lemmatized']\n",
    "    df = df.drop(columns=redundant_cols)\n",
    "    # Alther the data type of company_rating and zipcode\n",
    "    df.company_rating = df.company_rating.apply(lambda i: float(i))\n",
    "    df.zipcode = df.zipcode.apply(lambda i: int(i))\n",
    "    # Save a JSON version of the prepared data\n",
    "    df.to_json(f\"{database}df_{initials}_tx_prepared_backup.json\", orient='records')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: \n",
      "web developer\n",
      "CPU times: user 50.3 s, sys: 303 ms, total: 50.6 s\n",
      "Wall time: 57 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>2021-01-12 00:00:00</td>\n",
       "      <td>Senior/Principal Software Engineer, Backend - ...</td>\n",
       "      <td>Project Admission</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.indeed.com/company/Project-Admissi...</td>\n",
       "      <td>Job Type: Full-time | RemoteCompany Headquarte...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>job type fulltime remotecompany headquarters n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>2021-02-03 00:00:00</td>\n",
       "      <td>Marketing Web Developer - Contractor</td>\n",
       "      <td>Evernote</td>\n",
       "      <td>3.4</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=326b5d531f080...</td>\n",
       "      <td>About the Role\\n\\nEvernote's websites inspire ...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>role evernotes website inspire motivate millio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date                                              title  \\\n",
       "4161  2021-01-12 00:00:00  Senior/Principal Software Engineer, Backend - ...   \n",
       "1604  2021-02-03 00:00:00               Marketing Web Developer - Contractor   \n",
       "\n",
       "                company  company_rating  \\\n",
       "4161  Project Admission             0.0   \n",
       "1604           Evernote             3.4   \n",
       "\n",
       "                                               job_link  \\\n",
       "4161  https://www.indeed.com/company/Project-Admissi...   \n",
       "1604  https://www.indeed.com/rc/clk?jk=326b5d531f080...   \n",
       "\n",
       "                                        job_description     city state  \\\n",
       "4161  Job Type: Full-time | RemoteCompany Headquarte...  Houston    TX   \n",
       "1604  About the Role\\n\\nEvernote's websites inspire ...   Austin    TX   \n",
       "\n",
       "      zipcode                                              clean  \n",
       "4161        0  job type fulltime remotecompany headquarters n...  \n",
       "1604        0  role evernotes website inspire motivate millio...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test the function: prepare_job_posts_indeed_wd\n",
    "df_test = prepare_job_posts_indeed()\n",
    "df_test.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4285 entries, 0 to 4284\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date             4285 non-null   object \n",
      " 1   title            4285 non-null   object \n",
      " 2   company          4285 non-null   object \n",
      " 3   company_rating   4285 non-null   float64\n",
      " 4   job_link         4285 non-null   object \n",
      " 5   job_description  4285 non-null   object \n",
      " 6   city             4285 non-null   object \n",
      " 7   state            4285 non-null   object \n",
      " 8   zipcode          4285 non-null   int64  \n",
      " 9   clean            4285 non-null   object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 334.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the data types\n",
    "\n",
    "# dtypes = {'company_rating': 'float16', \n",
    "#           'zipcode': 'int16'}\n",
    "# df_test.astype(dtypes).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Takeaways:</b>\n",
    "There are two ways to change the datatype: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the columns for identifying duplicates\n",
    "columns = ['date', 'title', 'company', 'job_link', 'job_description', 'city', 'state', 'zipcode']\n",
    "   \n",
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=columns,keep='last')\n",
    "duplicates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Takeaways:</b>\n",
    "After the job titles are cleaned, the duplicates of job postings of web developer positions increase from 0 to 625. It suggests that when the same job is re-posted, the job title changes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Read the json file\n",
    "\n",
    "# result = open(f\"{database}df_wd_tx_prepared_backup.json\")\n",
    "# parsed = json.load(result)\n",
    "# parsed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Files to AWS S3 Bucket\n",
    "- Up-to-date job postings of data scientist positions in TX\n",
    "- Cleaned job postings of data scientist positions in TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: \n",
      "web developer\n",
      "CPU times: user 820 ms, sys: 602 ms, total: 1.42 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Upload the json file to AWS\n",
    "\n",
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Create the job title initials\n",
    "initials = job_title_initials()\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket(f'{initials}rawjobpostings').upload_file(f\"{database}df_{initials}_tx_backup.csv\", f\"df_{initials}_tx_backup.csv\")\n",
    "\n",
    "# Upload df_ds_tx_prepared_backup.json file\n",
    "s3.Bucket(f'{initials}preparedjobpostings').upload_file(f\"{database}df_{initials}_tx_prepared_backup.json\", \n",
    "                                               f\"df_{initials}_tx_prepared_backup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Deveopment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 2, 23, 17, 43, 26, tzinfo=tzutc())"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When is the df_wd_tx_prepared.json last modified?\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "object_json = s3.Object('wdpreparedjobpostings', 'df_wd_tx_prepared_backup.json')\n",
    "object_json.last_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 2, 23, 17, 42, 45, tzinfo=tzutc())"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the time when the prepared json file is last modified\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "prepared_json = s3.Object('dspreparedjobpostings', 'df_ds_tx_prepared_backup.json')\n",
    "prepared_json.last_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'boto3.resources.factory.s3.ServiceResource'> \n",
      "\n",
      "\u001b[1mList of Buckets in S3:\u001b[0m\n",
      "additionaljobinfo\n",
      "amplify-jobdashboardfront-dev-180611-deployment\n",
      "dspreparedjobpostings\n",
      "dsrawjobpostings\n",
      "wdpreparedjobpostings\n",
      "wdrawjobpostings\n"
     ]
    }
   ],
   "source": [
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print the data type of s3\n",
    "print(type(s3), '\\n')\n",
    "\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "# Print the bucket names\n",
    "print(color.BOLD + \"List of Buckets in S3:\" + color.END)\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.factory.s3.Bucket"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the bucket object\n",
    "ds_raw = s3.Bucket('dsrawjobpostings')\n",
    "\n",
    "# Print the data type of ds_raw\n",
    "type(ds_raw)\n",
    "\n",
    "# List all the files inside the bucket\n",
    "\n",
    "for page in ds_raw.objects.pages():\n",
    "    for obj in page:\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket('dsrawjobpostings').upload_file(f\"{database}df_ds_tx_backup.csv\", \"df_ds_tx_backup.csv\")\n",
    "\n",
    "# Upload df_ds_tx_prepared_backup.json file\n",
    "s3.Bucket('dspreparedjobpostings').upload_file(f\"{database}df_ds_tx_prepared_backup.json\", \n",
    "                                               \"df_ds_tx_prepared_backup.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download JSON file from AWS \n",
    "# database = env_Shi.database\n",
    "# s3.Bucket('dsrawjobpostings').download_file('df_ds_tx_backup.csv', \n",
    "#                                             f\"{database}df_ds_tx_aws.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**: The file is successfully downloaded and saved in the customized location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON Files from the Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the initials of the job title\n",
    "\n",
    "def job_title_initials(job_title):\n",
    "    '''\n",
    "    This function accepts the job title in a string format (all lower case) and \n",
    "    returns the initials of the job titles.\n",
    "    '''\n",
    "    match = re.findall(r'([a-z])\\w+', job_title)\n",
    "    initials = ''.join(match)\n",
    "    return initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ds'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function `job_title_initials`\n",
    "job_title_initials('data scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load JSON file of job postings\n",
    "\n",
    "def read_job_postings_json():\n",
    "    '''\n",
    "    This function reads the JSON file of prepared job postings into a pandas dataframe \n",
    "    based on a job title and set the date as the index.\n",
    "    '''\n",
    "    # Load the file path of the local database\n",
    "    database = env_Shi.database\n",
    "    # Create the file name\n",
    "    initials = job_title_initials()\n",
    "    file_name = 'df_' + initials + '_tx_prepared_backup.json'\n",
    "    # Read the JSON file into a pandas dataframe\n",
    "    df = pd.read_json(f'{database}{file_name}')\n",
    "    # Print the numbr of job posts\n",
    "    print(\"Number of Job Postings: \", df.shape[0])\n",
    "    # Convert the string date to datetime\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    # Set the date as the index and sort the dataframe\n",
    "    df = df.set_index('date').sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the job title: \n",
      "data scientist\n",
      "Number of Job Postings:  2234\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2234 entries, 2021-02-23 to 2020-12-22\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   title            2234 non-null   object \n",
      " 1   company          2234 non-null   object \n",
      " 2   company_rating   2234 non-null   float64\n",
      " 3   job_link         2234 non-null   object \n",
      " 4   job_description  2234 non-null   object \n",
      " 5   city             2234 non-null   object \n",
      " 6   state            2234 non-null   object \n",
      " 7   zipcode          2234 non-null   int64  \n",
      " 8   clean            2234 non-null   object \n",
      "dtypes: float64(1), int64(1), object(7)\n",
      "memory usage: 174.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the job postings of data scientist position in TX\n",
    "\n",
    "df_ds_tx = read_job_postings_json()\n",
    "df_ds_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-22</th>\n",
       "      <td>Machine Learning &amp; Data Engineer</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=b3b1aca86cce5...</td>\n",
       "      <td>Corporate Banking Technology is hiring a multi...</td>\n",
       "      <td>Plano</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>corporate banking technology hiring multiskill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-22</th>\n",
       "      <td>Data Scientist Senior - Computer Vision/Deep L...</td>\n",
       "      <td>USAA</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Purpose of Job We are currently seeking a tale...</td>\n",
       "      <td>Leming</td>\n",
       "      <td>TX</td>\n",
       "      <td>78050</td>\n",
       "      <td>purpose job currently seeking talented data sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "date                                                            \n",
       "2021-02-22                   Machine Learning & Data Engineer   \n",
       "2021-02-22  Data Scientist Senior - Computer Vision/Deep L...   \n",
       "\n",
       "                              company  company_rating  \\\n",
       "date                                                    \n",
       "2021-02-22  JPMorgan Chase Bank, N.A.             3.9   \n",
       "2021-02-22                       USAA             3.9   \n",
       "\n",
       "                                                     job_link  \\\n",
       "date                                                            \n",
       "2021-02-22  https://www.indeed.com/rc/clk?jk=b3b1aca86cce5...   \n",
       "2021-02-22  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                              job_description    city state  \\\n",
       "date                                                                          \n",
       "2021-02-22  Corporate Banking Technology is hiring a multi...   Plano    TX   \n",
       "2021-02-22  Purpose of Job We are currently seeking a tale...  Leming    TX   \n",
       "\n",
       "            zipcode                                              clean  \n",
       "date                                                                    \n",
       "2021-02-22        0  corporate banking technology hiring multiskill...  \n",
       "2021-02-22    78050  purpose job currently seeking talented data sc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first two rows of the dataframe\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name\n",
       "0  User 1\n",
       "1  User 2\n",
       "2  User 3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-21 09:00:33,780 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n",
      "2021-02-21 09:00:33,781 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,783 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n",
      "2021-02-21 09:00:33,783 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,785 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"users\")\n",
      "2021-02-21 09:00:33,785 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,786 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info(\"users\")\n",
      "2021-02-21 09:00:33,787 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,789 INFO sqlalchemy.engine.base.Engine \n",
      "CREATE TABLE users (\n",
      "\t\"index\" BIGINT, \n",
      "\tname TEXT\n",
      ")\n",
      "\n",
      "\n",
      "2021-02-21 09:00:33,789 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,790 INFO sqlalchemy.engine.base.Engine COMMIT\n",
      "2021-02-21 09:00:33,791 INFO sqlalchemy.engine.base.Engine CREATE INDEX ix_users_index ON users (\"index\")\n",
      "2021-02-21 09:00:33,792 INFO sqlalchemy.engine.base.Engine ()\n",
      "2021-02-21 09:00:33,793 INFO sqlalchemy.engine.base.Engine COMMIT\n",
      "2021-02-21 09:00:33,794 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n",
      "2021-02-21 09:00:33,795 INFO sqlalchemy.engine.base.Engine INSERT INTO users (\"index\", name) VALUES (?, ?)\n",
      "2021-02-21 09:00:33,796 INFO sqlalchemy.engine.base.Engine ((0, 'User 1'), (1, 'User 2'), (2, 'User 3'))\n",
      "2021-02-21 09:00:33,797 INFO sqlalchemy.engine.base.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# Store the dataframe as a aql\n",
    "\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "df.to_sql('users', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'User 1'), (1, 'User 2'), (2, 'User 3')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECT * FROM users\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cognizant Technology Solutions    63\n",
       "Facebook                          49\n",
       "Dell Technologies                 46\n",
       "Deloitte                          44\n",
       "USAA                              41\n",
       "StataCorp                         34\n",
       "JPMorgan Chase Bank, N.A.         32\n",
       "Pearson                           30\n",
       "Accenture                         29\n",
       "Apple                             28\n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 companies by the number of posts\n",
    "df_ds_tx.company.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austin             619\n",
       "Dallas             355\n",
       "Houston            267\n",
       "Plano              169\n",
       "San Antonio        166\n",
       "Irving             109\n",
       "0                  101\n",
       "Fort Worth          60\n",
       "Round Rock          40\n",
       "College Station     36\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 cities by the number of posts\n",
    "df_ds_tx.city.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-12-27    392\n",
       "2021-01-03    136\n",
       "2021-01-10    212\n",
       "2021-01-17    187\n",
       "2021-01-24    352\n",
       "2021-01-31    291\n",
       "2021-02-07    249\n",
       "2021-02-14    228\n",
       "2021-02-21     89\n",
       "Freq: W-SUN, Name: title, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: the dataframe has datetime index\n",
    "df_ds_tx.resample(\"W\").title.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Scientist                                225\n",
       "Senior Data Scientist                          82\n",
       "Machine Learning Engineer                      52\n",
       "Senior Data Analyst                            48\n",
       "Sr. Data Scientist                             38\n",
       "Principal Data Scientist                       37\n",
       "Sr. Data Analyst                               31\n",
       "Data Engineer                                  23\n",
       "Senior Statistician and Software Developer     20\n",
       "Data Scientist II                              19\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df_ds_tx.title.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Company Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/rc/clk?jk=3ec2fc58833a3965&fccid=1639254ea84748b5&vjs=3'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a link of a job post\n",
    "\n",
    "sample = df_ds_tx.job_link[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Takeaways</b>: Detailed physical address can't be found on the Indeed website. It has to be obtained through other approaches. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Job Requirements by Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random job link\n",
    "\n",
    "job_url = df_ds.job_link.sample(1, random_state=1)[0]\n",
    "job_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the rquest\n",
    "\n",
    "response = requests.get(job_url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a soup to hold the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 'words' variable\n",
    "words = [re.sub(r'([^a-z0-9\\s]|\\s.\\s)', '', doc).split() for doc in df_ds_tx.clean]\n",
    "\n",
    "# Add 'words' column to dataframe\n",
    "# Column will contain lists of separated words in each repo\n",
    "df_ds_tx = pd.concat([df_ds_tx, pd.DataFrame({'words': words})], axis=1)\n",
    "\n",
    "df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis of Mono-, Bi-, and Tri-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of all the words appear in the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the words that appear in the job descriptions\n",
    "\n",
    "def words_variables_v1(df):\n",
    "    '''\n",
    "    This function accepts the dataframe with cleaned job description \n",
    "    and return a dictionary in which the values are the words that \n",
    "    appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'frequency': all_words}\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frequency'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opportunity develop team mentor manager develop innovative data science solution utilize machine lea'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the helper function: words_variables_v1\n",
    "dic = words_variables_v1(df_ds_tx)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic.keys())\n",
    "\n",
    "# Print the first 100 characters of the value\n",
    "dic['frequency'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade the function `words_variables_v1`\n",
    "\n",
    "def words_variables_v2(df, companies):\n",
    "    '''\n",
    "    This function accepts the dataframe containing cleaned job description and \n",
    "    a list of company names and return a dictionary in which the values are the words \n",
    "    that appear in the job description. \n",
    "    '''\n",
    "    # Create the words that appear all the job descritipons\n",
    "    all_words = ' '.join(df.clean)\n",
    "    # Create a dictionary to hold the variable all_words\n",
    "    d_words = {'all': all_words}\n",
    "    # For loop the companies and create the words that appear in their job descriptions\n",
    "    for company in companies:\n",
    "        mask = (df.company == company)\n",
    "        s_company = df[mask].clean\n",
    "        words = ' '.join(s_company)\n",
    "        d_words[company] = words\n",
    "    return d_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the helper function: words_variables_v2\n",
    "\n",
    "companies = ['Apple']\n",
    "dic_v2 = words_variables_v2(df_ds_tx, companies)\n",
    "\n",
    "# Print out the keys\n",
    "print(dic_v2.keys())\n",
    "\n",
    "# Print the first 100 characters of the value of `Apple`\n",
    "dic_v2['Apple'][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monogram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the word frequency in the job description\n",
    "\n",
    "def word_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(d_words['frequency'].split()).value_counts()\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = d_words.keys()\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade `word_frequency_v1`\n",
    "\n",
    "def word_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(d_words[company].split()).value_counts()\n",
    "        word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    word_counts.columns = companies\n",
    "    word_counts = word_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    word_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Added 'Bigram' column to dataframe\n",
    "# df_ds_tx['bigrams'] = [list(nltk.ngrams(wordlist, 2)) for wordlist in df_ds_tx.words]\n",
    "# df_ds_tx.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 2))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the bigrams frequency in the job description\n",
    "\n",
    "def bigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the bigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    bigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 2))).value_counts()\n",
    "        bigrams_counts = pd.concat([bigrams_counts, freq], axis=1, sort=True)\n",
    "    bigrams_counts.columns = companies\n",
    "    bigrams_counts = bigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    bigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return bigrams_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_frequency_v1(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v1\n",
    "    and return the word frequency in the job description. \n",
    "    '''\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    word_counts = pd.DataFrame()\n",
    "    # Compute the words frequency\n",
    "    freq = pd.Series(list(nltk.ngrams(d_words['frequency'].split(), 3))).value_counts()\n",
    "    # Add the `freq` seires to `word_counts` dataframe\n",
    "    word_counts = pd.concat([word_counts, freq], axis=1, sort=True)\n",
    "    # Rename the coumns\n",
    "    word_counts.columns = d_words.keys()\n",
    "    # Sort the dataframe by the values in column `frequency`\n",
    "    word_counts.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the trigrams frequency in the job description\n",
    "\n",
    "def trigrams_frequency_v2(d_words):\n",
    "    '''\n",
    "    This function accept the dictionary created by function words_variables_v2\n",
    "    and return the trigrams frequency in the job description. \n",
    "    '''\n",
    "    # Read the company names from the dictionary\n",
    "    companies = d_words.keys()\n",
    "    # Create a dataframe to hold the word frequency\n",
    "    trigrams_counts = pd.DataFrame()\n",
    "    # For loop through the companies and generate the word frequency in their job descriptions\n",
    "    for company in companies:\n",
    "        freq = pd.Series(list(nltk.ngrams(d_words[company].split(), 3))).value_counts()\n",
    "        trigrams_counts = pd.concat([trigrams_counts, freq], axis=1, sort=True)\n",
    "    trigrams_counts.columns = companies\n",
    "    trigrams_counts = trigrams_counts.fillna(0).apply(lambda s: s.astype(int))\n",
    "    trigrams_counts.sort_values(by='all', ascending=False, inplace=True)\n",
    "    return trigrams_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Mono-, Bi- and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Simple concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2:  Use nltk.util.everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the frequence of the mono-, bi-, and tri-grams of the job description\n",
    "\n",
    "def everygram_frequency_v1(d_words, max_len=3):\n",
    "    '''\n",
    "    This function accetps the dictionary produced by the function `words_variables_v1` and \n",
    "    return mono-, bi-, and tri-grams along with their frequency. \n",
    "    '''\n",
    "    # Generate mono-, bi-, and tri-grams\n",
    "    grams = nltk.everygrams(d_words['frequency'].split(), max_len=max_len) # dtype of grams: <class 'genertor'>\n",
    "    # Convert to a list of tuples\n",
    "    grams = list(grams)\n",
    "    # Create an empty list to hold mono-, bi-, and tri-grams\n",
    "    everygram = []\n",
    "    # For loop the list of tuples and convert the grams to strings\n",
    "    for gram in grams:\n",
    "        str_gram = gram[0]\n",
    "        for i in gram[1:]:\n",
    "            str_gram = str_gram + ' ' + i\n",
    "        everygram.append(str_gram)\n",
    "    # Compute the frequency of the everygrams\n",
    "    everygram = pd.Series(everygram).value_counts()\n",
    "    return everygram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data          16501\n",
       "experience     9129\n",
       "business       5749\n",
       "team           5112\n",
       "work           4516\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function above\n",
    "\n",
    "everygram = everygram_frequency_v1(dic)\n",
    "everygram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ds_grams as json file\n",
    "everygram.to_json(f\"{database}ds_grams.json\", orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploade to AWS s3 bucket\n",
    "\n",
    "# Create the s3 resource object\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read the location of the database\n",
    "database = env_Shi.database\n",
    "\n",
    "# Upload df_ds_tx_backup.csv file\n",
    "s3.Bucket('additionaljobinfo').upload_file(f\"{database}ds_grams.json\", \"ds_grams.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Top 5 Skills in a Predifined Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pick up the top k skills of a data scientict from a skillset library\n",
    "\n",
    "def top_skills_ds_v1(k, library):\n",
    "    '''\n",
    "    This function accepts a positive integer k and a skillset library and \n",
    "    returns a dataframe containing the top k skills needed for data scientist positions.\n",
    "    '''\n",
    "    # Import the file path\n",
    "    database = env_Shi.database\n",
    "    # Load the prepared dataframe with job search results\n",
    "    df = pd.read_json(f\"{database}df_ds_tx_prepared_backup.json\")\n",
    "    # Create a string of all words that appear in the job description\n",
    "    dic = words_variables_v1(df)\n",
    "    # Compute the words frequency\n",
    "    everygram_frequency = everygram_frequency_v1(dic)\n",
    "    # Create a empty dataframe to hold the rank of the skills\n",
    "    df_skills = pd.DataFrame()\n",
    "    # For loop through the library to find out the frequency of the skills mentioned in the job description\n",
    "    for skill in library:\n",
    "        mask = (everygram_frequency.index == skill)\n",
    "        df =  everygram_frequency[mask]\n",
    "        df_skills = pd.concat([df_skills, df])\n",
    "    df_skills.columns = dic.keys()\n",
    "    df_skills.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    return df_skills.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in tech skill library:  63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine learning</th>\n",
       "      <td>2521.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>1329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sql</th>\n",
       "      <td>1012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aws</th>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big data</th>\n",
       "      <td>622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spark</th>\n",
       "      <td>569.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadoop</th>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agile</th>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deep learning</th>\n",
       "      <td>386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tableau</th>\n",
       "      <td>374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>azure</th>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data visualization</th>\n",
       "      <td>358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data mining</th>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etl</th>\n",
       "      <td>307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dashboard</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>math</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    frequency\n",
       "machine learning       2521.0\n",
       "python                 1329.0\n",
       "sql                    1012.0\n",
       "r                       760.0\n",
       "aws                     689.0\n",
       "big data                622.0\n",
       "spark                   569.0\n",
       "hadoop                  539.0\n",
       "c                       442.0\n",
       "java                    439.0\n",
       "agile                   393.0\n",
       "deep learning           386.0\n",
       "tableau                 374.0\n",
       "azure                   372.0\n",
       "data visualization      358.0\n",
       "data mining             349.0\n",
       "etl                     307.0\n",
       "dashboard               268.0\n",
       "math                    266.0\n",
       "nlp                     266.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tech library\n",
    "tech_library = ['python', 'sql', 'pandas','numpy','matplotlib','scikit learn','spark','hadoop',\n",
    "           'aws','amazon web services','azure','microsoft word','microsoft excel','excel',\n",
    "           'tableau','tensor flow','pytorch','hive', 'impala', 'matlab','etl',\n",
    "           'statistics','exploration', 'extraction', 'data wrangling','math',\n",
    "           'machine learning','data visualization','java','js',\n",
    "           'javascript','scala','r','c','c++','power bi','dashboard','linear algebra',\n",
    "           'calculus','neural networks','eda','big data','frameworks','database management',\n",
    "           'testing hypotheses','probability','data mining','perl','nosql','saas','git',\n",
    "           'github','natural language processing','nlp','deep learning','agile','kanban',\n",
    "           'project management','julia','devops','google cloud','pytorch','computer vision']\n",
    "\n",
    "# Print the number of skills in the library\n",
    "print(\"Number of skills in tech skill library: \", len(tech_library))\n",
    "\n",
    "# Test function: top_skills_ds_v1\n",
    "df_test = top_skills_ds_v1(20, tech_library)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skills in soft skill library:  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>3576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communication</th>\n",
       "      <td>1122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leadership</th>\n",
       "      <td>707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collaboration</th>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem solving</th>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>written communication</th>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision making</th>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verbal communication</th>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity</th>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team player</th>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time management</th>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business acumen</th>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critical thinking</th>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teamwork</th>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storytelling</th>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain knowledge</th>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adaptability</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       frequency\n",
       "learning                  3576.0\n",
       "communication             1122.0\n",
       "leadership                 707.0\n",
       "collaboration              348.0\n",
       "problem solving            244.0\n",
       "written communication      199.0\n",
       "decision making            146.0\n",
       "verbal communication       109.0\n",
       "curiosity                   93.0\n",
       "creativity                  91.0\n",
       "team player                 84.0\n",
       "time management             79.0\n",
       "business acumen             73.0\n",
       "critical thinking           70.0\n",
       "teamwork                    53.0\n",
       "storytelling                17.0\n",
       "domain knowledge            16.0\n",
       "adaptability                12.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick up the top 20 soft skills\n",
    "soft_library = ['critical thinking','communication','problem solving','teamwork','ethics','business acumen',\n",
    "           'interpersonal skills','curiosity','storytelling','adaptability','team player','collaboration',\n",
    "                'time management','leadership','domain knowledge','creativity','decision making',\n",
    "           'verbal communication','written communication']\n",
    "\n",
    "# Print the number of skills in the library\n",
    "print(\"Number of skills in soft skill library: \", len(soft_library))\n",
    "\n",
    "top_soft_skills = top_skills_ds_v1(20, soft_library)\n",
    "top_soft_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Match Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the job position that match the skills of a applicant\n",
    "\n",
    "def skills_match_search(job_title, skills):\n",
    "    '''\n",
    "    '''  \n",
    "    # Create the initials of the job_title\n",
    "    initials = job_title_initials(job_title)\n",
    "    # Load the file path\n",
    "    database = env_Shi.database\n",
    "    # Create the file name\n",
    "    file_name = 'df_' + initials + '_tx_prepared_backup.json'\n",
    "    # Load the job postings file\n",
    "    df = pd.read_json(f'{database}{file_name}')\n",
    "    # Create a list variable to hold the boolean values\n",
    "    mask = []\n",
    "    # For loop \n",
    "    for clean in df.clean:\n",
    "        if all(skill in clean for skill in skills):\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    df_match = df[mask]\n",
    "    # Drop redudant columns\n",
    "    cols = ['date', 'zipcode', 'clean', 'tokenized', 'stemmed', 'lemmatized']\n",
    "    df_match.drop(columns=cols, inplace=True)\n",
    "    print(\"Number of the Matched Companies: \", df_match.shape[0])\n",
    "    return df_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the Matched Companies:  72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(72, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = ['python', 'sql', 'tableau', 'aws']\n",
    "\n",
    "df = skills_match_search('data scientist', skills)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Cloud Data Engineer</td>\n",
       "      <td>Spectral MD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Company OverviewSpectral MD, Inc. is a medical...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>Cloud Data Engineer\\nnew</td>\n",
       "      <td>Spectral MD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://www.indeed.com/pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Company OverviewSpectral MD, Inc. is a medical...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title      company  company_rating  \\\n",
       "464       Cloud Data Engineer  Spectral MD             0.0   \n",
       "370  Cloud Data Engineer\\nnew  Spectral MD             0.0   \n",
       "\n",
       "                                              job_link  \\\n",
       "464  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "370  https://www.indeed.com/pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                       job_description    city state  \n",
       "464  Company OverviewSpectral MD, Inc. is a medical...  Dallas    TX  \n",
       "370  Company OverviewSpectral MD, Inc. is a medical...  Dallas    TX  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the masks for different skills\n",
    "\n",
    "mask_python = df_ds_tx.clean.str.contains('python')\n",
    "mask_sql = df_ds_tx.clean.str.contains('sql')\n",
    "mask_ml = df_ds_tx.clean.str.contains('machine learning')\n",
    "mask_tableau = df_ds_tx.clean.str.contains('tableau')\n",
    "mask_aws = df_ds_tx.clean.str.contains('aws')\n",
    "\n",
    "mask = mask_python & mask_sql & mask_tableau & mask_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many companies need all three skills: python, sql and tableau\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_description</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-19</th>\n",
       "      <td>Analyst, Data Science - Product Analytics</td>\n",
       "      <td>Expedia Group</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=e447fed7ec145...</td>\n",
       "      <td>This is a great opportunity to join Vrbo’s glo...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>78758</td>\n",
       "      <td>great opportunity join vrbos global analytics ...</td>\n",
       "      <td>this is a great opportunity to join vrbos glob...</td>\n",
       "      <td>thi is a great opportun to join vrbo global an...</td>\n",
       "      <td>this is a great opportunity to join vrbos glob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title        company  \\\n",
       "date                                                                   \n",
       "2021-02-19  Analyst, Data Science - Product Analytics  Expedia Group   \n",
       "\n",
       "            company_rating                                           job_link  \\\n",
       "date                                                                            \n",
       "2021-02-19             3.9  https://www.indeed.com/rc/clk?jk=e447fed7ec145...   \n",
       "\n",
       "                                              job_description    city state  \\\n",
       "date                                                                          \n",
       "2021-02-19  This is a great opportunity to join Vrbo’s glo...  Austin    TX   \n",
       "\n",
       "            zipcode                                              clean  \\\n",
       "date                                                                     \n",
       "2021-02-19    78758  great opportunity join vrbos global analytics ...   \n",
       "\n",
       "                                                    tokenized  \\\n",
       "date                                                            \n",
       "2021-02-19  this is a great opportunity to join vrbos glob...   \n",
       "\n",
       "                                                      stemmed  \\\n",
       "date                                                            \n",
       "2021-02-19  thi is a great opportun to join vrbo global an...   \n",
       "\n",
       "                                                   lemmatized  \n",
       "date                                                           \n",
       "2021-02-19  this is a great opportunity to join vrbos glob...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ds_tx[mask].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds_tx.clean[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis\n",
    "- Centered on Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-ff77852d28dc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-ff77852d28dc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    x for x in range(5)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
